{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4843e104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from math import sqrt\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "from helper import series_to_supervised\n",
    "from model.cnn import cnn_4layer\n",
    "from model.mlp import mlp_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "460ee40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9b8e616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# random.seed(10)\n",
    "# print(random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb30958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>price_dayahead</th>\n",
       "      <th>gen_coal</th>\n",
       "      <th>gen_gas</th>\n",
       "      <th>load_actual</th>\n",
       "      <th>gen_lig</th>\n",
       "      <th>gen_oil</th>\n",
       "      <th>gen_oth_renew</th>\n",
       "      <th>pressure_Barcelona</th>\n",
       "      <th>pressure_Bilbao</th>\n",
       "      <th>...</th>\n",
       "      <th>wind_deg_Bilbao</th>\n",
       "      <th>clouds_all_Bilbao</th>\n",
       "      <th>gen_hyd_river</th>\n",
       "      <th>wind_deg_Seville</th>\n",
       "      <th>wind_speed_Barcelona</th>\n",
       "      <th>wind_speed_Valencia</th>\n",
       "      <th>wind_speed_Bilbao</th>\n",
       "      <th>gen_wind</th>\n",
       "      <th>wind_speed_Madrid</th>\n",
       "      <th>gen_hyd_pump</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:00:00+00:00</th>\n",
       "      <td>64.92</td>\n",
       "      <td>48.10</td>\n",
       "      <td>4755.0</td>\n",
       "      <td>5196.0</td>\n",
       "      <td>24382.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>...</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5890.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>920.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 01:00:00+00:00</th>\n",
       "      <td>64.48</td>\n",
       "      <td>47.33</td>\n",
       "      <td>4581.0</td>\n",
       "      <td>4857.0</td>\n",
       "      <td>22734.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>...</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5461.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 02:00:00+00:00</th>\n",
       "      <td>59.32</td>\n",
       "      <td>42.27</td>\n",
       "      <td>4131.0</td>\n",
       "      <td>4314.0</td>\n",
       "      <td>21286.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>...</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>949.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5238.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 03:00:00+00:00</th>\n",
       "      <td>56.04</td>\n",
       "      <td>38.41</td>\n",
       "      <td>3840.0</td>\n",
       "      <td>4130.0</td>\n",
       "      <td>20264.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>...</td>\n",
       "      <td>221.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>953.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4935.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1826.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 04:00:00+00:00</th>\n",
       "      <td>53.63</td>\n",
       "      <td>35.72</td>\n",
       "      <td>3590.0</td>\n",
       "      <td>4038.0</td>\n",
       "      <td>19905.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1037.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>...</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>952.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4618.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 18:00:00+00:00</th>\n",
       "      <td>77.02</td>\n",
       "      <td>68.85</td>\n",
       "      <td>2628.0</td>\n",
       "      <td>7634.0</td>\n",
       "      <td>30653.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1027.0</td>\n",
       "      <td>1033.0</td>\n",
       "      <td>...</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1135.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3113.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 19:00:00+00:00</th>\n",
       "      <td>76.16</td>\n",
       "      <td>68.40</td>\n",
       "      <td>2566.0</td>\n",
       "      <td>7241.0</td>\n",
       "      <td>29735.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1027.0</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1172.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3288.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 20:00:00+00:00</th>\n",
       "      <td>74.30</td>\n",
       "      <td>66.88</td>\n",
       "      <td>2422.0</td>\n",
       "      <td>7025.0</td>\n",
       "      <td>28071.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>...</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1148.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3503.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 21:00:00+00:00</th>\n",
       "      <td>69.89</td>\n",
       "      <td>63.93</td>\n",
       "      <td>2293.0</td>\n",
       "      <td>6562.0</td>\n",
       "      <td>25801.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>...</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1128.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3586.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 22:00:00+00:00</th>\n",
       "      <td>69.88</td>\n",
       "      <td>64.27</td>\n",
       "      <td>2166.0</td>\n",
       "      <td>6926.0</td>\n",
       "      <td>24455.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3651.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35063 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           price  price_dayahead  gen_coal  gen_gas   \n",
       "time                                                                  \n",
       "2015-01-01 00:00:00+00:00  64.92           48.10    4755.0   5196.0  \\\n",
       "2015-01-01 01:00:00+00:00  64.48           47.33    4581.0   4857.0   \n",
       "2015-01-01 02:00:00+00:00  59.32           42.27    4131.0   4314.0   \n",
       "2015-01-01 03:00:00+00:00  56.04           38.41    3840.0   4130.0   \n",
       "2015-01-01 04:00:00+00:00  53.63           35.72    3590.0   4038.0   \n",
       "...                          ...             ...       ...      ...   \n",
       "2018-12-31 18:00:00+00:00  77.02           68.85    2628.0   7634.0   \n",
       "2018-12-31 19:00:00+00:00  76.16           68.40    2566.0   7241.0   \n",
       "2018-12-31 20:00:00+00:00  74.30           66.88    2422.0   7025.0   \n",
       "2018-12-31 21:00:00+00:00  69.89           63.93    2293.0   6562.0   \n",
       "2018-12-31 22:00:00+00:00  69.88           64.27    2166.0   6926.0   \n",
       "\n",
       "                           load_actual  gen_lig  gen_oil  gen_oth_renew   \n",
       "time                                                                      \n",
       "2015-01-01 00:00:00+00:00      24382.0    328.0    158.0           71.0  \\\n",
       "2015-01-01 01:00:00+00:00      22734.0    323.0    157.0           73.0   \n",
       "2015-01-01 02:00:00+00:00      21286.0    254.0    160.0           75.0   \n",
       "2015-01-01 03:00:00+00:00      20264.0    187.0    156.0           74.0   \n",
       "2015-01-01 04:00:00+00:00      19905.0    178.0    156.0           74.0   \n",
       "...                                ...      ...      ...            ...   \n",
       "2018-12-31 18:00:00+00:00      30653.0      0.0    178.0           95.0   \n",
       "2018-12-31 19:00:00+00:00      29735.0      0.0    174.0           95.0   \n",
       "2018-12-31 20:00:00+00:00      28071.0      0.0    168.0           94.0   \n",
       "2018-12-31 21:00:00+00:00      25801.0      0.0    163.0           93.0   \n",
       "2018-12-31 22:00:00+00:00      24455.0      0.0    163.0           92.0   \n",
       "\n",
       "                           pressure_Barcelona  pressure_Bilbao  ...   \n",
       "time                                                            ...   \n",
       "2015-01-01 00:00:00+00:00              1035.0           1035.0  ...  \\\n",
       "2015-01-01 01:00:00+00:00              1036.0           1036.0  ...   \n",
       "2015-01-01 02:00:00+00:00              1036.0           1035.0  ...   \n",
       "2015-01-01 03:00:00+00:00              1036.0           1035.0  ...   \n",
       "2015-01-01 04:00:00+00:00              1037.0           1035.0  ...   \n",
       "...                                       ...              ...  ...   \n",
       "2018-12-31 18:00:00+00:00              1027.0           1033.0  ...   \n",
       "2018-12-31 19:00:00+00:00              1027.0           1034.0  ...   \n",
       "2018-12-31 20:00:00+00:00              1028.0           1034.0  ...   \n",
       "2018-12-31 21:00:00+00:00              1028.0           1034.0  ...   \n",
       "2018-12-31 22:00:00+00:00              1028.0           1034.0  ...   \n",
       "\n",
       "                           wind_deg_Bilbao  clouds_all_Bilbao  gen_hyd_river   \n",
       "time                                                                           \n",
       "2015-01-01 00:00:00+00:00            229.0                0.0         1009.0  \\\n",
       "2015-01-01 01:00:00+00:00            224.0                0.0          973.0   \n",
       "2015-01-01 02:00:00+00:00            225.0                0.0          949.0   \n",
       "2015-01-01 03:00:00+00:00            221.0                0.0          953.0   \n",
       "2015-01-01 04:00:00+00:00            224.0                0.0          952.0   \n",
       "...                                    ...                ...            ...   \n",
       "2018-12-31 18:00:00+00:00             57.0                0.0         1135.0   \n",
       "2018-12-31 19:00:00+00:00              0.0                0.0         1172.0   \n",
       "2018-12-31 20:00:00+00:00            140.0                0.0         1148.0   \n",
       "2018-12-31 21:00:00+00:00            120.0                0.0         1128.0   \n",
       "2018-12-31 22:00:00+00:00            100.0                0.0         1069.0   \n",
       "\n",
       "                           wind_deg_Seville  wind_speed_Barcelona   \n",
       "time                                                                \n",
       "2015-01-01 00:00:00+00:00              21.0                   7.0  \\\n",
       "2015-01-01 01:00:00+00:00              27.0                   7.0   \n",
       "2015-01-01 02:00:00+00:00              27.0                   7.0   \n",
       "2015-01-01 03:00:00+00:00              27.0                   7.0   \n",
       "2015-01-01 04:00:00+00:00              57.0                   5.0   \n",
       "...                                     ...                   ...   \n",
       "2018-12-31 18:00:00+00:00              30.0                   1.0   \n",
       "2018-12-31 19:00:00+00:00              30.0                   3.0   \n",
       "2018-12-31 20:00:00+00:00              50.0                   4.0   \n",
       "2018-12-31 21:00:00+00:00              60.0                   5.0   \n",
       "2018-12-31 22:00:00+00:00              50.0                   5.0   \n",
       "\n",
       "                           wind_speed_Valencia  wind_speed_Bilbao  gen_wind   \n",
       "time                                                                          \n",
       "2015-01-01 00:00:00+00:00                  1.0                0.0    5890.0  \\\n",
       "2015-01-01 01:00:00+00:00                  0.0                1.0    5461.0   \n",
       "2015-01-01 02:00:00+00:00                  0.0                1.0    5238.0   \n",
       "2015-01-01 03:00:00+00:00                  0.0                1.0    4935.0   \n",
       "2015-01-01 04:00:00+00:00                  2.0                1.0    4618.0   \n",
       "...                                        ...                ...       ...   \n",
       "2018-12-31 18:00:00+00:00                  2.0                0.0    3113.0   \n",
       "2018-12-31 19:00:00+00:00                  1.0                1.0    3288.0   \n",
       "2018-12-31 20:00:00+00:00                  3.0                1.0    3503.0   \n",
       "2018-12-31 21:00:00+00:00                  2.0                1.0    3586.0   \n",
       "2018-12-31 22:00:00+00:00                  2.0                2.0    3651.0   \n",
       "\n",
       "                           wind_speed_Madrid  gen_hyd_pump  \n",
       "time                                                        \n",
       "2015-01-01 00:00:00+00:00                1.0         920.0  \n",
       "2015-01-01 01:00:00+00:00                1.0        1164.0  \n",
       "2015-01-01 02:00:00+00:00                1.0        1503.0  \n",
       "2015-01-01 03:00:00+00:00                1.0        1826.0  \n",
       "2015-01-01 04:00:00+00:00                0.0        2109.0  \n",
       "...                                      ...           ...  \n",
       "2018-12-31 18:00:00+00:00                1.0           1.0  \n",
       "2018-12-31 19:00:00+00:00                1.0           1.0  \n",
       "2018-12-31 20:00:00+00:00                1.0          50.0  \n",
       "2018-12-31 21:00:00+00:00                2.0         108.0  \n",
       "2018-12-31 22:00:00+00:00                1.0         108.0  \n",
       "\n",
       "[35063 rows x 26 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../../data/energy_weather.csv', index_col=0)\n",
    "# https://www.kaggle.com/datasets/nicholasjhana/energy-consumption-generation-prices-and-weather\n",
    "\n",
    "dataset.fillna(0, inplace=True)\n",
    "data = dataset\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3855fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'price_dayahead', 'gen_coal', 'gen_gas', 'load_actual',\n",
       "       'gen_lig', 'gen_oil', 'gen_oth_renew', 'pressure_Barcelona',\n",
       "       'pressure_Bilbao', 'gen_waste', 'gen_bio', 'temp_min_Valencia',\n",
       "       'pressure_Valencia', 'temp_min_Barcelona', 'humidity_Seville',\n",
       "       'wind_deg_Bilbao', 'clouds_all_Bilbao', 'gen_hyd_river',\n",
       "       'wind_deg_Seville', 'wind_speed_Barcelona', 'wind_speed_Valencia',\n",
       "       'wind_speed_Bilbao', 'gen_wind', 'wind_speed_Madrid', 'gen_hyd_pump'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95df4f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(data['pollution'], bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2191f187",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33b340cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reframed.shape: (34980, 2184)\n"
     ]
    }
   ],
   "source": [
    "values = data.values\n",
    "\n",
    "# specify the number of lag hours\n",
    "n_hours = 24*3\n",
    "n_features = data.shape[-1]\n",
    "k = 12\n",
    "split1 = 0.7\n",
    "split2 = 0.85\n",
    "\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(values, n_hours, k)\n",
    "print(\"reframed.shape:\", reframed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e2084b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape (24486, 1872) (24486, 12) (5247, 1872) (5247, 12) (5247, 1872) (5247, 12)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "reframed_values = reframed.values\n",
    "n_train_hours = int(len(reframed_values)*split1)\n",
    "n_valid_hours = int(len(reframed_values)*split2)\n",
    "\n",
    "train = reframed_values[:n_train_hours, :]\n",
    "val = reframed_values[n_train_hours:n_valid_hours, :]\n",
    "test = reframed_values[n_valid_hours:, :]\n",
    "\n",
    "\n",
    "# split into input and outputs\n",
    "n_obs = n_hours * n_features\n",
    "feature_idx = 0\n",
    "train_X, train_y = train[:, :n_obs], train[:, [n_obs + feature_idx + n_features * i for i in range(k)]]\n",
    "val_X, val_y = val[:, :n_obs], val[:, [n_obs + feature_idx + n_features * i for i in range(k)]]\n",
    "test_X, test_y = test[:, :n_obs], test[:, [n_obs + feature_idx + n_features * i for i in range(k)]]\n",
    "\n",
    "\n",
    "print(\"train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape\", \n",
    "      train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1653a1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape (24486, 72, 26) (24486, 12) (5247, 72, 26) (5247, 12) (5247, 72, 26) (5247, 12)\n"
     ]
    }
   ],
   "source": [
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "train_y = scaler.fit_transform(train_y)\n",
    "\n",
    "val_X = scaler.fit_transform(val_X)\n",
    "val_y = scaler.fit_transform(val_y)\n",
    "\n",
    "test_X = scaler.fit_transform(test_X)\n",
    "test_y = scaler.fit_transform(test_y)\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
    "val_X = val_X.reshape((val_X.shape[0], n_hours, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
    "\n",
    "print(\"train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape\", \n",
    "      train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd896d5",
   "metadata": {},
   "source": [
    "### PM threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8885dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24486,)\n",
      "(5247,)\n",
      "(5247,)\n"
     ]
    }
   ],
   "source": [
    "train_X_pm = train_X[:, 0, feature_idx]\n",
    "print(train_X_pm.shape)\n",
    "\n",
    "val_X_pm = val_X[:, 0, feature_idx]\n",
    "print(val_X_pm.shape)\n",
    "\n",
    "test_X_pm = test_X[:, 0, feature_idx]\n",
    "print(test_X_pm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8dfaa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95th Percentile of Daily Rain: 0.7037105569454176\n"
     ]
    }
   ],
   "source": [
    "percentile = 95\n",
    "\n",
    "merged_array = np.concatenate((train_X_pm, val_X_pm, test_X_pm))\n",
    "\n",
    "percentile_pm = np.percentile(merged_array, percentile)\n",
    "\n",
    "print(\"{}th Percentile of Daily Rain:\".format(percentile), percentile_pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402dd2d8",
   "metadata": {},
   "source": [
    "### train_X_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba3913c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(696, 72, 26)\n",
      "(696, 12)\n"
     ]
    }
   ],
   "source": [
    "train_X_extreme = train_X[train_X_pm > percentile_pm]\n",
    "print(train_X_extreme.shape)\n",
    "\n",
    "train_y_extreme = train_y[train_X_pm > percentile_pm]\n",
    "print(train_y_extreme.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52ea31cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23790, 72, 26)\n",
      "(23790, 12)\n"
     ]
    }
   ],
   "source": [
    "train_X_normal = train_X[train_X_pm <= percentile_pm]\n",
    "print(train_X_normal.shape)\n",
    "\n",
    "train_y_normal = train_y[train_X_pm <= percentile_pm]\n",
    "print(train_y_normal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e02929",
   "metadata": {},
   "source": [
    "### val_X_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea19b1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55, 72, 26)\n",
      "(55, 12)\n"
     ]
    }
   ],
   "source": [
    "val_X_extreme = val_X[val_X_pm > percentile_pm]\n",
    "print(val_X_extreme.shape)\n",
    "\n",
    "val_y_extreme = val_y[val_X_pm > percentile_pm]\n",
    "print(val_y_extreme.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b795f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5192, 72, 26)\n",
      "(5192, 12)\n"
     ]
    }
   ],
   "source": [
    "val_X_normal = val_X[val_X_pm <= percentile_pm]\n",
    "print(val_X_normal.shape)\n",
    "\n",
    "val_y_normal = val_y[val_X_pm <= percentile_pm]\n",
    "print(val_y_normal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f8631c",
   "metadata": {},
   "source": [
    "### test_X_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e45ab52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(998, 72, 26)\n",
      "(998, 12)\n"
     ]
    }
   ],
   "source": [
    "test_X_extreme = test_X[test_X_pm > percentile_pm]\n",
    "print(test_X_extreme.shape)\n",
    "\n",
    "test_y_extreme = test_y[test_X_pm > percentile_pm]\n",
    "print(test_y_extreme.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c82a8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4249, 72, 26)\n",
      "(4249, 12)\n"
     ]
    }
   ],
   "source": [
    "test_X_normal = test_X[test_X_pm <= percentile_pm]\n",
    "print(test_X_normal.shape)\n",
    "\n",
    "test_y_normal = test_y[test_X_pm <= percentile_pm]\n",
    "print(test_y_normal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f54127e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "D_f, y_f = train_X, train_y                # Training data (includes both X_rain and X_norm)\n",
    "D_g, y_g = val_X_extreme, val_y_extreme    # Validation data\n",
    "# D_g, y_g = np.concatenate((val_X_extreme, test_X_extreme)), np.concatenate((val_y_extreme, test_y_extreme))    # Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42bd360b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24486, 72, 26)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa5ce3",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5435f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== model parameters ======\n",
    "mlp_unit1 = 128\n",
    "mlp_unit2 = 128\n",
    "mlp_unit3 = 64\n",
    "mlp_unit4 = 64\n",
    "mlp_unit5 = 32\n",
    "mlp_unit6 = 32\n",
    "mlp_unit7 = 16\n",
    "mlp_unit8 = 16\n",
    "dropout = 0  # 0.1\n",
    "kernel_size = 2\n",
    "pool_size = 2\n",
    "learning_rate = 1e-4\n",
    "decay_steps = 10000\n",
    "decay_rate = 0.95\n",
    "PATIENCE = 100\n",
    "EPOCHS = 1000\n",
    "BATCH = 512\n",
    "opt_num = k\n",
    "input_shape = train_X.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0605f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlp_layer(input_shape=input_shape,\n",
    "                   mlp_unit1=mlp_unit1,\n",
    "                   mlp_unit2=mlp_unit2,\n",
    "                   mlp_unit3=mlp_unit3,\n",
    "                   mlp_unit4=mlp_unit4,\n",
    "                   mlp_unit5=mlp_unit5,\n",
    "                   mlp_unit6=mlp_unit6,\n",
    "                   mlp_unit7=mlp_unit7,\n",
    "                   mlp_unit8=mlp_unit8,\n",
    "                   dropout=dropout,\n",
    "                   masked_value=-1,\n",
    "                   opt_num=opt_num\n",
    "                  )\n",
    "# model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mae']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1adc7a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forward function for the model\n",
    "def forward(model, X, y, weights):\n",
    "    predictions = model(X, training=True)\n",
    "    loss = tf.reduce_mean(tf.losses.mean_squared_error(y, predictions) * weights)\n",
    "    return loss, predictions\n",
    "\n",
    "\n",
    "def backward(loss):\n",
    "    # Compute gradients of loss with respect to model parameters\n",
    "    gradients = tf.gradients(loss, model.trainable_variables)\n",
    "    return gradients\n",
    "\n",
    "\n",
    "\n",
    "# Assuming model, D_f, y_f, example_weights, etc. are defined\n",
    "\n",
    "# Define the loss function\n",
    "def calculate_loss(model, X, y, weights):\n",
    "    predictions = model(X)\n",
    "    individual_losses = tf.losses.mean_squared_error(y, predictions)  # This gives a loss per sample\n",
    "    weighted_losses = individual_losses * weights\n",
    "    return tf.reduce_mean(weighted_losses), individual_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d245aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the optimizer for the primary model\n",
    "# optimizer = Adam(learning_rate=0.01)\n",
    "\n",
    "# Initialize the example weights wi with ones\n",
    "# example_weights = tf.ones(shape=(len(D_f)))\n",
    "\n",
    "sample_weights_IPF = np.load('sample_weights_price_IPF.npy')\n",
    "example_weights = tf.convert_to_tensor(sample_weights_IPF, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# Batch size\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8110fba7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 0.000000, Validation Loss = 0.010242 \n",
      "\n",
      "Epoch 2: Training Loss = 0.000000, Validation Loss = 0.010414 \n",
      "\n",
      "Epoch 3: Training Loss = 0.000000, Validation Loss = 0.010424 \n",
      "\n",
      "Epoch 4: Training Loss = 0.000000, Validation Loss = 0.010430 \n",
      "\n",
      "Epoch 5: Training Loss = 0.000000, Validation Loss = 0.010437 \n",
      "\n",
      "Epoch 6: Training Loss = 0.000000, Validation Loss = 0.010441 \n",
      "\n",
      "Epoch 7: Training Loss = 0.000000, Validation Loss = 0.010448 \n",
      "\n",
      "Epoch 8: Training Loss = 0.000000, Validation Loss = 0.010458 \n",
      "\n",
      "Epoch 9: Training Loss = 0.000000, Validation Loss = 0.010467 \n",
      "\n",
      "Epoch 10: Training Loss = 0.000000, Validation Loss = 0.010478 \n",
      "\n",
      "Epoch 11: Training Loss = 0.000000, Validation Loss = 0.010485 \n",
      "\n",
      "Epoch 12: Training Loss = 0.000000, Validation Loss = 0.010491 \n",
      "\n",
      "Epoch 13: Training Loss = 0.000000, Validation Loss = 0.010499 \n",
      "\n",
      "Epoch 14: Training Loss = 0.000000, Validation Loss = 0.010504 \n",
      "\n",
      "Epoch 15: Training Loss = 0.000000, Validation Loss = 0.010507 \n",
      "\n",
      "Epoch 16: Training Loss = 0.000000, Validation Loss = 0.010506 \n",
      "\n",
      "Epoch 17: Training Loss = 0.000000, Validation Loss = 0.010505 \n",
      "\n",
      "Epoch 18: Training Loss = 0.000000, Validation Loss = 0.010511 \n",
      "\n",
      "Epoch 19: Training Loss = 0.000000, Validation Loss = 0.010514 \n",
      "\n",
      "Epoch 20: Training Loss = 0.000000, Validation Loss = 0.010514 \n",
      "\n",
      "Epoch 21: Training Loss = 0.000000, Validation Loss = 0.010514 \n",
      "\n",
      "Epoch 22: Training Loss = 0.000000, Validation Loss = 0.010510 \n",
      "\n",
      "Epoch 23: Training Loss = 0.000000, Validation Loss = 0.010496 \n",
      "\n",
      "Epoch 24: Training Loss = 0.000000, Validation Loss = 0.010494 \n",
      "\n",
      "Epoch 25: Training Loss = 0.000000, Validation Loss = 0.010491 \n",
      "\n",
      "Epoch 26: Training Loss = 0.000000, Validation Loss = 0.010505 \n",
      "\n",
      "Epoch 27: Training Loss = 0.000000, Validation Loss = 0.010513 \n",
      "\n",
      "Epoch 28: Training Loss = 0.000000, Validation Loss = 0.010394 \n",
      "\n",
      "Epoch 29: Training Loss = 0.000000, Validation Loss = 0.010590 \n",
      "\n",
      "Epoch 30: Training Loss = 0.000000, Validation Loss = 0.008168 \n",
      "\n",
      "Epoch 31: Training Loss = 0.000000, Validation Loss = 0.009080 \n",
      "\n",
      "Epoch 32: Training Loss = 0.000000, Validation Loss = 0.006854 \n",
      "\n",
      "Epoch 33: Training Loss = 0.000000, Validation Loss = 0.006600 \n",
      "\n",
      "Epoch 34: Training Loss = 0.000001, Validation Loss = 0.008090 \n",
      "\n",
      "Epoch 35: Training Loss = 0.000001, Validation Loss = 0.009219 \n",
      "\n",
      "Epoch 36: Training Loss = 0.000001, Validation Loss = 0.008023 \n",
      "\n",
      "Epoch 37: Training Loss = 0.000001, Validation Loss = 0.007089 \n",
      "\n",
      "Epoch 38: Training Loss = 0.000001, Validation Loss = 0.008882 \n",
      "\n",
      "Epoch 39: Training Loss = 0.000001, Validation Loss = 0.011445 \n",
      "\n",
      "Epoch 40: Training Loss = 0.000001, Validation Loss = 0.009997 \n",
      "\n",
      "Epoch 41: Training Loss = 0.000001, Validation Loss = 0.015644 \n",
      "\n",
      "Epoch 42: Training Loss = 0.000001, Validation Loss = 0.014834 \n",
      "\n",
      "Epoch 43: Training Loss = 0.000001, Validation Loss = 0.016038 \n",
      "\n",
      "Epoch 44: Training Loss = 0.000001, Validation Loss = 0.014939 \n",
      "\n",
      "Epoch 45: Training Loss = 0.000001, Validation Loss = 0.015200 \n",
      "\n",
      "Epoch 46: Training Loss = 0.000001, Validation Loss = 0.015642 \n",
      "\n",
      "Epoch 47: Training Loss = 0.000001, Validation Loss = 0.014015 \n",
      "\n",
      "Epoch 48: Training Loss = 0.000001, Validation Loss = 0.014616 \n",
      "\n",
      "Epoch 49: Training Loss = 0.000001, Validation Loss = 0.013950 \n",
      "\n",
      "Epoch 50: Training Loss = 0.000001, Validation Loss = 0.014800 \n",
      "\n",
      "Epoch 51: Training Loss = 0.000001, Validation Loss = 0.009095 \n",
      "\n",
      "Epoch 52: Training Loss = 0.000001, Validation Loss = 0.015404 \n",
      "\n",
      "Epoch 53: Training Loss = 0.000001, Validation Loss = 0.010559 \n",
      "\n",
      "Epoch 54: Training Loss = 0.000001, Validation Loss = 0.015552 \n",
      "\n",
      "Epoch 55: Training Loss = 0.000001, Validation Loss = 0.015307 \n",
      "\n",
      "Epoch 56: Training Loss = 0.000001, Validation Loss = 0.014157 \n",
      "\n",
      "Epoch 57: Training Loss = 0.000001, Validation Loss = 0.012129 \n",
      "\n",
      "Epoch 58: Training Loss = 0.000001, Validation Loss = 0.016810 \n",
      "\n",
      "Epoch 59: Training Loss = 0.000001, Validation Loss = 0.014280 \n",
      "\n",
      "Epoch 60: Training Loss = 0.000001, Validation Loss = 0.014899 \n",
      "\n",
      "Epoch 61: Training Loss = 0.000001, Validation Loss = 0.012054 \n",
      "\n",
      "Epoch 62: Training Loss = 0.000001, Validation Loss = 0.012544 \n",
      "\n",
      "Epoch 63: Training Loss = 0.000001, Validation Loss = 0.011859 \n",
      "\n",
      "Epoch 64: Training Loss = 0.000001, Validation Loss = 0.014449 \n",
      "\n",
      "Epoch 65: Training Loss = 0.000001, Validation Loss = 0.011371 \n",
      "\n",
      "Epoch 66: Training Loss = 0.000001, Validation Loss = 0.010623 \n",
      "\n",
      "Epoch 67: Training Loss = 0.000001, Validation Loss = 0.011416 \n",
      "\n",
      "Epoch 68: Training Loss = 0.000001, Validation Loss = 0.009710 \n",
      "\n",
      "Epoch 69: Training Loss = 0.000001, Validation Loss = 0.011899 \n",
      "\n",
      "Epoch 70: Training Loss = 0.000001, Validation Loss = 0.011993 \n",
      "\n",
      "Epoch 71: Training Loss = 0.000001, Validation Loss = 0.011512 \n",
      "\n",
      "Epoch 72: Training Loss = 0.000000, Validation Loss = 0.012716 \n",
      "\n",
      "Epoch 73: Training Loss = 0.000000, Validation Loss = 0.011239 \n",
      "\n",
      "Epoch 74: Training Loss = 0.000000, Validation Loss = 0.011874 \n",
      "\n",
      "Epoch 75: Training Loss = 0.000000, Validation Loss = 0.012492 \n",
      "\n",
      "Epoch 76: Training Loss = 0.000001, Validation Loss = 0.013254 \n",
      "\n",
      "Epoch 77: Training Loss = 0.000001, Validation Loss = 0.013906 \n",
      "\n",
      "Epoch 78: Training Loss = 0.000001, Validation Loss = 0.013947 \n",
      "\n",
      "Epoch 79: Training Loss = 0.000000, Validation Loss = 0.011340 \n",
      "\n",
      "Epoch 80: Training Loss = 0.000001, Validation Loss = 0.013832 \n",
      "\n",
      "Epoch 81: Training Loss = 0.000000, Validation Loss = 0.013821 \n",
      "\n",
      "Epoch 82: Training Loss = 0.000001, Validation Loss = 0.014285 \n",
      "\n",
      "Epoch 83: Training Loss = 0.000000, Validation Loss = 0.010368 \n",
      "\n",
      "Epoch 84: Training Loss = 0.000000, Validation Loss = 0.013699 \n",
      "\n",
      "Epoch 85: Training Loss = 0.000000, Validation Loss = 0.016352 \n",
      "\n",
      "Epoch 86: Training Loss = 0.000000, Validation Loss = 0.014321 \n",
      "\n",
      "Epoch 87: Training Loss = 0.000000, Validation Loss = 0.016512 \n",
      "\n",
      "Epoch 88: Training Loss = 0.000000, Validation Loss = 0.011784 \n",
      "\n",
      "Epoch 89: Training Loss = 0.000000, Validation Loss = 0.018899 \n",
      "\n",
      "Epoch 90: Training Loss = 0.000000, Validation Loss = 0.014968 \n",
      "\n",
      "Epoch 91: Training Loss = 0.000000, Validation Loss = 0.019789 \n",
      "\n",
      "Epoch 92: Training Loss = 0.000000, Validation Loss = 0.015691 \n",
      "\n",
      "Epoch 93: Training Loss = 0.000000, Validation Loss = 0.013541 \n",
      "\n",
      "Epoch 94: Training Loss = 0.000000, Validation Loss = 0.017105 \n",
      "\n",
      "Epoch 95: Training Loss = 0.000000, Validation Loss = 0.016927 \n",
      "\n",
      "Epoch 96: Training Loss = 0.000000, Validation Loss = 0.017589 \n",
      "\n",
      "Epoch 97: Training Loss = 0.000000, Validation Loss = 0.018845 \n",
      "\n",
      "Epoch 98: Training Loss = 0.000000, Validation Loss = 0.018576 \n",
      "\n",
      "Epoch 99: Training Loss = 0.000000, Validation Loss = 0.017378 \n",
      "\n",
      "Epoch 100: Training Loss = 0.000000, Validation Loss = 0.018648 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for t in range(100):  # Number of iterations\n",
    "    # Shuffle the training data and example weights at the beginning of each epoch\n",
    "    indices = np.arange(len(D_f))\n",
    "    #np.random.shuffle(indices)\n",
    "    D_f_shuffled = tf.gather(D_f, indices)\n",
    "    y_f_shuffled = tf.gather(y_f, indices)\n",
    "\n",
    "    \n",
    "    example_weights_shuffled = tf.gather(example_weights, indices)\n",
    "    #print(\"example_weights_shuffled:\", example_weights_shuffled.numpy()[:10])\n",
    "    \n",
    "    \n",
    "    # Convert example_weights_shuffled back to a tf.Variable if it's not one after tf.gather\n",
    "    example_weights_shuffled = tf.Variable(example_weights_shuffled)\n",
    "    #print(\"example_weights_shuffled:\", example_weights_shuffled.numpy()[:10])\n",
    "\n",
    "    \n",
    "    # Iterate over mini-batches\n",
    "    for i in range(0, len(D_f), batch_size):\n",
    "        # Slice the mini-batch\n",
    "        X_batch = D_f_shuffled[i:i+batch_size]\n",
    "        y_batch = y_f_shuffled[i:i+batch_size]\n",
    "        w_batch = example_weights_shuffled[i:i+batch_size]\n",
    "\n",
    "        # Forward pass on the training data with example weights\n",
    "        with tf.GradientTape() as tape:\n",
    "            train_loss_f, _ = forward(model, X_batch, y_batch, w_batch)\n",
    "        gradients_f = tape.gradient(train_loss_f, model.trainable_variables)\n",
    "\n",
    "        # Apply gradients to update the model parameters\n",
    "        model.optimizer.apply_gradients(zip(gradients_f, model.trainable_variables))\n",
    "\n",
    "        \n",
    "    # After processing all mini-batches, evaluate on validation set\n",
    "    val_loss_g, val_predictions_g = forward(model, D_g, y_g, tf.ones(shape=(len(D_g), 1)))\n",
    "\n",
    "    \n",
    "    # Meta-learning step: Compute the influence of training example weights on validation loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Re-compute training loss for shuffled data with current model parameters and example weights\n",
    "        tape.watch(example_weights_shuffled)\n",
    "        train_loss_f, _ = forward(model, D_f_shuffled, y_f_shuffled, example_weights_shuffled)\n",
    "\n",
    "        \n",
    "    # Now compute gradients of this loss w.r.t. the example weights\n",
    "    example_weight_gradients = tape.gradient(train_loss_f, example_weights_shuffled)\n",
    "    \n",
    "    \n",
    "    # Adjust example weights based on gradients: This is a simplistic approach; actual update rule may differ\n",
    "    example_weight_updates = -0.001 * example_weight_gradients\n",
    "    \n",
    "    \n",
    "    # Apply updates to the example weights\n",
    "    example_weights_shuffled.assign_add(example_weight_updates)\n",
    "    \n",
    "    # Ensure example_weights remain non-negative and re-normalize\n",
    "    example_weights_shuffled.assign(tf.maximum(example_weights_shuffled, 0))\n",
    "    example_weights_shuffled.assign(example_weights_shuffled / tf.reduce_sum(example_weights_shuffled))\n",
    "    \n",
    "    \n",
    "    # Update the original example weights\n",
    "    indices_matrix = tf.reshape(indices, (len(indices), 1)) \n",
    "    example_weights = tf.tensor_scatter_nd_update(example_weights, indices_matrix, example_weights_shuffled)\n",
    "    #print(\"example_weights:\", example_weights[:3])\n",
    "    \n",
    "\n",
    "    \n",
    "    # Log the progress\n",
    "    print(f'Epoch {t+1}: Training Loss = {train_loss_f.numpy():.6f}, Validation Loss = {val_loss_g.numpy():.6f} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d922d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_weights = example_weights.numpy()\n",
    "# example_weights /= np.max(example_weights) \n",
    "# example_weights = (example_weights - np.min(example_weights)) / (np.max(example_weights) - np.min(example_weights)) + 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "196a2686",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_weights /= np.max(example_weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d5a711c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.9942e+04, 2.6140e+03, 1.2390e+03, 2.7200e+02, 1.6500e+02,\n",
       "        0.0000e+00, 0.0000e+00, 1.5100e+02, 5.4000e+01, 0.0000e+00,\n",
       "        3.7000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2000e+01]),\n",
       " array([0.05353577, 0.10085899, 0.1481822 , 0.19550541, 0.24282862,\n",
       "        0.29015183, 0.33747503, 0.38479826, 0.43212146, 0.47944468,\n",
       "        0.52676791, 0.57409108, 0.6214143 , 0.66873753, 0.71606076,\n",
       "        0.76338392, 0.81070715, 0.85803038, 0.90535361, 0.95267677,\n",
       "        1.        ]),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(example_weights, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "43622a95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\n",
      "Epoch 1: val_mae improved from inf to 0.06884, saving model to ../../saved_models/energy_all_weighted_META.h5\n",
      "48/48 - 1s - loss: 1.0215e-04 - mae: 0.0273 - val_loss: 0.0084 - val_mae: 0.0688 - 999ms/epoch - 21ms/step\n",
      "Epoch 2/1000\n",
      "\n",
      "Epoch 2: val_mae did not improve from 0.06884\n",
      "48/48 - 0s - loss: 5.2725e-05 - mae: 0.0203 - val_loss: 0.0086 - val_mae: 0.0691 - 304ms/epoch - 6ms/step\n",
      "Epoch 3/1000\n",
      "\n",
      "Epoch 3: val_mae did not improve from 0.06884\n",
      "48/48 - 0s - loss: 4.9162e-05 - mae: 0.0196 - val_loss: 0.0086 - val_mae: 0.0690 - 306ms/epoch - 6ms/step\n",
      "Epoch 4/1000\n",
      "\n",
      "Epoch 4: val_mae did not improve from 0.06884\n",
      "48/48 - 0s - loss: 4.7300e-05 - mae: 0.0192 - val_loss: 0.0086 - val_mae: 0.0691 - 307ms/epoch - 6ms/step\n",
      "Epoch 5/1000\n",
      "\n",
      "Epoch 5: val_mae did not improve from 0.06884\n",
      "48/48 - 0s - loss: 4.5574e-05 - mae: 0.0189 - val_loss: 0.0086 - val_mae: 0.0698 - 301ms/epoch - 6ms/step\n",
      "Epoch 6/1000\n",
      "\n",
      "Epoch 6: val_mae did not improve from 0.06884\n",
      "48/48 - 0s - loss: 4.3809e-05 - mae: 0.0185 - val_loss: 0.0087 - val_mae: 0.0700 - 298ms/epoch - 6ms/step\n",
      "Epoch 7/1000\n",
      "\n",
      "Epoch 7: val_mae did not improve from 0.06884\n",
      "48/48 - 0s - loss: 4.2122e-05 - mae: 0.0182 - val_loss: 0.0087 - val_mae: 0.0700 - 299ms/epoch - 6ms/step\n",
      "Epoch 8/1000\n",
      "\n",
      "Epoch 8: val_mae did not improve from 0.06884\n",
      "48/48 - 0s - loss: 4.0406e-05 - mae: 0.0177 - val_loss: 0.0089 - val_mae: 0.0705 - 299ms/epoch - 6ms/step\n",
      "Epoch 9/1000\n",
      "\n",
      "Epoch 9: val_mae did not improve from 0.06884\n",
      "48/48 - 0s - loss: 3.8977e-05 - mae: 0.0174 - val_loss: 0.0091 - val_mae: 0.0714 - 303ms/epoch - 6ms/step\n",
      "Epoch 10/1000\n",
      "\n",
      "Epoch 10: val_mae did not improve from 0.06884\n",
      "48/48 - 0s - loss: 3.9082e-05 - mae: 0.0174 - val_loss: 0.0092 - val_mae: 0.0719 - 299ms/epoch - 6ms/step\n",
      "Epoch 11/1000\n",
      "\n",
      "Epoch 11: val_mae did not improve from 0.06884\n",
      "48/48 - 0s - loss: 3.7912e-05 - mae: 0.0172 - val_loss: 0.0092 - val_mae: 0.0717 - 301ms/epoch - 6ms/step\n",
      "Epoch 12/1000\n",
      "\n",
      "Epoch 12: val_mae did not improve from 0.06884\n",
      "48/48 - 0s - loss: 3.5912e-05 - mae: 0.0167 - val_loss: 0.0091 - val_mae: 0.0714 - 303ms/epoch - 6ms/step\n",
      "Epoch 13/1000\n",
      "\n",
      "Epoch 13: val_mae did not improve from 0.06884\n",
      "48/48 - 0s - loss: 3.4657e-05 - mae: 0.0164 - val_loss: 0.0092 - val_mae: 0.0721 - 299ms/epoch - 6ms/step\n",
      "Epoch 14/1000\n",
      "\n",
      "Epoch 14: val_mae did not improve from 0.06884\n",
      "48/48 - 0s - loss: 3.2781e-05 - mae: 0.0160 - val_loss: 0.0095 - val_mae: 0.0729 - 302ms/epoch - 6ms/step\n",
      "Epoch 15/1000\n",
      "\n",
      "Epoch 15: val_mae did not improve from 0.06884\n",
      "48/48 - 0s - loss: 3.2538e-05 - mae: 0.0159 - val_loss: 0.0096 - val_mae: 0.0734 - 300ms/epoch - 6ms/step\n",
      "Epoch 16/1000\n",
      "\n",
      "Epoch 16: val_mae did not improve from 0.06884\n",
      "48/48 - 0s - loss: 3.3043e-05 - mae: 0.0160 - val_loss: 0.0096 - val_mae: 0.0737 - 300ms/epoch - 6ms/step\n",
      "Epoch 17/1000\n",
      "\n",
      "Epoch 17: val_mae did not improve from 0.06884\n",
      "48/48 - 0s - loss: 3.4082e-05 - mae: 0.0163 - val_loss: 0.0098 - val_mae: 0.0742 - 299ms/epoch - 6ms/step\n",
      "Epoch 18/1000\n",
      "\n",
      "Epoch 18: val_mae did not improve from 0.06884\n",
      "48/48 - 0s - loss: 3.1587e-05 - mae: 0.0156 - val_loss: 0.0097 - val_mae: 0.0743 - 304ms/epoch - 6ms/step\n",
      "Epoch 19/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 17\u001b[0m\n\u001b[1;32m      7\u001b[0m es \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_mae\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, patience\u001b[38;5;241m=\u001b[39mPATIENCE)\n\u001b[1;32m      8\u001b[0m mc \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../saved_models/energy_all_weighted_META.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      9\u001b[0m                      monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_mae\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     10\u001b[0m                      mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#                      custom_objects={'custom_weight_loss':custom_weight_loss}\u001b[39;00m\n\u001b[1;32m     14\u001b[0m                     )\n\u001b[0;32m---> 17\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m          \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m          \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m          \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample_weights\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m         \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/flood/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/flood/lib/python3.9/site-packages/keras/engine/training.py:1555\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m   1552\u001b[0m     data_handler\u001b[38;5;241m.\u001b[39m_initial_step \u001b[38;5;241m=\u001b[39m data_handler\u001b[38;5;241m.\u001b[39m_initial_step \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1553\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_load_initial_step_from_ckpt()\n\u001b[1;32m   1554\u001b[0m     )\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   1556\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m             epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m             _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m         ):\n\u001b[1;32m   1563\u001b[0m             callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[0;32m~/miniconda3/envs/flood/lib/python3.9/site-packages/keras/engine/data_adapter.py:1374\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1374\u001b[0m original_spe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1375\u001b[0m can_run_full_execution \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1376\u001b[0m     original_spe \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m original_spe\n\u001b[1;32m   1379\u001b[0m )\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[0;32m~/miniconda3/envs/flood/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py:637\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    636\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    639\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/flood/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \n\u001b[1;32m   1136\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/miniconda3/envs/flood/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1122\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1124\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "#               loss=[custom_weight_loss],\n",
    "              loss='mse',\n",
    "              metrics=['mae']\n",
    "             )\n",
    "\n",
    "es = EarlyStopping(monitor='val_mae', mode='min', verbose=2, patience=PATIENCE)\n",
    "mc = ModelCheckpoint('../../saved_models/price_all_weighted_META.h5', \n",
    "                     monitor='val_mae', \n",
    "                     mode='min', \n",
    "                     verbose=2, \n",
    "                     save_best_only=True,\n",
    "#                      custom_objects={'custom_weight_loss':custom_weight_loss}\n",
    "                    )\n",
    "\n",
    "\n",
    "model.fit(train_X, train_y,\n",
    "          validation_data=(val_X, val_y),\n",
    "          epochs=EPOCHS,\n",
    "          batch_size=BATCH,\n",
    "          verbose=2,\n",
    "          shuffle=True,\n",
    "          callbacks=[es, mc],\n",
    "          sample_weight=example_weights\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b313809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae25379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d60633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e325397f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
