{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4843e104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-10 12:27:33.031905: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from math import sqrt\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "from helper import series_to_supervised\n",
    "from model.mlp import mlp_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "460ee40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cb30958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>price_dayahead</th>\n",
       "      <th>gen_coal</th>\n",
       "      <th>gen_gas</th>\n",
       "      <th>load_actual</th>\n",
       "      <th>gen_lig</th>\n",
       "      <th>gen_oil</th>\n",
       "      <th>gen_oth_renew</th>\n",
       "      <th>pressure_Barcelona</th>\n",
       "      <th>pressure_Bilbao</th>\n",
       "      <th>...</th>\n",
       "      <th>wind_deg_Bilbao</th>\n",
       "      <th>clouds_all_Bilbao</th>\n",
       "      <th>gen_hyd_river</th>\n",
       "      <th>wind_deg_Seville</th>\n",
       "      <th>wind_speed_Barcelona</th>\n",
       "      <th>wind_speed_Valencia</th>\n",
       "      <th>wind_speed_Bilbao</th>\n",
       "      <th>gen_wind</th>\n",
       "      <th>wind_speed_Madrid</th>\n",
       "      <th>gen_hyd_pump</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:00:00+00:00</th>\n",
       "      <td>64.92</td>\n",
       "      <td>48.10</td>\n",
       "      <td>4755.0</td>\n",
       "      <td>5196.0</td>\n",
       "      <td>24382.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>...</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5890.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>920.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 01:00:00+00:00</th>\n",
       "      <td>64.48</td>\n",
       "      <td>47.33</td>\n",
       "      <td>4581.0</td>\n",
       "      <td>4857.0</td>\n",
       "      <td>22734.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>...</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5461.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 02:00:00+00:00</th>\n",
       "      <td>59.32</td>\n",
       "      <td>42.27</td>\n",
       "      <td>4131.0</td>\n",
       "      <td>4314.0</td>\n",
       "      <td>21286.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>...</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>949.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5238.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 03:00:00+00:00</th>\n",
       "      <td>56.04</td>\n",
       "      <td>38.41</td>\n",
       "      <td>3840.0</td>\n",
       "      <td>4130.0</td>\n",
       "      <td>20264.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>...</td>\n",
       "      <td>221.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>953.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4935.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1826.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 04:00:00+00:00</th>\n",
       "      <td>53.63</td>\n",
       "      <td>35.72</td>\n",
       "      <td>3590.0</td>\n",
       "      <td>4038.0</td>\n",
       "      <td>19905.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1037.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>...</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>952.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4618.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 18:00:00+00:00</th>\n",
       "      <td>77.02</td>\n",
       "      <td>68.85</td>\n",
       "      <td>2628.0</td>\n",
       "      <td>7634.0</td>\n",
       "      <td>30653.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1027.0</td>\n",
       "      <td>1033.0</td>\n",
       "      <td>...</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1135.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3113.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 19:00:00+00:00</th>\n",
       "      <td>76.16</td>\n",
       "      <td>68.40</td>\n",
       "      <td>2566.0</td>\n",
       "      <td>7241.0</td>\n",
       "      <td>29735.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1027.0</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1172.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3288.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 20:00:00+00:00</th>\n",
       "      <td>74.30</td>\n",
       "      <td>66.88</td>\n",
       "      <td>2422.0</td>\n",
       "      <td>7025.0</td>\n",
       "      <td>28071.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>...</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1148.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3503.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 21:00:00+00:00</th>\n",
       "      <td>69.89</td>\n",
       "      <td>63.93</td>\n",
       "      <td>2293.0</td>\n",
       "      <td>6562.0</td>\n",
       "      <td>25801.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>...</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1128.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3586.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 22:00:00+00:00</th>\n",
       "      <td>69.88</td>\n",
       "      <td>64.27</td>\n",
       "      <td>2166.0</td>\n",
       "      <td>6926.0</td>\n",
       "      <td>24455.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3651.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35063 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           price  price_dayahead  gen_coal  gen_gas  \\\n",
       "time                                                                  \n",
       "2015-01-01 00:00:00+00:00  64.92           48.10    4755.0   5196.0   \n",
       "2015-01-01 01:00:00+00:00  64.48           47.33    4581.0   4857.0   \n",
       "2015-01-01 02:00:00+00:00  59.32           42.27    4131.0   4314.0   \n",
       "2015-01-01 03:00:00+00:00  56.04           38.41    3840.0   4130.0   \n",
       "2015-01-01 04:00:00+00:00  53.63           35.72    3590.0   4038.0   \n",
       "...                          ...             ...       ...      ...   \n",
       "2018-12-31 18:00:00+00:00  77.02           68.85    2628.0   7634.0   \n",
       "2018-12-31 19:00:00+00:00  76.16           68.40    2566.0   7241.0   \n",
       "2018-12-31 20:00:00+00:00  74.30           66.88    2422.0   7025.0   \n",
       "2018-12-31 21:00:00+00:00  69.89           63.93    2293.0   6562.0   \n",
       "2018-12-31 22:00:00+00:00  69.88           64.27    2166.0   6926.0   \n",
       "\n",
       "                           load_actual  gen_lig  gen_oil  gen_oth_renew  \\\n",
       "time                                                                      \n",
       "2015-01-01 00:00:00+00:00      24382.0    328.0    158.0           71.0   \n",
       "2015-01-01 01:00:00+00:00      22734.0    323.0    157.0           73.0   \n",
       "2015-01-01 02:00:00+00:00      21286.0    254.0    160.0           75.0   \n",
       "2015-01-01 03:00:00+00:00      20264.0    187.0    156.0           74.0   \n",
       "2015-01-01 04:00:00+00:00      19905.0    178.0    156.0           74.0   \n",
       "...                                ...      ...      ...            ...   \n",
       "2018-12-31 18:00:00+00:00      30653.0      0.0    178.0           95.0   \n",
       "2018-12-31 19:00:00+00:00      29735.0      0.0    174.0           95.0   \n",
       "2018-12-31 20:00:00+00:00      28071.0      0.0    168.0           94.0   \n",
       "2018-12-31 21:00:00+00:00      25801.0      0.0    163.0           93.0   \n",
       "2018-12-31 22:00:00+00:00      24455.0      0.0    163.0           92.0   \n",
       "\n",
       "                           pressure_Barcelona  pressure_Bilbao  ...  \\\n",
       "time                                                            ...   \n",
       "2015-01-01 00:00:00+00:00              1035.0           1035.0  ...   \n",
       "2015-01-01 01:00:00+00:00              1036.0           1036.0  ...   \n",
       "2015-01-01 02:00:00+00:00              1036.0           1035.0  ...   \n",
       "2015-01-01 03:00:00+00:00              1036.0           1035.0  ...   \n",
       "2015-01-01 04:00:00+00:00              1037.0           1035.0  ...   \n",
       "...                                       ...              ...  ...   \n",
       "2018-12-31 18:00:00+00:00              1027.0           1033.0  ...   \n",
       "2018-12-31 19:00:00+00:00              1027.0           1034.0  ...   \n",
       "2018-12-31 20:00:00+00:00              1028.0           1034.0  ...   \n",
       "2018-12-31 21:00:00+00:00              1028.0           1034.0  ...   \n",
       "2018-12-31 22:00:00+00:00              1028.0           1034.0  ...   \n",
       "\n",
       "                           wind_deg_Bilbao  clouds_all_Bilbao  gen_hyd_river  \\\n",
       "time                                                                           \n",
       "2015-01-01 00:00:00+00:00            229.0                0.0         1009.0   \n",
       "2015-01-01 01:00:00+00:00            224.0                0.0          973.0   \n",
       "2015-01-01 02:00:00+00:00            225.0                0.0          949.0   \n",
       "2015-01-01 03:00:00+00:00            221.0                0.0          953.0   \n",
       "2015-01-01 04:00:00+00:00            224.0                0.0          952.0   \n",
       "...                                    ...                ...            ...   \n",
       "2018-12-31 18:00:00+00:00             57.0                0.0         1135.0   \n",
       "2018-12-31 19:00:00+00:00              0.0                0.0         1172.0   \n",
       "2018-12-31 20:00:00+00:00            140.0                0.0         1148.0   \n",
       "2018-12-31 21:00:00+00:00            120.0                0.0         1128.0   \n",
       "2018-12-31 22:00:00+00:00            100.0                0.0         1069.0   \n",
       "\n",
       "                           wind_deg_Seville  wind_speed_Barcelona  \\\n",
       "time                                                                \n",
       "2015-01-01 00:00:00+00:00              21.0                   7.0   \n",
       "2015-01-01 01:00:00+00:00              27.0                   7.0   \n",
       "2015-01-01 02:00:00+00:00              27.0                   7.0   \n",
       "2015-01-01 03:00:00+00:00              27.0                   7.0   \n",
       "2015-01-01 04:00:00+00:00              57.0                   5.0   \n",
       "...                                     ...                   ...   \n",
       "2018-12-31 18:00:00+00:00              30.0                   1.0   \n",
       "2018-12-31 19:00:00+00:00              30.0                   3.0   \n",
       "2018-12-31 20:00:00+00:00              50.0                   4.0   \n",
       "2018-12-31 21:00:00+00:00              60.0                   5.0   \n",
       "2018-12-31 22:00:00+00:00              50.0                   5.0   \n",
       "\n",
       "                           wind_speed_Valencia  wind_speed_Bilbao  gen_wind  \\\n",
       "time                                                                          \n",
       "2015-01-01 00:00:00+00:00                  1.0                0.0    5890.0   \n",
       "2015-01-01 01:00:00+00:00                  0.0                1.0    5461.0   \n",
       "2015-01-01 02:00:00+00:00                  0.0                1.0    5238.0   \n",
       "2015-01-01 03:00:00+00:00                  0.0                1.0    4935.0   \n",
       "2015-01-01 04:00:00+00:00                  2.0                1.0    4618.0   \n",
       "...                                        ...                ...       ...   \n",
       "2018-12-31 18:00:00+00:00                  2.0                0.0    3113.0   \n",
       "2018-12-31 19:00:00+00:00                  1.0                1.0    3288.0   \n",
       "2018-12-31 20:00:00+00:00                  3.0                1.0    3503.0   \n",
       "2018-12-31 21:00:00+00:00                  2.0                1.0    3586.0   \n",
       "2018-12-31 22:00:00+00:00                  2.0                2.0    3651.0   \n",
       "\n",
       "                           wind_speed_Madrid  gen_hyd_pump  \n",
       "time                                                        \n",
       "2015-01-01 00:00:00+00:00                1.0         920.0  \n",
       "2015-01-01 01:00:00+00:00                1.0        1164.0  \n",
       "2015-01-01 02:00:00+00:00                1.0        1503.0  \n",
       "2015-01-01 03:00:00+00:00                1.0        1826.0  \n",
       "2015-01-01 04:00:00+00:00                0.0        2109.0  \n",
       "...                                      ...           ...  \n",
       "2018-12-31 18:00:00+00:00                1.0           1.0  \n",
       "2018-12-31 19:00:00+00:00                1.0           1.0  \n",
       "2018-12-31 20:00:00+00:00                1.0          50.0  \n",
       "2018-12-31 21:00:00+00:00                2.0         108.0  \n",
       "2018-12-31 22:00:00+00:00                1.0         108.0  \n",
       "\n",
       "[35063 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../../data/energy_weather.csv', index_col=0)\n",
    "# https://www.kaggle.com/datasets/nicholasjhana/energy-consumption-generation-prices-and-weather\n",
    "\n",
    "dataset.fillna(0, inplace=True)\n",
    "data = dataset\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3855fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'price_dayahead', 'gen_coal', 'gen_gas', 'load_actual',\n",
       "       'gen_lig', 'gen_oil', 'gen_oth_renew', 'pressure_Barcelona',\n",
       "       'pressure_Bilbao', 'gen_waste', 'gen_bio', 'temp_min_Valencia',\n",
       "       'pressure_Valencia', 'temp_min_Barcelona', 'humidity_Seville',\n",
       "       'wind_deg_Bilbao', 'clouds_all_Bilbao', 'gen_hyd_river',\n",
       "       'wind_deg_Seville', 'wind_speed_Barcelona', 'wind_speed_Valencia',\n",
       "       'wind_speed_Bilbao', 'gen_wind', 'wind_speed_Madrid', 'gen_hyd_pump'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2191f187",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33b340cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reframed.shape: (34980, 2184)\n"
     ]
    }
   ],
   "source": [
    "values = data.values\n",
    "\n",
    "# specify the number of lag hours\n",
    "n_hours = 24*3\n",
    "n_features = data.shape[-1]\n",
    "k = 12\n",
    "split1 = 0.7\n",
    "split2 = 0.85\n",
    "\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(values, n_hours, k)\n",
    "print(\"reframed.shape:\", reframed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e2084b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape (24486, 1872) (24486, 12) (5247, 1872) (5247, 12) (5247, 1872) (5247, 12)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "reframed_values = reframed.values\n",
    "n_train_hours = int(len(reframed_values)*split1)\n",
    "n_valid_hours = int(len(reframed_values)*split2)\n",
    "\n",
    "train = reframed_values[:n_train_hours, :]\n",
    "val = reframed_values[n_train_hours:n_valid_hours, :]\n",
    "test = reframed_values[n_valid_hours:, :]\n",
    "\n",
    "\n",
    "# split into input and outputs\n",
    "n_obs = n_hours * n_features\n",
    "feature_idx = 0\n",
    "train_X, train_y = train[:, :n_obs], train[:, [n_obs + feature_idx + n_features * i for i in range(k)]]\n",
    "val_X, val_y = val[:, :n_obs], val[:, [n_obs + feature_idx + n_features * i for i in range(k)]]\n",
    "test_X, test_y = test[:, :n_obs], test[:, [n_obs + feature_idx + n_features * i for i in range(k)]]\n",
    "\n",
    "\n",
    "print(\"train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape\", \n",
    "      train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1653a1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape (24486, 72, 26) (24486, 12) (5247, 72, 26) (5247, 12) (5247, 72, 26) (5247, 12)\n"
     ]
    }
   ],
   "source": [
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "train_y = scaler.fit_transform(train_y)\n",
    "\n",
    "val_X = scaler.fit_transform(val_X)\n",
    "val_y = scaler.fit_transform(val_y)\n",
    "\n",
    "test_X = scaler.fit_transform(test_X)\n",
    "test_y = scaler.fit_transform(test_y)\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
    "val_X = val_X.reshape((val_X.shape[0], n_hours, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
    "\n",
    "print(\"train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape\", \n",
    "      train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd896d5",
   "metadata": {},
   "source": [
    "### PM threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8885dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24486,)\n",
      "(5247,)\n",
      "(5247,)\n"
     ]
    }
   ],
   "source": [
    "train_X_pm = train_X[:, 0, feature_idx]\n",
    "print(train_X_pm.shape)\n",
    "\n",
    "val_X_pm = val_X[:, 0, feature_idx]\n",
    "print(val_X_pm.shape)\n",
    "\n",
    "test_X_pm = test_X[:, 0, feature_idx]\n",
    "print(test_X_pm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f154878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_nums = 20\n",
    "\n",
    "# Step 1: Calculate the histogram\n",
    "counts, bin_edges = np.histogram(train_X_pm, bins=bin_nums)\n",
    "\n",
    "# Step 2: Invert counts to assign lower weights to more frequent bins, avoid division by zero by adding a small number (epsilon)\n",
    "epsilon = 1e-8\n",
    "weights = np.sqrt(1.0 / (counts + epsilon))\n",
    "# weights = 1.0 / (counts + epsilon)\n",
    "\n",
    "# Step 3: Normalize the weights (optional)\n",
    "# weights /= np.max(weights)\n",
    "weights /= np.sum(weights) * len(counts)\n",
    "\n",
    "# Step 4: Assign weights to each sample based on the bin it falls into\n",
    "sample_weights = np.zeros(train_X_pm.shape[0])\n",
    "\n",
    "for i, value in enumerate(train_X_pm):\n",
    "    \n",
    "    # Find the index of the bin this sample falls into\n",
    "    bin_index = np.digitize(value, bin_edges) - 1\n",
    "    bin_index = min(bin_index, bin_nums - 1)\n",
    "    \n",
    "    # Assign the corresponding weight\n",
    "    sample_weights[i] = weights[bin_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba3913c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24486,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b70ef2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_weights /= np.sum(sample_weights)\n",
    "# sample_weights /= np.max(sample_weights)\n",
    "# sample_weights = (sample_weights - sample_weights.min()) / (sample_weights.max()-sample_weights.min()) + 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69798e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.9942e+04, 2.6140e+03, 1.2390e+03, 2.7200e+02, 1.6500e+02,\n",
       "        0.0000e+00, 0.0000e+00, 1.5100e+02, 5.4000e+01, 0.0000e+00,\n",
       "        3.7000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2000e+01]),\n",
       " array([0.00057706, 0.0010838 , 0.00159055, 0.00209729, 0.00260403,\n",
       "        0.00311078, 0.00361752, 0.00412427, 0.00463101, 0.00513775,\n",
       "        0.0056445 , 0.00615124, 0.00665798, 0.00716473, 0.00767147,\n",
       "        0.00817822, 0.00868496, 0.0091917 , 0.00969845, 0.01020519,\n",
       "        0.01071193]),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sample_weights, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "85808ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('sample_weights_price_IPF.npy', sample_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa5ce3",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5435f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== model parameters ======\n",
    "mlp_unit1 = 128\n",
    "mlp_unit2 = 128\n",
    "mlp_unit3 = 64\n",
    "mlp_unit4 = 64\n",
    "mlp_unit5 = 32\n",
    "mlp_unit6 = 32\n",
    "mlp_unit7 = 16\n",
    "mlp_unit8 = 16\n",
    "dropout = 0.0\n",
    "kernel_size = 2\n",
    "pool_size = 2\n",
    "learning_rate = 1e-4\n",
    "decay_steps = 10000\n",
    "decay_rate = 0.95\n",
    "PATIENCE = 100\n",
    "EPOCHS = 1000\n",
    "BATCH = 512\n",
    "opt_num = k\n",
    "input_shape = train_X.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0605f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlp_layer(input_shape=input_shape,\n",
    "                   mlp_unit1=mlp_unit1,\n",
    "                   mlp_unit2=mlp_unit2,\n",
    "                   mlp_unit3=mlp_unit3,\n",
    "                   mlp_unit4=mlp_unit4,\n",
    "                   mlp_unit5=mlp_unit5,\n",
    "                   mlp_unit6=mlp_unit6,\n",
    "                   mlp_unit7=mlp_unit7,\n",
    "                   mlp_unit8=mlp_unit8,\n",
    "                   dropout=dropout,\n",
    "                   masked_value=-1,\n",
    "                   opt_num=opt_num\n",
    "                  )\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43622a95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "48/48 - 2s - loss: 4.6506e-05 - mae: 0.1639 - val_loss: 0.0144 - val_mae: 0.0943\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 0.09432, saving model to ../../saved_models/energy_all_weighted_IPF_95.h5\n",
      "Epoch 2/1000\n",
      "48/48 - 1s - loss: 1.2088e-05 - mae: 0.0848 - val_loss: 0.0140 - val_mae: 0.0888\n",
      "\n",
      "Epoch 00002: val_mae improved from 0.09432 to 0.08879, saving model to ../../saved_models/energy_all_weighted_IPF_95.h5\n",
      "Epoch 3/1000\n",
      "48/48 - 1s - loss: 9.2641e-06 - mae: 0.0763 - val_loss: 0.0131 - val_mae: 0.0853\n",
      "\n",
      "Epoch 00003: val_mae improved from 0.08879 to 0.08526, saving model to ../../saved_models/energy_all_weighted_IPF_95.h5\n",
      "Epoch 4/1000\n",
      "48/48 - 1s - loss: 8.0349e-06 - mae: 0.0714 - val_loss: 0.0122 - val_mae: 0.0824\n",
      "\n",
      "Epoch 00004: val_mae improved from 0.08526 to 0.08245, saving model to ../../saved_models/energy_all_weighted_IPF_95.h5\n",
      "Epoch 5/1000\n",
      "48/48 - 1s - loss: 6.8183e-06 - mae: 0.0655 - val_loss: 0.0080 - val_mae: 0.0675\n",
      "\n",
      "Epoch 00005: val_mae improved from 0.08245 to 0.06749, saving model to ../../saved_models/energy_all_weighted_IPF_95.h5\n",
      "Epoch 6/1000\n",
      "48/48 - 1s - loss: 5.1694e-06 - mae: 0.0567 - val_loss: 0.0064 - val_mae: 0.0598\n",
      "\n",
      "Epoch 00006: val_mae improved from 0.06749 to 0.05980, saving model to ../../saved_models/energy_all_weighted_IPF_95.h5\n",
      "Epoch 7/1000\n",
      "48/48 - 1s - loss: 4.1677e-06 - mae: 0.0508 - val_loss: 0.0065 - val_mae: 0.0600\n",
      "\n",
      "Epoch 00007: val_mae did not improve from 0.05980\n",
      "Epoch 8/1000\n",
      "48/48 - 1s - loss: 3.8887e-06 - mae: 0.0493 - val_loss: 0.0056 - val_mae: 0.0558\n",
      "\n",
      "Epoch 00008: val_mae improved from 0.05980 to 0.05581, saving model to ../../saved_models/energy_all_weighted_IPF_95.h5\n",
      "Epoch 9/1000\n",
      "48/48 - 1s - loss: 3.6282e-06 - mae: 0.0475 - val_loss: 0.0055 - val_mae: 0.0559\n",
      "\n",
      "Epoch 00009: val_mae did not improve from 0.05581\n",
      "Epoch 10/1000\n",
      "48/48 - 1s - loss: 3.4593e-06 - mae: 0.0465 - val_loss: 0.0055 - val_mae: 0.0554\n",
      "\n",
      "Epoch 00010: val_mae improved from 0.05581 to 0.05540, saving model to ../../saved_models/energy_all_weighted_IPF_95.h5\n",
      "Epoch 11/1000\n",
      "48/48 - 1s - loss: 3.4078e-06 - mae: 0.0463 - val_loss: 0.0057 - val_mae: 0.0566\n",
      "\n",
      "Epoch 00011: val_mae did not improve from 0.05540\n",
      "Epoch 12/1000\n",
      "48/48 - 1s - loss: 3.2889e-06 - mae: 0.0453 - val_loss: 0.0050 - val_mae: 0.0527\n",
      "\n",
      "Epoch 00012: val_mae improved from 0.05540 to 0.05272, saving model to ../../saved_models/energy_all_weighted_IPF_95.h5\n",
      "Epoch 13/1000\n",
      "48/48 - 1s - loss: 3.2263e-06 - mae: 0.0450 - val_loss: 0.0054 - val_mae: 0.0545\n",
      "\n",
      "Epoch 00013: val_mae did not improve from 0.05272\n",
      "Epoch 14/1000\n",
      "48/48 - 1s - loss: 3.1248e-06 - mae: 0.0443 - val_loss: 0.0049 - val_mae: 0.0518\n",
      "\n",
      "Epoch 00014: val_mae improved from 0.05272 to 0.05175, saving model to ../../saved_models/energy_all_weighted_IPF_95.h5\n",
      "Epoch 15/1000\n",
      "48/48 - 1s - loss: 3.1042e-06 - mae: 0.0441 - val_loss: 0.0048 - val_mae: 0.0512\n",
      "\n",
      "Epoch 00015: val_mae improved from 0.05175 to 0.05123, saving model to ../../saved_models/energy_all_weighted_IPF_95.h5\n",
      "Epoch 16/1000\n",
      "48/48 - 1s - loss: 3.1053e-06 - mae: 0.0442 - val_loss: 0.0051 - val_mae: 0.0544\n",
      "\n",
      "Epoch 00016: val_mae did not improve from 0.05123\n",
      "Epoch 17/1000\n",
      "48/48 - 1s - loss: 2.9486e-06 - mae: 0.0430 - val_loss: 0.0049 - val_mae: 0.0522\n",
      "\n",
      "Epoch 00017: val_mae did not improve from 0.05123\n",
      "Epoch 18/1000\n",
      "48/48 - 1s - loss: 2.8730e-06 - mae: 0.0424 - val_loss: 0.0048 - val_mae: 0.0512\n",
      "\n",
      "Epoch 00018: val_mae improved from 0.05123 to 0.05122, saving model to ../../saved_models/energy_all_weighted_IPF_95.h5\n",
      "Epoch 19/1000\n",
      "48/48 - 1s - loss: 2.8154e-06 - mae: 0.0419 - val_loss: 0.0046 - val_mae: 0.0502\n",
      "\n",
      "Epoch 00019: val_mae improved from 0.05122 to 0.05025, saving model to ../../saved_models/energy_all_weighted_IPF_95.h5\n",
      "Epoch 20/1000\n",
      "48/48 - 1s - loss: 2.7796e-06 - mae: 0.0418 - val_loss: 0.0046 - val_mae: 0.0503\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 0.05025\n",
      "Epoch 21/1000\n",
      "48/48 - 1s - loss: 2.8292e-06 - mae: 0.0422 - val_loss: 0.0047 - val_mae: 0.0511\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 0.05025\n",
      "Epoch 22/1000\n",
      "48/48 - 1s - loss: 2.7450e-06 - mae: 0.0416 - val_loss: 0.0048 - val_mae: 0.0524\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 0.05025\n",
      "Epoch 23/1000\n",
      "48/48 - 1s - loss: 2.6757e-06 - mae: 0.0410 - val_loss: 0.0049 - val_mae: 0.0517\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 0.05025\n",
      "Epoch 24/1000\n",
      "48/48 - 1s - loss: 2.6528e-06 - mae: 0.0408 - val_loss: 0.0045 - val_mae: 0.0499\n",
      "\n",
      "Epoch 00024: val_mae improved from 0.05025 to 0.04992, saving model to ../../saved_models/energy_all_weighted_IPF_95.h5\n",
      "Epoch 25/1000\n",
      "48/48 - 1s - loss: 2.6658e-06 - mae: 0.0410 - val_loss: 0.0045 - val_mae: 0.0497\n",
      "\n",
      "Epoch 00025: val_mae improved from 0.04992 to 0.04965, saving model to ../../saved_models/energy_all_weighted_IPF_95.h5\n",
      "Epoch 26/1000\n",
      "48/48 - 1s - loss: 2.5695e-06 - mae: 0.0401 - val_loss: 0.0044 - val_mae: 0.0490\n",
      "\n",
      "Epoch 00026: val_mae improved from 0.04965 to 0.04903, saving model to ../../saved_models/energy_all_weighted_IPF_95.h5\n",
      "Epoch 27/1000\n",
      "48/48 - 1s - loss: 2.5253e-06 - mae: 0.0398 - val_loss: 0.0044 - val_mae: 0.0491\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 0.04903\n",
      "Epoch 28/1000\n",
      "48/48 - 1s - loss: 2.4874e-06 - mae: 0.0396 - val_loss: 0.0046 - val_mae: 0.0500\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 0.04903\n",
      "Epoch 29/1000\n",
      "48/48 - 1s - loss: 2.5929e-06 - mae: 0.0407 - val_loss: 0.0045 - val_mae: 0.0498\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 0.04903\n",
      "Epoch 30/1000\n",
      "48/48 - 1s - loss: 2.4817e-06 - mae: 0.0397 - val_loss: 0.0052 - val_mae: 0.0553\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 0.04903\n",
      "Epoch 31/1000\n",
      "48/48 - 1s - loss: 2.5063e-06 - mae: 0.0400 - val_loss: 0.0044 - val_mae: 0.0489\n",
      "\n",
      "Epoch 00031: val_mae improved from 0.04903 to 0.04893, saving model to ../../saved_models/energy_all_weighted_IPF_95.h5\n",
      "Epoch 32/1000\n",
      "48/48 - 1s - loss: 2.3692e-06 - mae: 0.0387 - val_loss: 0.0044 - val_mae: 0.0487\n",
      "\n",
      "Epoch 00032: val_mae improved from 0.04893 to 0.04869, saving model to ../../saved_models/energy_all_weighted_IPF_95.h5\n",
      "Epoch 33/1000\n",
      "48/48 - 1s - loss: 2.3623e-06 - mae: 0.0387 - val_loss: 0.0045 - val_mae: 0.0502\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 0.04869\n",
      "Epoch 34/1000\n",
      "48/48 - 1s - loss: 2.3789e-06 - mae: 0.0390 - val_loss: 0.0049 - val_mae: 0.0529\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 0.04869\n",
      "Epoch 35/1000\n",
      "48/48 - 1s - loss: 2.3797e-06 - mae: 0.0390 - val_loss: 0.0044 - val_mae: 0.0487\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 0.04869\n",
      "Epoch 36/1000\n",
      "48/48 - 1s - loss: 2.3365e-06 - mae: 0.0387 - val_loss: 0.0048 - val_mae: 0.0527\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 0.04869\n",
      "Epoch 37/1000\n",
      "48/48 - 1s - loss: 2.2379e-06 - mae: 0.0377 - val_loss: 0.0048 - val_mae: 0.0524\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 0.04869\n",
      "Epoch 38/1000\n",
      "48/48 - 1s - loss: 2.5045e-06 - mae: 0.0405 - val_loss: 0.0045 - val_mae: 0.0500\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 0.04869\n",
      "Epoch 39/1000\n",
      "48/48 - 1s - loss: 2.2537e-06 - mae: 0.0381 - val_loss: 0.0045 - val_mae: 0.0495\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 0.04869\n",
      "Epoch 40/1000\n",
      "48/48 - 1s - loss: 2.1900e-06 - mae: 0.0375 - val_loss: 0.0045 - val_mae: 0.0498\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 0.04869\n",
      "Epoch 41/1000\n",
      "48/48 - 1s - loss: 2.1367e-06 - mae: 0.0371 - val_loss: 0.0044 - val_mae: 0.0496\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 0.04869\n",
      "Epoch 42/1000\n",
      "48/48 - 1s - loss: 2.1204e-06 - mae: 0.0370 - val_loss: 0.0044 - val_mae: 0.0490\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 0.04869\n",
      "Epoch 43/1000\n",
      "48/48 - 1s - loss: 2.1507e-06 - mae: 0.0373 - val_loss: 0.0048 - val_mae: 0.0523\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 0.04869\n",
      "Epoch 44/1000\n",
      "48/48 - 1s - loss: 2.1549e-06 - mae: 0.0375 - val_loss: 0.0045 - val_mae: 0.0498\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 0.04869\n",
      "Epoch 45/1000\n",
      "48/48 - 1s - loss: 2.0355e-06 - mae: 0.0364 - val_loss: 0.0045 - val_mae: 0.0501\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 0.04869\n",
      "Epoch 46/1000\n",
      "48/48 - 1s - loss: 2.0805e-06 - mae: 0.0369 - val_loss: 0.0053 - val_mae: 0.0556\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 0.04869\n",
      "Epoch 47/1000\n",
      "48/48 - 1s - loss: 1.9896e-06 - mae: 0.0360 - val_loss: 0.0046 - val_mae: 0.0503\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 0.04869\n",
      "Epoch 48/1000\n",
      "48/48 - 1s - loss: 2.0198e-06 - mae: 0.0365 - val_loss: 0.0044 - val_mae: 0.0495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00048: val_mae did not improve from 0.04869\n",
      "Epoch 49/1000\n",
      "48/48 - 1s - loss: 1.9506e-06 - mae: 0.0358 - val_loss: 0.0049 - val_mae: 0.0527\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 0.04869\n",
      "Epoch 50/1000\n",
      "48/48 - 1s - loss: 1.9357e-06 - mae: 0.0357 - val_loss: 0.0048 - val_mae: 0.0519\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 0.04869\n",
      "Epoch 51/1000\n",
      "48/48 - 1s - loss: 1.9036e-06 - mae: 0.0355 - val_loss: 0.0047 - val_mae: 0.0513\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 0.04869\n",
      "Epoch 52/1000\n",
      "48/48 - 1s - loss: 1.8603e-06 - mae: 0.0351 - val_loss: 0.0045 - val_mae: 0.0499\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 0.04869\n",
      "Epoch 53/1000\n",
      "48/48 - 1s - loss: 1.8795e-06 - mae: 0.0354 - val_loss: 0.0046 - val_mae: 0.0503\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 0.04869\n",
      "Epoch 54/1000\n",
      "48/48 - 1s - loss: 1.9189e-06 - mae: 0.0358 - val_loss: 0.0046 - val_mae: 0.0506\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 0.04869\n",
      "Epoch 55/1000\n",
      "48/48 - 1s - loss: 1.8964e-06 - mae: 0.0356 - val_loss: 0.0050 - val_mae: 0.0536\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 0.04869\n",
      "Epoch 56/1000\n",
      "48/48 - 1s - loss: 1.7867e-06 - mae: 0.0346 - val_loss: 0.0050 - val_mae: 0.0537\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 0.04869\n",
      "Epoch 57/1000\n",
      "48/48 - 1s - loss: 1.7550e-06 - mae: 0.0342 - val_loss: 0.0049 - val_mae: 0.0525\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 0.04869\n",
      "Epoch 58/1000\n",
      "48/48 - 1s - loss: 1.7382e-06 - mae: 0.0342 - val_loss: 0.0052 - val_mae: 0.0552\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 0.04869\n",
      "Epoch 59/1000\n",
      "48/48 - 1s - loss: 1.7112e-06 - mae: 0.0339 - val_loss: 0.0050 - val_mae: 0.0536\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 0.04869\n",
      "Epoch 60/1000\n",
      "48/48 - 1s - loss: 1.6746e-06 - mae: 0.0336 - val_loss: 0.0049 - val_mae: 0.0526\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 0.04869\n",
      "Epoch 61/1000\n",
      "48/48 - 1s - loss: 1.6788e-06 - mae: 0.0337 - val_loss: 0.0054 - val_mae: 0.0560\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 0.04869\n",
      "Epoch 62/1000\n",
      "48/48 - 1s - loss: 1.6659e-06 - mae: 0.0337 - val_loss: 0.0053 - val_mae: 0.0554\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 0.04869\n",
      "Epoch 63/1000\n",
      "48/48 - 1s - loss: 1.6182e-06 - mae: 0.0332 - val_loss: 0.0049 - val_mae: 0.0525\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 0.04869\n",
      "Epoch 64/1000\n",
      "48/48 - 1s - loss: 1.5922e-06 - mae: 0.0330 - val_loss: 0.0054 - val_mae: 0.0562\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 0.04869\n",
      "Epoch 65/1000\n",
      "48/48 - 1s - loss: 1.5811e-06 - mae: 0.0329 - val_loss: 0.0049 - val_mae: 0.0524\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 0.04869\n",
      "Epoch 66/1000\n",
      "48/48 - 1s - loss: 1.7160e-06 - mae: 0.0344 - val_loss: 0.0049 - val_mae: 0.0523\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 0.04869\n",
      "Epoch 67/1000\n",
      "48/48 - 1s - loss: 1.6165e-06 - mae: 0.0334 - val_loss: 0.0050 - val_mae: 0.0533\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 0.04869\n",
      "Epoch 68/1000\n",
      "48/48 - 1s - loss: 1.5800e-06 - mae: 0.0330 - val_loss: 0.0055 - val_mae: 0.0571\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 0.04869\n",
      "Epoch 69/1000\n",
      "48/48 - 1s - loss: 1.5105e-06 - mae: 0.0323 - val_loss: 0.0050 - val_mae: 0.0531\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 0.04869\n",
      "Epoch 70/1000\n",
      "48/48 - 1s - loss: 1.4679e-06 - mae: 0.0318 - val_loss: 0.0051 - val_mae: 0.0540\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 0.04869\n",
      "Epoch 71/1000\n",
      "48/48 - 1s - loss: 1.4473e-06 - mae: 0.0316 - val_loss: 0.0051 - val_mae: 0.0535\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 0.04869\n",
      "Epoch 72/1000\n",
      "48/48 - 1s - loss: 1.5307e-06 - mae: 0.0326 - val_loss: 0.0058 - val_mae: 0.0584\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 0.04869\n",
      "Epoch 73/1000\n",
      "48/48 - 1s - loss: 1.4400e-06 - mae: 0.0316 - val_loss: 0.0055 - val_mae: 0.0563\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 0.04869\n",
      "Epoch 74/1000\n",
      "48/48 - 1s - loss: 1.3883e-06 - mae: 0.0311 - val_loss: 0.0055 - val_mae: 0.0566\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 0.04869\n",
      "Epoch 75/1000\n",
      "48/48 - 1s - loss: 1.4182e-06 - mae: 0.0315 - val_loss: 0.0053 - val_mae: 0.0547\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 0.04869\n",
      "Epoch 76/1000\n",
      "48/48 - 1s - loss: 1.3670e-06 - mae: 0.0309 - val_loss: 0.0057 - val_mae: 0.0573\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 0.04869\n",
      "Epoch 77/1000\n",
      "48/48 - 1s - loss: 1.3909e-06 - mae: 0.0313 - val_loss: 0.0057 - val_mae: 0.0573\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 0.04869\n",
      "Epoch 78/1000\n",
      "48/48 - 1s - loss: 1.3180e-06 - mae: 0.0304 - val_loss: 0.0054 - val_mae: 0.0549\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 0.04869\n",
      "Epoch 79/1000\n",
      "48/48 - 1s - loss: 1.3113e-06 - mae: 0.0303 - val_loss: 0.0055 - val_mae: 0.0554\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 0.04869\n",
      "Epoch 80/1000\n",
      "48/48 - 1s - loss: 1.3551e-06 - mae: 0.0309 - val_loss: 0.0057 - val_mae: 0.0572\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 0.04869\n",
      "Epoch 81/1000\n",
      "48/48 - 1s - loss: 1.3024e-06 - mae: 0.0303 - val_loss: 0.0057 - val_mae: 0.0569\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 0.04869\n",
      "Epoch 82/1000\n",
      "48/48 - 1s - loss: 1.2817e-06 - mae: 0.0301 - val_loss: 0.0058 - val_mae: 0.0578\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 0.04869\n",
      "Epoch 83/1000\n",
      "48/48 - 1s - loss: 1.2782e-06 - mae: 0.0301 - val_loss: 0.0058 - val_mae: 0.0579\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 0.04869\n",
      "Epoch 84/1000\n",
      "48/48 - 1s - loss: 1.2214e-06 - mae: 0.0294 - val_loss: 0.0062 - val_mae: 0.0600\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 0.04869\n",
      "Epoch 85/1000\n",
      "48/48 - 1s - loss: 1.2225e-06 - mae: 0.0294 - val_loss: 0.0057 - val_mae: 0.0568\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 0.04869\n",
      "Epoch 86/1000\n",
      "48/48 - 1s - loss: 1.2192e-06 - mae: 0.0294 - val_loss: 0.0060 - val_mae: 0.0587\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 0.04869\n",
      "Epoch 87/1000\n",
      "48/48 - 1s - loss: 1.2309e-06 - mae: 0.0296 - val_loss: 0.0057 - val_mae: 0.0568\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 0.04869\n",
      "Epoch 88/1000\n",
      "48/48 - 1s - loss: 1.1972e-06 - mae: 0.0291 - val_loss: 0.0061 - val_mae: 0.0592\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 0.04869\n",
      "Epoch 89/1000\n",
      "48/48 - 1s - loss: 1.1895e-06 - mae: 0.0290 - val_loss: 0.0062 - val_mae: 0.0601\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 0.04869\n",
      "Epoch 90/1000\n",
      "48/48 - 1s - loss: 1.1455e-06 - mae: 0.0285 - val_loss: 0.0060 - val_mae: 0.0585\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 0.04869\n",
      "Epoch 91/1000\n",
      "48/48 - 1s - loss: 1.1804e-06 - mae: 0.0290 - val_loss: 0.0059 - val_mae: 0.0577\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 0.04869\n",
      "Epoch 92/1000\n",
      "48/48 - 1s - loss: 1.1264e-06 - mae: 0.0283 - val_loss: 0.0061 - val_mae: 0.0591\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 0.04869\n",
      "Epoch 93/1000\n",
      "48/48 - 1s - loss: 1.0835e-06 - mae: 0.0278 - val_loss: 0.0063 - val_mae: 0.0603\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 0.04869\n",
      "Epoch 94/1000\n",
      "48/48 - 1s - loss: 1.1308e-06 - mae: 0.0284 - val_loss: 0.0061 - val_mae: 0.0588\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 0.04869\n",
      "Epoch 95/1000\n",
      "48/48 - 1s - loss: 1.1255e-06 - mae: 0.0284 - val_loss: 0.0060 - val_mae: 0.0586\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 0.04869\n",
      "Epoch 96/1000\n",
      "48/48 - 1s - loss: 1.1199e-06 - mae: 0.0282 - val_loss: 0.0060 - val_mae: 0.0582\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 0.04869\n",
      "Epoch 97/1000\n",
      "48/48 - 1s - loss: 1.0926e-06 - mae: 0.0279 - val_loss: 0.0063 - val_mae: 0.0604\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 0.04869\n",
      "Epoch 98/1000\n",
      "48/48 - 1s - loss: 1.1049e-06 - mae: 0.0281 - val_loss: 0.0064 - val_mae: 0.0607\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 0.04869\n",
      "Epoch 99/1000\n",
      "48/48 - 1s - loss: 1.0283e-06 - mae: 0.0271 - val_loss: 0.0059 - val_mae: 0.0578\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 0.04869\n",
      "Epoch 100/1000\n",
      "48/48 - 1s - loss: 1.0510e-06 - mae: 0.0274 - val_loss: 0.0060 - val_mae: 0.0583\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 0.04869\n",
      "Epoch 101/1000\n",
      "48/48 - 1s - loss: 1.0252e-06 - mae: 0.0270 - val_loss: 0.0065 - val_mae: 0.0615\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 0.04869\n",
      "Epoch 102/1000\n",
      "48/48 - 1s - loss: 1.1031e-06 - mae: 0.0281 - val_loss: 0.0060 - val_mae: 0.0579\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 0.04869\n",
      "Epoch 103/1000\n",
      "48/48 - 1s - loss: 1.0393e-06 - mae: 0.0272 - val_loss: 0.0065 - val_mae: 0.0611\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 0.04869\n",
      "Epoch 104/1000\n",
      "48/48 - 1s - loss: 9.6844e-07 - mae: 0.0263 - val_loss: 0.0061 - val_mae: 0.0589\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 0.04869\n",
      "Epoch 105/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 - 1s - loss: 9.8960e-07 - mae: 0.0266 - val_loss: 0.0062 - val_mae: 0.0592\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 0.04869\n",
      "Epoch 106/1000\n",
      "48/48 - 1s - loss: 9.5624e-07 - mae: 0.0261 - val_loss: 0.0064 - val_mae: 0.0601\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 0.04869\n",
      "Epoch 107/1000\n",
      "48/48 - 1s - loss: 9.9436e-07 - mae: 0.0267 - val_loss: 0.0065 - val_mae: 0.0609\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 0.04869\n",
      "Epoch 108/1000\n",
      "48/48 - 1s - loss: 9.4203e-07 - mae: 0.0259 - val_loss: 0.0064 - val_mae: 0.0607\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 0.04869\n",
      "Epoch 109/1000\n",
      "48/48 - 1s - loss: 9.7037e-07 - mae: 0.0263 - val_loss: 0.0066 - val_mae: 0.0615\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 0.04869\n",
      "Epoch 110/1000\n",
      "48/48 - 1s - loss: 1.0108e-06 - mae: 0.0269 - val_loss: 0.0068 - val_mae: 0.0628\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 0.04869\n",
      "Epoch 111/1000\n",
      "48/48 - 1s - loss: 9.1881e-07 - mae: 0.0256 - val_loss: 0.0067 - val_mae: 0.0623\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 0.04869\n",
      "Epoch 112/1000\n",
      "48/48 - 1s - loss: 8.9807e-07 - mae: 0.0253 - val_loss: 0.0065 - val_mae: 0.0605\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 0.04869\n",
      "Epoch 113/1000\n",
      "48/48 - 1s - loss: 9.1302e-07 - mae: 0.0255 - val_loss: 0.0065 - val_mae: 0.0607\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 0.04869\n",
      "Epoch 114/1000\n",
      "48/48 - 1s - loss: 8.6718e-07 - mae: 0.0249 - val_loss: 0.0065 - val_mae: 0.0606\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 0.04869\n",
      "Epoch 115/1000\n",
      "48/48 - 1s - loss: 8.8568e-07 - mae: 0.0252 - val_loss: 0.0067 - val_mae: 0.0621\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 0.04869\n",
      "Epoch 116/1000\n",
      "48/48 - 1s - loss: 8.8060e-07 - mae: 0.0251 - val_loss: 0.0066 - val_mae: 0.0615\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 0.04869\n",
      "Epoch 117/1000\n",
      "48/48 - 1s - loss: 8.6786e-07 - mae: 0.0249 - val_loss: 0.0066 - val_mae: 0.0614\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 0.04869\n",
      "Epoch 118/1000\n",
      "48/48 - 1s - loss: 8.9954e-07 - mae: 0.0254 - val_loss: 0.0068 - val_mae: 0.0623\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 0.04869\n",
      "Epoch 119/1000\n",
      "48/48 - 1s - loss: 8.4779e-07 - mae: 0.0246 - val_loss: 0.0066 - val_mae: 0.0617\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 0.04869\n",
      "Epoch 120/1000\n",
      "48/48 - 1s - loss: 8.4202e-07 - mae: 0.0246 - val_loss: 0.0064 - val_mae: 0.0601\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 0.04869\n",
      "Epoch 121/1000\n",
      "48/48 - 1s - loss: 8.2896e-07 - mae: 0.0244 - val_loss: 0.0067 - val_mae: 0.0614\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 0.04869\n",
      "Epoch 122/1000\n",
      "48/48 - 1s - loss: 8.4961e-07 - mae: 0.0247 - val_loss: 0.0066 - val_mae: 0.0612\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 0.04869\n",
      "Epoch 123/1000\n",
      "48/48 - 1s - loss: 8.2417e-07 - mae: 0.0243 - val_loss: 0.0069 - val_mae: 0.0631\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 0.04869\n",
      "Epoch 124/1000\n",
      "48/48 - 1s - loss: 8.7750e-07 - mae: 0.0251 - val_loss: 0.0070 - val_mae: 0.0632\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 0.04869\n",
      "Epoch 125/1000\n",
      "48/48 - 1s - loss: 8.1231e-07 - mae: 0.0241 - val_loss: 0.0069 - val_mae: 0.0629\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 0.04869\n",
      "Epoch 126/1000\n",
      "48/48 - 1s - loss: 9.0511e-07 - mae: 0.0255 - val_loss: 0.0071 - val_mae: 0.0642\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 0.04869\n",
      "Epoch 127/1000\n",
      "48/48 - 1s - loss: 8.0949e-07 - mae: 0.0240 - val_loss: 0.0067 - val_mae: 0.0614\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 0.04869\n",
      "Epoch 128/1000\n",
      "48/48 - 1s - loss: 7.6227e-07 - mae: 0.0233 - val_loss: 0.0073 - val_mae: 0.0649\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 0.04869\n",
      "Epoch 129/1000\n",
      "48/48 - 1s - loss: 8.5064e-07 - mae: 0.0246 - val_loss: 0.0071 - val_mae: 0.0643\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 0.04869\n",
      "Epoch 130/1000\n",
      "48/48 - 1s - loss: 8.0643e-07 - mae: 0.0239 - val_loss: 0.0069 - val_mae: 0.0628\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 0.04869\n",
      "Epoch 131/1000\n",
      "48/48 - 1s - loss: 7.4342e-07 - mae: 0.0231 - val_loss: 0.0072 - val_mae: 0.0645\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 0.04869\n",
      "Epoch 132/1000\n",
      "48/48 - 1s - loss: 7.5165e-07 - mae: 0.0232 - val_loss: 0.0073 - val_mae: 0.0647\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 0.04869\n",
      "Epoch 133/1000\n",
      "48/48 - 1s - loss: 8.6041e-07 - mae: 0.0249 - val_loss: 0.0072 - val_mae: 0.0645\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 0.04869\n",
      "Epoch 134/1000\n",
      "48/48 - 1s - loss: 7.5496e-07 - mae: 0.0232 - val_loss: 0.0072 - val_mae: 0.0645\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 0.04869\n",
      "Epoch 135/1000\n",
      "48/48 - 1s - loss: 7.2622e-07 - mae: 0.0228 - val_loss: 0.0070 - val_mae: 0.0633\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 0.04869\n",
      "Epoch 136/1000\n",
      "48/48 - 1s - loss: 7.1878e-07 - mae: 0.0227 - val_loss: 0.0076 - val_mae: 0.0663\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 0.04869\n",
      "Epoch 137/1000\n",
      "48/48 - 1s - loss: 7.6396e-07 - mae: 0.0234 - val_loss: 0.0077 - val_mae: 0.0668\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 0.04869\n",
      "Epoch 138/1000\n",
      "48/48 - 1s - loss: 7.5451e-07 - mae: 0.0232 - val_loss: 0.0071 - val_mae: 0.0636\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 0.04869\n",
      "Epoch 139/1000\n",
      "48/48 - 1s - loss: 7.2066e-07 - mae: 0.0227 - val_loss: 0.0071 - val_mae: 0.0637\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 0.04869\n",
      "Epoch 140/1000\n",
      "48/48 - 1s - loss: 7.3093e-07 - mae: 0.0229 - val_loss: 0.0073 - val_mae: 0.0647\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 0.04869\n",
      "Epoch 141/1000\n",
      "48/48 - 1s - loss: 6.8858e-07 - mae: 0.0222 - val_loss: 0.0071 - val_mae: 0.0639\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 0.04869\n",
      "Epoch 142/1000\n",
      "48/48 - 1s - loss: 7.3622e-07 - mae: 0.0230 - val_loss: 0.0074 - val_mae: 0.0651\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 0.04869\n",
      "Epoch 143/1000\n",
      "48/48 - 1s - loss: 7.0827e-07 - mae: 0.0225 - val_loss: 0.0074 - val_mae: 0.0653\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 0.04869\n",
      "Epoch 144/1000\n",
      "48/48 - 1s - loss: 7.2931e-07 - mae: 0.0228 - val_loss: 0.0074 - val_mae: 0.0651\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 0.04869\n",
      "Epoch 145/1000\n",
      "48/48 - 1s - loss: 6.8416e-07 - mae: 0.0221 - val_loss: 0.0074 - val_mae: 0.0649\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 0.04869\n",
      "Epoch 146/1000\n",
      "48/48 - 1s - loss: 6.7996e-07 - mae: 0.0220 - val_loss: 0.0073 - val_mae: 0.0647\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 0.04869\n",
      "Epoch 147/1000\n",
      "48/48 - 1s - loss: 6.6208e-07 - mae: 0.0217 - val_loss: 0.0075 - val_mae: 0.0655\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 0.04869\n",
      "Epoch 148/1000\n",
      "48/48 - 1s - loss: 6.4941e-07 - mae: 0.0215 - val_loss: 0.0075 - val_mae: 0.0655\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 0.04869\n",
      "Epoch 149/1000\n",
      "48/48 - 1s - loss: 6.6668e-07 - mae: 0.0218 - val_loss: 0.0076 - val_mae: 0.0662\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 0.04869\n",
      "Epoch 150/1000\n",
      "48/48 - 1s - loss: 6.5058e-07 - mae: 0.0216 - val_loss: 0.0078 - val_mae: 0.0669\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 0.04869\n",
      "Epoch 151/1000\n",
      "48/48 - 1s - loss: 6.5649e-07 - mae: 0.0217 - val_loss: 0.0077 - val_mae: 0.0669\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 0.04869\n",
      "Epoch 152/1000\n",
      "48/48 - 1s - loss: 6.7790e-07 - mae: 0.0220 - val_loss: 0.0077 - val_mae: 0.0666\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 0.04869\n",
      "Epoch 153/1000\n",
      "48/48 - 1s - loss: 6.9590e-07 - mae: 0.0223 - val_loss: 0.0083 - val_mae: 0.0693\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 0.04869\n",
      "Epoch 154/1000\n",
      "48/48 - 1s - loss: 6.4470e-07 - mae: 0.0215 - val_loss: 0.0078 - val_mae: 0.0668\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 0.04869\n",
      "Epoch 155/1000\n",
      "48/48 - 1s - loss: 6.1478e-07 - mae: 0.0210 - val_loss: 0.0078 - val_mae: 0.0672\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 0.04869\n",
      "Epoch 156/1000\n",
      "48/48 - 1s - loss: 6.3143e-07 - mae: 0.0212 - val_loss: 0.0079 - val_mae: 0.0675\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 0.04869\n",
      "Epoch 157/1000\n",
      "48/48 - 1s - loss: 6.2209e-07 - mae: 0.0211 - val_loss: 0.0076 - val_mae: 0.0660\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 0.04869\n",
      "Epoch 158/1000\n",
      "48/48 - 1s - loss: 6.3134e-07 - mae: 0.0212 - val_loss: 0.0079 - val_mae: 0.0678\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 0.04869\n",
      "Epoch 159/1000\n",
      "48/48 - 1s - loss: 6.6070e-07 - mae: 0.0217 - val_loss: 0.0079 - val_mae: 0.0676\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 0.04869\n",
      "Epoch 160/1000\n",
      "48/48 - 1s - loss: 6.9971e-07 - mae: 0.0223 - val_loss: 0.0080 - val_mae: 0.0682\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 0.04869\n",
      "Epoch 161/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 - 1s - loss: 6.3074e-07 - mae: 0.0212 - val_loss: 0.0077 - val_mae: 0.0668\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 0.04869\n",
      "Epoch 162/1000\n",
      "48/48 - 1s - loss: 6.5116e-07 - mae: 0.0216 - val_loss: 0.0080 - val_mae: 0.0680\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 0.04869\n",
      "Epoch 163/1000\n",
      "48/48 - 1s - loss: 6.4236e-07 - mae: 0.0214 - val_loss: 0.0080 - val_mae: 0.0681\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 0.04869\n",
      "Epoch 164/1000\n",
      "48/48 - 1s - loss: 6.2001e-07 - mae: 0.0210 - val_loss: 0.0081 - val_mae: 0.0682\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 0.04869\n",
      "Epoch 165/1000\n",
      "48/48 - 1s - loss: 6.0544e-07 - mae: 0.0208 - val_loss: 0.0082 - val_mae: 0.0688\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 0.04869\n",
      "Epoch 166/1000\n",
      "48/48 - 1s - loss: 5.8750e-07 - mae: 0.0205 - val_loss: 0.0084 - val_mae: 0.0700\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 0.04869\n",
      "Epoch 167/1000\n",
      "48/48 - 1s - loss: 6.2208e-07 - mae: 0.0210 - val_loss: 0.0082 - val_mae: 0.0688\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 0.04869\n",
      "Epoch 168/1000\n",
      "48/48 - 1s - loss: 6.0635e-07 - mae: 0.0208 - val_loss: 0.0078 - val_mae: 0.0668\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 0.04869\n",
      "Epoch 169/1000\n",
      "48/48 - 1s - loss: 6.4299e-07 - mae: 0.0214 - val_loss: 0.0084 - val_mae: 0.0699\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 0.04869\n",
      "Epoch 170/1000\n",
      "48/48 - 1s - loss: 5.6957e-07 - mae: 0.0201 - val_loss: 0.0084 - val_mae: 0.0696\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 0.04869\n",
      "Epoch 171/1000\n",
      "48/48 - 1s - loss: 6.3196e-07 - mae: 0.0212 - val_loss: 0.0084 - val_mae: 0.0697\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 0.04869\n",
      "Epoch 172/1000\n",
      "48/48 - 1s - loss: 6.0830e-07 - mae: 0.0208 - val_loss: 0.0084 - val_mae: 0.0700\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 0.04869\n",
      "Epoch 173/1000\n",
      "48/48 - 1s - loss: 6.2100e-07 - mae: 0.0210 - val_loss: 0.0082 - val_mae: 0.0687\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 0.04869\n",
      "Epoch 174/1000\n",
      "48/48 - 1s - loss: 5.8198e-07 - mae: 0.0204 - val_loss: 0.0081 - val_mae: 0.0683\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 0.04869\n",
      "Epoch 175/1000\n",
      "48/48 - 1s - loss: 5.6781e-07 - mae: 0.0201 - val_loss: 0.0085 - val_mae: 0.0701\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 0.04869\n",
      "Epoch 176/1000\n",
      "48/48 - 1s - loss: 5.3908e-07 - mae: 0.0196 - val_loss: 0.0085 - val_mae: 0.0702\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 0.04869\n",
      "Epoch 177/1000\n",
      "48/48 - 1s - loss: 5.4461e-07 - mae: 0.0197 - val_loss: 0.0086 - val_mae: 0.0706\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 0.04869\n",
      "Epoch 178/1000\n",
      "48/48 - 1s - loss: 5.5572e-07 - mae: 0.0199 - val_loss: 0.0086 - val_mae: 0.0703\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 0.04869\n",
      "Epoch 179/1000\n",
      "48/48 - 1s - loss: 5.3967e-07 - mae: 0.0196 - val_loss: 0.0084 - val_mae: 0.0698\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 0.04869\n",
      "Epoch 180/1000\n",
      "48/48 - 1s - loss: 5.4026e-07 - mae: 0.0196 - val_loss: 0.0085 - val_mae: 0.0701\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 0.04869\n",
      "Epoch 181/1000\n",
      "48/48 - 1s - loss: 5.3675e-07 - mae: 0.0195 - val_loss: 0.0084 - val_mae: 0.0698\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 0.04869\n",
      "Epoch 182/1000\n",
      "48/48 - 1s - loss: 5.3448e-07 - mae: 0.0195 - val_loss: 0.0086 - val_mae: 0.0706\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 0.04869\n",
      "Epoch 183/1000\n",
      "48/48 - 1s - loss: 5.3981e-07 - mae: 0.0196 - val_loss: 0.0088 - val_mae: 0.0716\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 0.04869\n",
      "Epoch 184/1000\n",
      "48/48 - 1s - loss: 6.1800e-07 - mae: 0.0209 - val_loss: 0.0090 - val_mae: 0.0725\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 0.04869\n",
      "Epoch 185/1000\n",
      "48/48 - 1s - loss: 5.2762e-07 - mae: 0.0194 - val_loss: 0.0088 - val_mae: 0.0717\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 0.04869\n",
      "Epoch 186/1000\n",
      "48/48 - 1s - loss: 5.1295e-07 - mae: 0.0191 - val_loss: 0.0088 - val_mae: 0.0717\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 0.04869\n",
      "Epoch 187/1000\n",
      "48/48 - 1s - loss: 5.1494e-07 - mae: 0.0191 - val_loss: 0.0087 - val_mae: 0.0708\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 0.04869\n",
      "Epoch 188/1000\n",
      "48/48 - 1s - loss: 5.2430e-07 - mae: 0.0193 - val_loss: 0.0089 - val_mae: 0.0719\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 0.04869\n",
      "Epoch 189/1000\n",
      "48/48 - 1s - loss: 5.2536e-07 - mae: 0.0193 - val_loss: 0.0088 - val_mae: 0.0715\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 0.04869\n",
      "Epoch 190/1000\n",
      "48/48 - 1s - loss: 5.6216e-07 - mae: 0.0200 - val_loss: 0.0092 - val_mae: 0.0736\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 0.04869\n",
      "Epoch 191/1000\n",
      "48/48 - 1s - loss: 5.2522e-07 - mae: 0.0193 - val_loss: 0.0088 - val_mae: 0.0715\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 0.04869\n",
      "Epoch 192/1000\n",
      "48/48 - 1s - loss: 5.2575e-07 - mae: 0.0193 - val_loss: 0.0089 - val_mae: 0.0721\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 0.04869\n",
      "Epoch 193/1000\n",
      "48/48 - 1s - loss: 5.2995e-07 - mae: 0.0194 - val_loss: 0.0088 - val_mae: 0.0713\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 0.04869\n",
      "Epoch 194/1000\n",
      "48/48 - 1s - loss: 4.9518e-07 - mae: 0.0188 - val_loss: 0.0091 - val_mae: 0.0727\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 0.04869\n",
      "Epoch 195/1000\n",
      "48/48 - 1s - loss: 4.9940e-07 - mae: 0.0188 - val_loss: 0.0091 - val_mae: 0.0730\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 0.04869\n",
      "Epoch 196/1000\n",
      "48/48 - 1s - loss: 6.1087e-07 - mae: 0.0209 - val_loss: 0.0091 - val_mae: 0.0726\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 0.04869\n",
      "Epoch 197/1000\n",
      "48/48 - 1s - loss: 5.6957e-07 - mae: 0.0201 - val_loss: 0.0095 - val_mae: 0.0746\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 0.04869\n",
      "Epoch 198/1000\n",
      "48/48 - 1s - loss: 5.0892e-07 - mae: 0.0190 - val_loss: 0.0091 - val_mae: 0.0728\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 0.04869\n",
      "Epoch 199/1000\n",
      "48/48 - 1s - loss: 5.1372e-07 - mae: 0.0191 - val_loss: 0.0090 - val_mae: 0.0723\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 0.04869\n",
      "Epoch 200/1000\n",
      "48/48 - 1s - loss: 4.7789e-07 - mae: 0.0184 - val_loss: 0.0092 - val_mae: 0.0732\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 0.04869\n",
      "Epoch 201/1000\n",
      "48/48 - 1s - loss: 4.9584e-07 - mae: 0.0188 - val_loss: 0.0092 - val_mae: 0.0732\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 0.04869\n",
      "Epoch 202/1000\n",
      "48/48 - 1s - loss: 4.9454e-07 - mae: 0.0187 - val_loss: 0.0090 - val_mae: 0.0724\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 0.04869\n",
      "Epoch 203/1000\n",
      "48/48 - 1s - loss: 5.3577e-07 - mae: 0.0195 - val_loss: 0.0092 - val_mae: 0.0730\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 0.04869\n",
      "Epoch 204/1000\n",
      "48/48 - 1s - loss: 4.9389e-07 - mae: 0.0187 - val_loss: 0.0094 - val_mae: 0.0740\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 0.04869\n",
      "Epoch 205/1000\n",
      "48/48 - 1s - loss: 4.8164e-07 - mae: 0.0185 - val_loss: 0.0095 - val_mae: 0.0746\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 0.04869\n",
      "Epoch 206/1000\n",
      "48/48 - 1s - loss: 4.8196e-07 - mae: 0.0185 - val_loss: 0.0095 - val_mae: 0.0746\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 0.04869\n",
      "Epoch 207/1000\n",
      "48/48 - 1s - loss: 4.9853e-07 - mae: 0.0188 - val_loss: 0.0092 - val_mae: 0.0732\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 0.04869\n",
      "Epoch 208/1000\n",
      "48/48 - 1s - loss: 5.6773e-07 - mae: 0.0201 - val_loss: 0.0092 - val_mae: 0.0734\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 0.04869\n",
      "Epoch 209/1000\n",
      "48/48 - 1s - loss: 4.9685e-07 - mae: 0.0188 - val_loss: 0.0095 - val_mae: 0.0746\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 0.04869\n",
      "Epoch 210/1000\n",
      "48/48 - 1s - loss: 4.5362e-07 - mae: 0.0179 - val_loss: 0.0092 - val_mae: 0.0732\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 0.04869\n",
      "Epoch 211/1000\n",
      "48/48 - 1s - loss: 4.7704e-07 - mae: 0.0184 - val_loss: 0.0094 - val_mae: 0.0744\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 0.04869\n",
      "Epoch 212/1000\n",
      "48/48 - 1s - loss: 4.7504e-07 - mae: 0.0184 - val_loss: 0.0094 - val_mae: 0.0740\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 0.04869\n",
      "Epoch 213/1000\n",
      "48/48 - 1s - loss: 4.5270e-07 - mae: 0.0179 - val_loss: 0.0094 - val_mae: 0.0741\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 0.04869\n",
      "Epoch 214/1000\n",
      "48/48 - 1s - loss: 4.7383e-07 - mae: 0.0183 - val_loss: 0.0095 - val_mae: 0.0747\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 0.04869\n",
      "Epoch 215/1000\n",
      "48/48 - 1s - loss: 4.7637e-07 - mae: 0.0184 - val_loss: 0.0094 - val_mae: 0.0740\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 0.04869\n",
      "Epoch 216/1000\n",
      "48/48 - 1s - loss: 4.8764e-07 - mae: 0.0186 - val_loss: 0.0094 - val_mae: 0.0740\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 0.04869\n",
      "Epoch 217/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 - 1s - loss: 5.2504e-07 - mae: 0.0193 - val_loss: 0.0095 - val_mae: 0.0743\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 0.04869\n",
      "Epoch 218/1000\n",
      "48/48 - 1s - loss: 4.4192e-07 - mae: 0.0177 - val_loss: 0.0095 - val_mae: 0.0745\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 0.04869\n",
      "Epoch 219/1000\n",
      "48/48 - 1s - loss: 4.5962e-07 - mae: 0.0181 - val_loss: 0.0096 - val_mae: 0.0751\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 0.04869\n",
      "Epoch 220/1000\n",
      "48/48 - 1s - loss: 4.6299e-07 - mae: 0.0181 - val_loss: 0.0098 - val_mae: 0.0760\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 0.04869\n",
      "Epoch 221/1000\n",
      "48/48 - 1s - loss: 4.7001e-07 - mae: 0.0182 - val_loss: 0.0097 - val_mae: 0.0750\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 0.04869\n",
      "Epoch 222/1000\n",
      "48/48 - 1s - loss: 4.6811e-07 - mae: 0.0182 - val_loss: 0.0093 - val_mae: 0.0738\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 0.04869\n",
      "Epoch 223/1000\n",
      "48/48 - 1s - loss: 4.4235e-07 - mae: 0.0177 - val_loss: 0.0098 - val_mae: 0.0759\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 0.04869\n",
      "Epoch 224/1000\n",
      "48/48 - 1s - loss: 4.5062e-07 - mae: 0.0178 - val_loss: 0.0098 - val_mae: 0.0758\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 0.04869\n",
      "Epoch 225/1000\n",
      "48/48 - 1s - loss: 4.5512e-07 - mae: 0.0180 - val_loss: 0.0096 - val_mae: 0.0751\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 0.04869\n",
      "Epoch 226/1000\n",
      "48/48 - 1s - loss: 4.6094e-07 - mae: 0.0181 - val_loss: 0.0096 - val_mae: 0.0751\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 0.04869\n",
      "Epoch 227/1000\n",
      "48/48 - 1s - loss: 4.5949e-07 - mae: 0.0181 - val_loss: 0.0097 - val_mae: 0.0751\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 0.04869\n",
      "Epoch 228/1000\n",
      "48/48 - 1s - loss: 4.4253e-07 - mae: 0.0177 - val_loss: 0.0098 - val_mae: 0.0757\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 0.04869\n",
      "Epoch 229/1000\n",
      "48/48 - 1s - loss: 4.4588e-07 - mae: 0.0178 - val_loss: 0.0097 - val_mae: 0.0754\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 0.04869\n",
      "Epoch 230/1000\n",
      "48/48 - 1s - loss: 4.4064e-07 - mae: 0.0177 - val_loss: 0.0099 - val_mae: 0.0760\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 0.04869\n",
      "Epoch 231/1000\n",
      "48/48 - 1s - loss: 4.4126e-07 - mae: 0.0177 - val_loss: 0.0097 - val_mae: 0.0752\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 0.04869\n",
      "Epoch 232/1000\n",
      "48/48 - 1s - loss: 4.9056e-07 - mae: 0.0187 - val_loss: 0.0096 - val_mae: 0.0750\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 0.04869\n",
      "Epoch 00232: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2f8e9f7130>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "#               loss=custom_weight_loss,\n",
    "              loss='mse',\n",
    "              metrics=['mae']\n",
    "             )\n",
    "\n",
    "es = EarlyStopping(monitor='val_mae', mode='min', verbose=2, patience=PATIENCE)\n",
    "mc = ModelCheckpoint('../../saved_models/price_all_weighted_IPF_95.h5', \n",
    "                     monitor='val_mae', \n",
    "                     mode='min', \n",
    "                     verbose=2, \n",
    "                     save_best_only=True,\n",
    "                    )\n",
    "\n",
    "\n",
    "model.fit(train_X, train_y,\n",
    "          validation_data=(val_X, val_y),\n",
    "          epochs=EPOCHS,\n",
    "          batch_size=BATCH,\n",
    "          verbose=2,\n",
    "          shuffle=True,\n",
    "          callbacks=[es, mc],\n",
    "          sample_weight=sample_weights\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b313809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95854ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6ba252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae65733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
