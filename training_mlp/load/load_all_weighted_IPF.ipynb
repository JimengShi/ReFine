{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4843e104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-13 16:04:30.373671: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from math import sqrt\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "from helper import series_to_supervised\n",
    "from model.mlp import mlp_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "460ee40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb30958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>price_dayahead</th>\n",
       "      <th>gen_coal</th>\n",
       "      <th>gen_gas</th>\n",
       "      <th>load_actual</th>\n",
       "      <th>gen_lig</th>\n",
       "      <th>gen_oil</th>\n",
       "      <th>gen_oth_renew</th>\n",
       "      <th>pressure_Barcelona</th>\n",
       "      <th>pressure_Bilbao</th>\n",
       "      <th>...</th>\n",
       "      <th>wind_deg_Bilbao</th>\n",
       "      <th>clouds_all_Bilbao</th>\n",
       "      <th>gen_hyd_river</th>\n",
       "      <th>wind_deg_Seville</th>\n",
       "      <th>wind_speed_Barcelona</th>\n",
       "      <th>wind_speed_Valencia</th>\n",
       "      <th>wind_speed_Bilbao</th>\n",
       "      <th>gen_wind</th>\n",
       "      <th>wind_speed_Madrid</th>\n",
       "      <th>gen_hyd_pump</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:00:00+00:00</th>\n",
       "      <td>64.92</td>\n",
       "      <td>48.10</td>\n",
       "      <td>4755.0</td>\n",
       "      <td>5196.0</td>\n",
       "      <td>24382.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>...</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5890.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>920.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 01:00:00+00:00</th>\n",
       "      <td>64.48</td>\n",
       "      <td>47.33</td>\n",
       "      <td>4581.0</td>\n",
       "      <td>4857.0</td>\n",
       "      <td>22734.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>...</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5461.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 02:00:00+00:00</th>\n",
       "      <td>59.32</td>\n",
       "      <td>42.27</td>\n",
       "      <td>4131.0</td>\n",
       "      <td>4314.0</td>\n",
       "      <td>21286.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>...</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>949.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5238.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 03:00:00+00:00</th>\n",
       "      <td>56.04</td>\n",
       "      <td>38.41</td>\n",
       "      <td>3840.0</td>\n",
       "      <td>4130.0</td>\n",
       "      <td>20264.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>...</td>\n",
       "      <td>221.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>953.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4935.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1826.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 04:00:00+00:00</th>\n",
       "      <td>53.63</td>\n",
       "      <td>35.72</td>\n",
       "      <td>3590.0</td>\n",
       "      <td>4038.0</td>\n",
       "      <td>19905.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1037.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>...</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>952.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4618.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 18:00:00+00:00</th>\n",
       "      <td>77.02</td>\n",
       "      <td>68.85</td>\n",
       "      <td>2628.0</td>\n",
       "      <td>7634.0</td>\n",
       "      <td>30653.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1027.0</td>\n",
       "      <td>1033.0</td>\n",
       "      <td>...</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1135.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3113.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 19:00:00+00:00</th>\n",
       "      <td>76.16</td>\n",
       "      <td>68.40</td>\n",
       "      <td>2566.0</td>\n",
       "      <td>7241.0</td>\n",
       "      <td>29735.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1027.0</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1172.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3288.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 20:00:00+00:00</th>\n",
       "      <td>74.30</td>\n",
       "      <td>66.88</td>\n",
       "      <td>2422.0</td>\n",
       "      <td>7025.0</td>\n",
       "      <td>28071.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>...</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1148.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3503.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 21:00:00+00:00</th>\n",
       "      <td>69.89</td>\n",
       "      <td>63.93</td>\n",
       "      <td>2293.0</td>\n",
       "      <td>6562.0</td>\n",
       "      <td>25801.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>...</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1128.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3586.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 22:00:00+00:00</th>\n",
       "      <td>69.88</td>\n",
       "      <td>64.27</td>\n",
       "      <td>2166.0</td>\n",
       "      <td>6926.0</td>\n",
       "      <td>24455.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3651.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35063 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           price  price_dayahead  gen_coal  gen_gas  \\\n",
       "time                                                                  \n",
       "2015-01-01 00:00:00+00:00  64.92           48.10    4755.0   5196.0   \n",
       "2015-01-01 01:00:00+00:00  64.48           47.33    4581.0   4857.0   \n",
       "2015-01-01 02:00:00+00:00  59.32           42.27    4131.0   4314.0   \n",
       "2015-01-01 03:00:00+00:00  56.04           38.41    3840.0   4130.0   \n",
       "2015-01-01 04:00:00+00:00  53.63           35.72    3590.0   4038.0   \n",
       "...                          ...             ...       ...      ...   \n",
       "2018-12-31 18:00:00+00:00  77.02           68.85    2628.0   7634.0   \n",
       "2018-12-31 19:00:00+00:00  76.16           68.40    2566.0   7241.0   \n",
       "2018-12-31 20:00:00+00:00  74.30           66.88    2422.0   7025.0   \n",
       "2018-12-31 21:00:00+00:00  69.89           63.93    2293.0   6562.0   \n",
       "2018-12-31 22:00:00+00:00  69.88           64.27    2166.0   6926.0   \n",
       "\n",
       "                           load_actual  gen_lig  gen_oil  gen_oth_renew  \\\n",
       "time                                                                      \n",
       "2015-01-01 00:00:00+00:00      24382.0    328.0    158.0           71.0   \n",
       "2015-01-01 01:00:00+00:00      22734.0    323.0    157.0           73.0   \n",
       "2015-01-01 02:00:00+00:00      21286.0    254.0    160.0           75.0   \n",
       "2015-01-01 03:00:00+00:00      20264.0    187.0    156.0           74.0   \n",
       "2015-01-01 04:00:00+00:00      19905.0    178.0    156.0           74.0   \n",
       "...                                ...      ...      ...            ...   \n",
       "2018-12-31 18:00:00+00:00      30653.0      0.0    178.0           95.0   \n",
       "2018-12-31 19:00:00+00:00      29735.0      0.0    174.0           95.0   \n",
       "2018-12-31 20:00:00+00:00      28071.0      0.0    168.0           94.0   \n",
       "2018-12-31 21:00:00+00:00      25801.0      0.0    163.0           93.0   \n",
       "2018-12-31 22:00:00+00:00      24455.0      0.0    163.0           92.0   \n",
       "\n",
       "                           pressure_Barcelona  pressure_Bilbao  ...  \\\n",
       "time                                                            ...   \n",
       "2015-01-01 00:00:00+00:00              1035.0           1035.0  ...   \n",
       "2015-01-01 01:00:00+00:00              1036.0           1036.0  ...   \n",
       "2015-01-01 02:00:00+00:00              1036.0           1035.0  ...   \n",
       "2015-01-01 03:00:00+00:00              1036.0           1035.0  ...   \n",
       "2015-01-01 04:00:00+00:00              1037.0           1035.0  ...   \n",
       "...                                       ...              ...  ...   \n",
       "2018-12-31 18:00:00+00:00              1027.0           1033.0  ...   \n",
       "2018-12-31 19:00:00+00:00              1027.0           1034.0  ...   \n",
       "2018-12-31 20:00:00+00:00              1028.0           1034.0  ...   \n",
       "2018-12-31 21:00:00+00:00              1028.0           1034.0  ...   \n",
       "2018-12-31 22:00:00+00:00              1028.0           1034.0  ...   \n",
       "\n",
       "                           wind_deg_Bilbao  clouds_all_Bilbao  gen_hyd_river  \\\n",
       "time                                                                           \n",
       "2015-01-01 00:00:00+00:00            229.0                0.0         1009.0   \n",
       "2015-01-01 01:00:00+00:00            224.0                0.0          973.0   \n",
       "2015-01-01 02:00:00+00:00            225.0                0.0          949.0   \n",
       "2015-01-01 03:00:00+00:00            221.0                0.0          953.0   \n",
       "2015-01-01 04:00:00+00:00            224.0                0.0          952.0   \n",
       "...                                    ...                ...            ...   \n",
       "2018-12-31 18:00:00+00:00             57.0                0.0         1135.0   \n",
       "2018-12-31 19:00:00+00:00              0.0                0.0         1172.0   \n",
       "2018-12-31 20:00:00+00:00            140.0                0.0         1148.0   \n",
       "2018-12-31 21:00:00+00:00            120.0                0.0         1128.0   \n",
       "2018-12-31 22:00:00+00:00            100.0                0.0         1069.0   \n",
       "\n",
       "                           wind_deg_Seville  wind_speed_Barcelona  \\\n",
       "time                                                                \n",
       "2015-01-01 00:00:00+00:00              21.0                   7.0   \n",
       "2015-01-01 01:00:00+00:00              27.0                   7.0   \n",
       "2015-01-01 02:00:00+00:00              27.0                   7.0   \n",
       "2015-01-01 03:00:00+00:00              27.0                   7.0   \n",
       "2015-01-01 04:00:00+00:00              57.0                   5.0   \n",
       "...                                     ...                   ...   \n",
       "2018-12-31 18:00:00+00:00              30.0                   1.0   \n",
       "2018-12-31 19:00:00+00:00              30.0                   3.0   \n",
       "2018-12-31 20:00:00+00:00              50.0                   4.0   \n",
       "2018-12-31 21:00:00+00:00              60.0                   5.0   \n",
       "2018-12-31 22:00:00+00:00              50.0                   5.0   \n",
       "\n",
       "                           wind_speed_Valencia  wind_speed_Bilbao  gen_wind  \\\n",
       "time                                                                          \n",
       "2015-01-01 00:00:00+00:00                  1.0                0.0    5890.0   \n",
       "2015-01-01 01:00:00+00:00                  0.0                1.0    5461.0   \n",
       "2015-01-01 02:00:00+00:00                  0.0                1.0    5238.0   \n",
       "2015-01-01 03:00:00+00:00                  0.0                1.0    4935.0   \n",
       "2015-01-01 04:00:00+00:00                  2.0                1.0    4618.0   \n",
       "...                                        ...                ...       ...   \n",
       "2018-12-31 18:00:00+00:00                  2.0                0.0    3113.0   \n",
       "2018-12-31 19:00:00+00:00                  1.0                1.0    3288.0   \n",
       "2018-12-31 20:00:00+00:00                  3.0                1.0    3503.0   \n",
       "2018-12-31 21:00:00+00:00                  2.0                1.0    3586.0   \n",
       "2018-12-31 22:00:00+00:00                  2.0                2.0    3651.0   \n",
       "\n",
       "                           wind_speed_Madrid  gen_hyd_pump  \n",
       "time                                                        \n",
       "2015-01-01 00:00:00+00:00                1.0         920.0  \n",
       "2015-01-01 01:00:00+00:00                1.0        1164.0  \n",
       "2015-01-01 02:00:00+00:00                1.0        1503.0  \n",
       "2015-01-01 03:00:00+00:00                1.0        1826.0  \n",
       "2015-01-01 04:00:00+00:00                0.0        2109.0  \n",
       "...                                      ...           ...  \n",
       "2018-12-31 18:00:00+00:00                1.0           1.0  \n",
       "2018-12-31 19:00:00+00:00                1.0           1.0  \n",
       "2018-12-31 20:00:00+00:00                1.0          50.0  \n",
       "2018-12-31 21:00:00+00:00                2.0         108.0  \n",
       "2018-12-31 22:00:00+00:00                1.0         108.0  \n",
       "\n",
       "[35063 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../../data/energy_weather.csv', index_col=0)\n",
    "# https://www.kaggle.com/datasets/nicholasjhana/energy-consumption-generation-prices-and-weather\n",
    "\n",
    "dataset.fillna(0, inplace=True)\n",
    "data = dataset\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3855fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'price_dayahead', 'gen_coal', 'gen_gas', 'load_actual',\n",
       "       'gen_lig', 'gen_oil', 'gen_oth_renew', 'pressure_Barcelona',\n",
       "       'pressure_Bilbao', 'gen_waste', 'gen_bio', 'temp_min_Valencia',\n",
       "       'pressure_Valencia', 'temp_min_Barcelona', 'humidity_Seville',\n",
       "       'wind_deg_Bilbao', 'clouds_all_Bilbao', 'gen_hyd_river',\n",
       "       'wind_deg_Seville', 'wind_speed_Barcelona', 'wind_speed_Valencia',\n",
       "       'wind_speed_Bilbao', 'gen_wind', 'wind_speed_Madrid', 'gen_hyd_pump'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2191f187",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "33b340cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reframed.shape: (34980, 2184)\n"
     ]
    }
   ],
   "source": [
    "values = data.values\n",
    "\n",
    "# specify the number of lag hours\n",
    "n_hours = 24*3\n",
    "n_features = data.shape[-1]\n",
    "k = 12\n",
    "split1 = 0.7\n",
    "split2 = 0.85\n",
    "\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(values, n_hours, k)\n",
    "print(\"reframed.shape:\", reframed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9e2084b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape (24486, 1872) (24486, 12) (5247, 1872) (5247, 12) (5247, 1872) (5247, 12)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "reframed_values = reframed.values\n",
    "n_train_hours = int(len(reframed_values)*split1)\n",
    "n_valid_hours = int(len(reframed_values)*split2)\n",
    "\n",
    "train = reframed_values[:n_train_hours, :]\n",
    "val = reframed_values[n_train_hours:n_valid_hours, :]\n",
    "test = reframed_values[n_valid_hours:, :]\n",
    "\n",
    "\n",
    "# split into input and outputs\n",
    "n_obs = n_hours * n_features\n",
    "feature_idx = 4\n",
    "train_X, train_y = train[:, :n_obs], train[:, [n_obs + feature_idx + n_features * i for i in range(k)]]\n",
    "val_X, val_y = val[:, :n_obs], val[:, [n_obs + feature_idx + n_features * i for i in range(k)]]\n",
    "test_X, test_y = test[:, :n_obs], test[:, [n_obs + feature_idx + n_features * i for i in range(k)]]\n",
    "\n",
    "\n",
    "print(\"train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape\", \n",
    "      train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1653a1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape (24486, 72, 26) (24486, 12) (5247, 72, 26) (5247, 12) (5247, 72, 26) (5247, 12)\n"
     ]
    }
   ],
   "source": [
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "train_y = scaler.fit_transform(train_y)\n",
    "\n",
    "val_X = scaler.fit_transform(val_X)\n",
    "val_y = scaler.fit_transform(val_y)\n",
    "\n",
    "test_X = scaler.fit_transform(test_X)\n",
    "test_y = scaler.fit_transform(test_y)\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
    "val_X = val_X.reshape((val_X.shape[0], n_hours, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
    "\n",
    "print(\"train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape\", \n",
    "      train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd896d5",
   "metadata": {},
   "source": [
    "### PM threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d8885dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24486,)\n",
      "(5247,)\n",
      "(5247,)\n"
     ]
    }
   ],
   "source": [
    "train_X_pm = train_X[:, 0, feature_idx]\n",
    "print(train_X_pm.shape)\n",
    "\n",
    "val_X_pm = val_X[:, 0, feature_idx]\n",
    "print(val_X_pm.shape)\n",
    "\n",
    "test_X_pm = test_X[:, 0, feature_idx]\n",
    "print(test_X_pm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f154878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_nums = 20\n",
    "\n",
    "# Step 1: Calculate the histogram\n",
    "counts, bin_edges = np.histogram(train_X_pm, bins=bin_nums)\n",
    "\n",
    "# Step 2: Invert counts to assign lower weights to more frequent bins, avoid division by zero by adding a small number (epsilon)\n",
    "epsilon = 1e-8\n",
    "weights = np.sqrt(1.0 / (counts + epsilon))\n",
    "# weights = 1.0 / (counts + epsilon)\n",
    "\n",
    "# Step 3: Normalize the weights (optional)\n",
    "# weights /= np.max(weights)\n",
    "weights /= np.sum(weights) * len(counts)\n",
    "\n",
    "# Step 4: Assign weights to each sample based on the bin it falls into\n",
    "sample_weights = np.zeros(train_X_pm.shape[0])\n",
    "\n",
    "for i, value in enumerate(train_X_pm):\n",
    "    \n",
    "    # Find the index of the bin this sample falls into\n",
    "    bin_index = np.digitize(value, bin_edges) - 1\n",
    "    bin_index = min(bin_index, bin_nums - 1)\n",
    "    \n",
    "    # Assign the corresponding weight\n",
    "    sample_weights[i] = weights[bin_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ba3913c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24486,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b70ef2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_weights /= np.sum(sample_weights)\n",
    "sample_weights /= np.max(sample_weights)\n",
    "# sample_weights = (sample_weights - sample_weights.min()) / (sample_weights.max()-sample_weights.min()) + 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "69798e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([21558.,  1669.,   464.,   328.,   288.,     0.,     0.,     0.,\n",
       "          105.,     0.,     0.,     0.,     0.,    50.,     0.,     0.,\n",
       "            0.,     0.,     0.,    24.]),\n",
       " array([0.10056729, 0.14553893, 0.19051056, 0.2354822 , 0.28045383,\n",
       "        0.32542547, 0.3703971 , 0.41536874, 0.46034037, 0.50531201,\n",
       "        0.55028365, 0.59525528, 0.64022692, 0.68519855, 0.73017019,\n",
       "        0.77514182, 0.82011346, 0.86508509, 0.91005673, 0.95502836,\n",
       "        1.        ]),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sample_weights, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "85808ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('sample_weights_load_IPF.npy', sample_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa5ce3",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5435f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== model parameters ======\n",
    "mlp_unit1 = 128\n",
    "mlp_unit2 = 128\n",
    "mlp_unit3 = 64\n",
    "mlp_unit4 = 64\n",
    "mlp_unit5 = 32\n",
    "mlp_unit6 = 32\n",
    "mlp_unit7 = 16\n",
    "mlp_unit8 = 16\n",
    "dropout = 0.0\n",
    "kernel_size = 2\n",
    "pool_size = 2\n",
    "learning_rate = 1e-4\n",
    "decay_steps = 10000\n",
    "decay_rate = 0.95\n",
    "PATIENCE = 100\n",
    "EPOCHS = 1000\n",
    "BATCH = 512\n",
    "opt_num = k\n",
    "input_shape = train_X.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0605f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlp_layer(input_shape=input_shape,\n",
    "                   mlp_unit1=mlp_unit1,\n",
    "                   mlp_unit2=mlp_unit2,\n",
    "                   mlp_unit3=mlp_unit3,\n",
    "                   mlp_unit4=mlp_unit4,\n",
    "                   mlp_unit5=mlp_unit5,\n",
    "                   mlp_unit6=mlp_unit6,\n",
    "                   mlp_unit7=mlp_unit7,\n",
    "                   mlp_unit8=mlp_unit8,\n",
    "                   dropout=dropout,\n",
    "                   masked_value=-1,\n",
    "                   opt_num=opt_num\n",
    "                  )\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "43622a95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "48/48 - 2s - loss: 0.0080 - mae: 0.2013 - val_loss: 0.0482 - val_mae: 0.1873\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 0.18727, saving model to ../../saved_models/hydro_all_weighted_IPF_95.h5\n",
      "Epoch 2/1000\n",
      "48/48 - 1s - loss: 0.0040 - mae: 0.1472 - val_loss: 0.0274 - val_mae: 0.1319\n",
      "\n",
      "Epoch 00002: val_mae improved from 0.18727 to 0.13186, saving model to ../../saved_models/hydro_all_weighted_IPF_95.h5\n",
      "Epoch 3/1000\n",
      "48/48 - 1s - loss: 0.0019 - mae: 0.0945 - val_loss: 0.0165 - val_mae: 0.0999\n",
      "\n",
      "Epoch 00003: val_mae improved from 0.13186 to 0.09993, saving model to ../../saved_models/hydro_all_weighted_IPF_95.h5\n",
      "Epoch 4/1000\n",
      "48/48 - 1s - loss: 0.0015 - mae: 0.0826 - val_loss: 0.0143 - val_mae: 0.0920\n",
      "\n",
      "Epoch 00004: val_mae improved from 0.09993 to 0.09199, saving model to ../../saved_models/hydro_all_weighted_IPF_95.h5\n",
      "Epoch 5/1000\n",
      "48/48 - 1s - loss: 0.0013 - mae: 0.0773 - val_loss: 0.0133 - val_mae: 0.0870\n",
      "\n",
      "Epoch 00005: val_mae improved from 0.09199 to 0.08702, saving model to ../../saved_models/hydro_all_weighted_IPF_95.h5\n",
      "Epoch 6/1000\n",
      "48/48 - 1s - loss: 0.0012 - mae: 0.0723 - val_loss: 0.0127 - val_mae: 0.0854\n",
      "\n",
      "Epoch 00006: val_mae improved from 0.08702 to 0.08541, saving model to ../../saved_models/hydro_all_weighted_IPF_95.h5\n",
      "Epoch 7/1000\n",
      "48/48 - 1s - loss: 0.0011 - mae: 0.0689 - val_loss: 0.0123 - val_mae: 0.0851\n",
      "\n",
      "Epoch 00007: val_mae improved from 0.08541 to 0.08506, saving model to ../../saved_models/hydro_all_weighted_IPF_95.h5\n",
      "Epoch 8/1000\n",
      "48/48 - 1s - loss: 9.9858e-04 - mae: 0.0666 - val_loss: 0.0119 - val_mae: 0.0828\n",
      "\n",
      "Epoch 00008: val_mae improved from 0.08506 to 0.08282, saving model to ../../saved_models/hydro_all_weighted_IPF_95.h5\n",
      "Epoch 9/1000\n",
      "48/48 - 1s - loss: 9.8181e-04 - mae: 0.0666 - val_loss: 0.0112 - val_mae: 0.0783\n",
      "\n",
      "Epoch 00009: val_mae improved from 0.08282 to 0.07833, saving model to ../../saved_models/hydro_all_weighted_IPF_95.h5\n",
      "Epoch 10/1000\n",
      "48/48 - 1s - loss: 8.8524e-04 - mae: 0.0626 - val_loss: 0.0107 - val_mae: 0.0774\n",
      "\n",
      "Epoch 00010: val_mae improved from 0.07833 to 0.07739, saving model to ../../saved_models/hydro_all_weighted_IPF_95.h5\n",
      "Epoch 11/1000\n",
      "48/48 - 1s - loss: 8.1819e-04 - mae: 0.0600 - val_loss: 0.0107 - val_mae: 0.0766\n",
      "\n",
      "Epoch 00011: val_mae improved from 0.07739 to 0.07664, saving model to ../../saved_models/hydro_all_weighted_IPF_95.h5\n",
      "Epoch 12/1000\n",
      "48/48 - 1s - loss: 7.8157e-04 - mae: 0.0587 - val_loss: 0.0105 - val_mae: 0.0764\n",
      "\n",
      "Epoch 00012: val_mae improved from 0.07664 to 0.07639, saving model to ../../saved_models/hydro_all_weighted_IPF_95.h5\n",
      "Epoch 13/1000\n",
      "48/48 - 1s - loss: 7.3702e-04 - mae: 0.0570 - val_loss: 0.0107 - val_mae: 0.0757\n",
      "\n",
      "Epoch 00013: val_mae improved from 0.07639 to 0.07565, saving model to ../../saved_models/hydro_all_weighted_IPF_95.h5\n",
      "Epoch 14/1000\n",
      "48/48 - 1s - loss: 7.0213e-04 - mae: 0.0557 - val_loss: 0.0116 - val_mae: 0.0818\n",
      "\n",
      "Epoch 00014: val_mae did not improve from 0.07565\n",
      "Epoch 15/1000\n",
      "48/48 - 1s - loss: 6.7082e-04 - mae: 0.0544 - val_loss: 0.0106 - val_mae: 0.0763\n",
      "\n",
      "Epoch 00015: val_mae did not improve from 0.07565\n",
      "Epoch 16/1000\n",
      "48/48 - 1s - loss: 6.6475e-04 - mae: 0.0545 - val_loss: 0.0118 - val_mae: 0.0819\n",
      "\n",
      "Epoch 00016: val_mae did not improve from 0.07565\n",
      "Epoch 17/1000\n",
      "48/48 - 1s - loss: 6.2719e-04 - mae: 0.0529 - val_loss: 0.0111 - val_mae: 0.0760\n",
      "\n",
      "Epoch 00017: val_mae did not improve from 0.07565\n",
      "Epoch 18/1000\n",
      "48/48 - 1s - loss: 6.3848e-04 - mae: 0.0538 - val_loss: 0.0105 - val_mae: 0.0748\n",
      "\n",
      "Epoch 00018: val_mae improved from 0.07565 to 0.07477, saving model to ../../saved_models/hydro_all_weighted_IPF_95.h5\n",
      "Epoch 19/1000\n",
      "48/48 - 1s - loss: 6.0483e-04 - mae: 0.0522 - val_loss: 0.0103 - val_mae: 0.0758\n",
      "\n",
      "Epoch 00019: val_mae did not improve from 0.07477\n",
      "Epoch 20/1000\n",
      "48/48 - 1s - loss: 5.4903e-04 - mae: 0.0494 - val_loss: 0.0117 - val_mae: 0.0784\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 0.07477\n",
      "Epoch 21/1000\n",
      "48/48 - 1s - loss: 5.4880e-04 - mae: 0.0497 - val_loss: 0.0124 - val_mae: 0.0808\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 0.07477\n",
      "Epoch 22/1000\n",
      "48/48 - 1s - loss: 5.1184e-04 - mae: 0.0478 - val_loss: 0.0122 - val_mae: 0.0808\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 0.07477\n",
      "Epoch 23/1000\n",
      "48/48 - 1s - loss: 4.9317e-04 - mae: 0.0470 - val_loss: 0.0123 - val_mae: 0.0810\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 0.07477\n",
      "Epoch 24/1000\n",
      "48/48 - 1s - loss: 4.8304e-04 - mae: 0.0465 - val_loss: 0.0136 - val_mae: 0.0844\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 0.07477\n",
      "Epoch 25/1000\n",
      "48/48 - 1s - loss: 4.8489e-04 - mae: 0.0469 - val_loss: 0.0133 - val_mae: 0.0834\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 0.07477\n",
      "Epoch 26/1000\n",
      "48/48 - 1s - loss: 4.5496e-04 - mae: 0.0453 - val_loss: 0.0144 - val_mae: 0.0867\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 0.07477\n",
      "Epoch 27/1000\n",
      "48/48 - 1s - loss: 4.3707e-04 - mae: 0.0444 - val_loss: 0.0148 - val_mae: 0.0874\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 0.07477\n",
      "Epoch 28/1000\n",
      "48/48 - 1s - loss: 4.3385e-04 - mae: 0.0441 - val_loss: 0.0133 - val_mae: 0.0838\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 0.07477\n",
      "Epoch 29/1000\n",
      "48/48 - 1s - loss: 4.1538e-04 - mae: 0.0434 - val_loss: 0.0146 - val_mae: 0.0876\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 0.07477\n",
      "Epoch 30/1000\n",
      "48/48 - 1s - loss: 4.0030e-04 - mae: 0.0427 - val_loss: 0.0165 - val_mae: 0.0927\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 0.07477\n",
      "Epoch 31/1000\n",
      "48/48 - 1s - loss: 3.9441e-04 - mae: 0.0423 - val_loss: 0.0152 - val_mae: 0.0892\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 0.07477\n",
      "Epoch 32/1000\n",
      "48/48 - 1s - loss: 3.7943e-04 - mae: 0.0416 - val_loss: 0.0181 - val_mae: 0.0980\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 0.07477\n",
      "Epoch 33/1000\n",
      "48/48 - 1s - loss: 3.6279e-04 - mae: 0.0407 - val_loss: 0.0155 - val_mae: 0.0904\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 0.07477\n",
      "Epoch 34/1000\n",
      "48/48 - 1s - loss: 3.5741e-04 - mae: 0.0403 - val_loss: 0.0166 - val_mae: 0.0946\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 0.07477\n",
      "Epoch 35/1000\n",
      "48/48 - 1s - loss: 3.4990e-04 - mae: 0.0401 - val_loss: 0.0182 - val_mae: 0.0988\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 0.07477\n",
      "Epoch 36/1000\n",
      "48/48 - 1s - loss: 3.3025e-04 - mae: 0.0390 - val_loss: 0.0199 - val_mae: 0.1026\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 0.07477\n",
      "Epoch 37/1000\n",
      "48/48 - 1s - loss: 3.4510e-04 - mae: 0.0400 - val_loss: 0.0163 - val_mae: 0.0936\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 0.07477\n",
      "Epoch 38/1000\n",
      "48/48 - 1s - loss: 3.2979e-04 - mae: 0.0390 - val_loss: 0.0217 - val_mae: 0.1074\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 0.07477\n",
      "Epoch 39/1000\n",
      "48/48 - 1s - loss: 3.0600e-04 - mae: 0.0375 - val_loss: 0.0199 - val_mae: 0.1033\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 0.07477\n",
      "Epoch 40/1000\n",
      "48/48 - 1s - loss: 2.9975e-04 - mae: 0.0373 - val_loss: 0.0196 - val_mae: 0.1031\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 0.07477\n",
      "Epoch 41/1000\n",
      "48/48 - 1s - loss: 3.0954e-04 - mae: 0.0380 - val_loss: 0.0188 - val_mae: 0.1008\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 0.07477\n",
      "Epoch 42/1000\n",
      "48/48 - 1s - loss: 2.8747e-04 - mae: 0.0364 - val_loss: 0.0183 - val_mae: 0.1003\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 0.07477\n",
      "Epoch 43/1000\n",
      "48/48 - 1s - loss: 2.8237e-04 - mae: 0.0361 - val_loss: 0.0187 - val_mae: 0.1014\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 0.07477\n",
      "Epoch 44/1000\n",
      "48/48 - 1s - loss: 2.6108e-04 - mae: 0.0348 - val_loss: 0.0207 - val_mae: 0.1070\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 0.07477\n",
      "Epoch 45/1000\n",
      "48/48 - 1s - loss: 2.5326e-04 - mae: 0.0342 - val_loss: 0.0226 - val_mae: 0.1110\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 0.07477\n",
      "Epoch 46/1000\n",
      "48/48 - 1s - loss: 2.5969e-04 - mae: 0.0347 - val_loss: 0.0187 - val_mae: 0.1016\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 0.07477\n",
      "Epoch 47/1000\n",
      "48/48 - 1s - loss: 2.5599e-04 - mae: 0.0344 - val_loss: 0.0227 - val_mae: 0.1119\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 0.07477\n",
      "Epoch 48/1000\n",
      "48/48 - 1s - loss: 2.4190e-04 - mae: 0.0335 - val_loss: 0.0221 - val_mae: 0.1108\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 0.07477\n",
      "Epoch 49/1000\n",
      "48/48 - 1s - loss: 2.2688e-04 - mae: 0.0324 - val_loss: 0.0207 - val_mae: 0.1068\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 0.07477\n",
      "Epoch 50/1000\n",
      "48/48 - 1s - loss: 2.2117e-04 - mae: 0.0320 - val_loss: 0.0208 - val_mae: 0.1070\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 0.07477\n",
      "Epoch 51/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 - 1s - loss: 2.2478e-04 - mae: 0.0324 - val_loss: 0.0217 - val_mae: 0.1097\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 0.07477\n",
      "Epoch 52/1000\n",
      "48/48 - 1s - loss: 2.1026e-04 - mae: 0.0312 - val_loss: 0.0241 - val_mae: 0.1154\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 0.07477\n",
      "Epoch 53/1000\n",
      "48/48 - 1s - loss: 2.0910e-04 - mae: 0.0312 - val_loss: 0.0240 - val_mae: 0.1158\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 0.07477\n",
      "Epoch 54/1000\n",
      "48/48 - 1s - loss: 2.0697e-04 - mae: 0.0310 - val_loss: 0.0249 - val_mae: 0.1174\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 0.07477\n",
      "Epoch 55/1000\n",
      "48/48 - 1s - loss: 1.9914e-04 - mae: 0.0304 - val_loss: 0.0261 - val_mae: 0.1193\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 0.07477\n",
      "Epoch 56/1000\n",
      "48/48 - 1s - loss: 1.9662e-04 - mae: 0.0302 - val_loss: 0.0242 - val_mae: 0.1156\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 0.07477\n",
      "Epoch 57/1000\n",
      "48/48 - 1s - loss: 1.9117e-04 - mae: 0.0298 - val_loss: 0.0246 - val_mae: 0.1173\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 0.07477\n",
      "Epoch 58/1000\n",
      "48/48 - 1s - loss: 1.9634e-04 - mae: 0.0303 - val_loss: 0.0272 - val_mae: 0.1229\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 0.07477\n",
      "Epoch 59/1000\n",
      "48/48 - 1s - loss: 1.8358e-04 - mae: 0.0292 - val_loss: 0.0251 - val_mae: 0.1182\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 0.07477\n",
      "Epoch 60/1000\n",
      "48/48 - 1s - loss: 1.7713e-04 - mae: 0.0287 - val_loss: 0.0242 - val_mae: 0.1162\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 0.07477\n",
      "Epoch 61/1000\n",
      "48/48 - 1s - loss: 1.8543e-04 - mae: 0.0296 - val_loss: 0.0273 - val_mae: 0.1235\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 0.07477\n",
      "Epoch 62/1000\n",
      "48/48 - 1s - loss: 1.8185e-04 - mae: 0.0291 - val_loss: 0.0239 - val_mae: 0.1159\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 0.07477\n",
      "Epoch 63/1000\n",
      "48/48 - 1s - loss: 1.7318e-04 - mae: 0.0284 - val_loss: 0.0276 - val_mae: 0.1238\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 0.07477\n",
      "Epoch 64/1000\n",
      "48/48 - 1s - loss: 1.5996e-04 - mae: 0.0273 - val_loss: 0.0247 - val_mae: 0.1173\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 0.07477\n",
      "Epoch 65/1000\n",
      "48/48 - 1s - loss: 1.5591e-04 - mae: 0.0269 - val_loss: 0.0272 - val_mae: 0.1235\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 0.07477\n",
      "Epoch 66/1000\n",
      "48/48 - 1s - loss: 1.6021e-04 - mae: 0.0274 - val_loss: 0.0235 - val_mae: 0.1148\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 0.07477\n",
      "Epoch 67/1000\n",
      "48/48 - 1s - loss: 1.7934e-04 - mae: 0.0289 - val_loss: 0.0244 - val_mae: 0.1175\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 0.07477\n",
      "Epoch 68/1000\n",
      "48/48 - 1s - loss: 1.5743e-04 - mae: 0.0271 - val_loss: 0.0257 - val_mae: 0.1203\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 0.07477\n",
      "Epoch 69/1000\n",
      "48/48 - 1s - loss: 1.5802e-04 - mae: 0.0272 - val_loss: 0.0296 - val_mae: 0.1290\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 0.07477\n",
      "Epoch 70/1000\n",
      "48/48 - 1s - loss: 1.5486e-04 - mae: 0.0269 - val_loss: 0.0273 - val_mae: 0.1237\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 0.07477\n",
      "Epoch 71/1000\n",
      "48/48 - 1s - loss: 1.3953e-04 - mae: 0.0254 - val_loss: 0.0261 - val_mae: 0.1211\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 0.07477\n",
      "Epoch 72/1000\n",
      "48/48 - 1s - loss: 1.3148e-04 - mae: 0.0247 - val_loss: 0.0282 - val_mae: 0.1258\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 0.07477\n",
      "Epoch 73/1000\n",
      "48/48 - 1s - loss: 1.4100e-04 - mae: 0.0256 - val_loss: 0.0288 - val_mae: 0.1272\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 0.07477\n",
      "Epoch 74/1000\n",
      "48/48 - 1s - loss: 1.5381e-04 - mae: 0.0267 - val_loss: 0.0251 - val_mae: 0.1191\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 0.07477\n",
      "Epoch 75/1000\n",
      "48/48 - 1s - loss: 1.2927e-04 - mae: 0.0245 - val_loss: 0.0280 - val_mae: 0.1257\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 0.07477\n",
      "Epoch 76/1000\n",
      "48/48 - 1s - loss: 1.2812e-04 - mae: 0.0244 - val_loss: 0.0290 - val_mae: 0.1281\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 0.07477\n",
      "Epoch 77/1000\n",
      "48/48 - 1s - loss: 1.3646e-04 - mae: 0.0253 - val_loss: 0.0266 - val_mae: 0.1225\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 0.07477\n",
      "Epoch 78/1000\n",
      "48/48 - 1s - loss: 1.3079e-04 - mae: 0.0246 - val_loss: 0.0290 - val_mae: 0.1281\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 0.07477\n",
      "Epoch 79/1000\n",
      "48/48 - 1s - loss: 1.2382e-04 - mae: 0.0239 - val_loss: 0.0278 - val_mae: 0.1252\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 0.07477\n",
      "Epoch 80/1000\n",
      "48/48 - 1s - loss: 1.3359e-04 - mae: 0.0249 - val_loss: 0.0270 - val_mae: 0.1239\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 0.07477\n",
      "Epoch 81/1000\n",
      "48/48 - 1s - loss: 1.2039e-04 - mae: 0.0236 - val_loss: 0.0290 - val_mae: 0.1279\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 0.07477\n",
      "Epoch 82/1000\n",
      "48/48 - 1s - loss: 1.1885e-04 - mae: 0.0235 - val_loss: 0.0280 - val_mae: 0.1259\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 0.07477\n",
      "Epoch 83/1000\n",
      "48/48 - 1s - loss: 1.1767e-04 - mae: 0.0234 - val_loss: 0.0283 - val_mae: 0.1266\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 0.07477\n",
      "Epoch 84/1000\n",
      "48/48 - 1s - loss: 1.6163e-04 - mae: 0.0274 - val_loss: 0.0263 - val_mae: 0.1224\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 0.07477\n",
      "Epoch 85/1000\n",
      "48/48 - 1s - loss: 1.1492e-04 - mae: 0.0230 - val_loss: 0.0279 - val_mae: 0.1259\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 0.07477\n",
      "Epoch 86/1000\n",
      "48/48 - 1s - loss: 1.1374e-04 - mae: 0.0230 - val_loss: 0.0274 - val_mae: 0.1249\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 0.07477\n",
      "Epoch 87/1000\n",
      "48/48 - 1s - loss: 1.0854e-04 - mae: 0.0224 - val_loss: 0.0285 - val_mae: 0.1274\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 0.07477\n",
      "Epoch 88/1000\n",
      "48/48 - 1s - loss: 1.1248e-04 - mae: 0.0228 - val_loss: 0.0289 - val_mae: 0.1281\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 0.07477\n",
      "Epoch 89/1000\n",
      "48/48 - 1s - loss: 1.1687e-04 - mae: 0.0233 - val_loss: 0.0272 - val_mae: 0.1244\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 0.07477\n",
      "Epoch 90/1000\n",
      "48/48 - 1s - loss: 1.1057e-04 - mae: 0.0226 - val_loss: 0.0310 - val_mae: 0.1326\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 0.07477\n",
      "Epoch 91/1000\n",
      "48/48 - 1s - loss: 1.0976e-04 - mae: 0.0225 - val_loss: 0.0276 - val_mae: 0.1256\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 0.07477\n",
      "Epoch 92/1000\n",
      "48/48 - 1s - loss: 1.0257e-04 - mae: 0.0217 - val_loss: 0.0292 - val_mae: 0.1291\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 0.07477\n",
      "Epoch 93/1000\n",
      "48/48 - 1s - loss: 9.6702e-05 - mae: 0.0211 - val_loss: 0.0292 - val_mae: 0.1290\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 0.07477\n",
      "Epoch 94/1000\n",
      "48/48 - 1s - loss: 9.9725e-05 - mae: 0.0215 - val_loss: 0.0305 - val_mae: 0.1317\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 0.07477\n",
      "Epoch 95/1000\n",
      "48/48 - 1s - loss: 1.4501e-04 - mae: 0.0258 - val_loss: 0.0247 - val_mae: 0.1194\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 0.07477\n",
      "Epoch 96/1000\n",
      "48/48 - 1s - loss: 1.3160e-04 - mae: 0.0248 - val_loss: 0.0269 - val_mae: 0.1241\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 0.07477\n",
      "Epoch 97/1000\n",
      "48/48 - 1s - loss: 9.9796e-05 - mae: 0.0214 - val_loss: 0.0297 - val_mae: 0.1300\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 0.07477\n",
      "Epoch 98/1000\n",
      "48/48 - 1s - loss: 9.9141e-05 - mae: 0.0214 - val_loss: 0.0281 - val_mae: 0.1268\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 0.07477\n",
      "Epoch 99/1000\n",
      "48/48 - 1s - loss: 8.9126e-05 - mae: 0.0202 - val_loss: 0.0283 - val_mae: 0.1273\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 0.07477\n",
      "Epoch 100/1000\n",
      "48/48 - 1s - loss: 9.0873e-05 - mae: 0.0204 - val_loss: 0.0291 - val_mae: 0.1291\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 0.07477\n",
      "Epoch 101/1000\n",
      "48/48 - 1s - loss: 8.8510e-05 - mae: 0.0201 - val_loss: 0.0298 - val_mae: 0.1305\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 0.07477\n",
      "Epoch 102/1000\n",
      "48/48 - 1s - loss: 8.8119e-05 - mae: 0.0201 - val_loss: 0.0297 - val_mae: 0.1304\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 0.07477\n",
      "Epoch 103/1000\n",
      "48/48 - 1s - loss: 1.0009e-04 - mae: 0.0215 - val_loss: 0.0326 - val_mae: 0.1364\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 0.07477\n",
      "Epoch 104/1000\n",
      "48/48 - 1s - loss: 1.1343e-04 - mae: 0.0229 - val_loss: 0.0287 - val_mae: 0.1288\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 0.07477\n",
      "Epoch 105/1000\n",
      "48/48 - 1s - loss: 1.0084e-04 - mae: 0.0216 - val_loss: 0.0324 - val_mae: 0.1360\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 0.07477\n",
      "Epoch 106/1000\n",
      "48/48 - 1s - loss: 8.7936e-05 - mae: 0.0200 - val_loss: 0.0304 - val_mae: 0.1319\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 0.07477\n",
      "Epoch 107/1000\n",
      "48/48 - 1s - loss: 8.4108e-05 - mae: 0.0196 - val_loss: 0.0311 - val_mae: 0.1333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00107: val_mae did not improve from 0.07477\n",
      "Epoch 108/1000\n",
      "48/48 - 1s - loss: 7.9012e-05 - mae: 0.0189 - val_loss: 0.0304 - val_mae: 0.1318\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 0.07477\n",
      "Epoch 109/1000\n",
      "48/48 - 1s - loss: 8.7079e-05 - mae: 0.0199 - val_loss: 0.0313 - val_mae: 0.1337\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 0.07477\n",
      "Epoch 110/1000\n",
      "48/48 - 1s - loss: 8.7271e-05 - mae: 0.0200 - val_loss: 0.0313 - val_mae: 0.1337\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 0.07477\n",
      "Epoch 111/1000\n",
      "48/48 - 1s - loss: 8.3761e-05 - mae: 0.0195 - val_loss: 0.0297 - val_mae: 0.1305\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 0.07477\n",
      "Epoch 112/1000\n",
      "48/48 - 1s - loss: 9.9170e-05 - mae: 0.0215 - val_loss: 0.0309 - val_mae: 0.1331\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 0.07477\n",
      "Epoch 113/1000\n",
      "48/48 - 1s - loss: 8.3405e-05 - mae: 0.0195 - val_loss: 0.0310 - val_mae: 0.1334\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 0.07477\n",
      "Epoch 114/1000\n",
      "48/48 - 1s - loss: 7.9048e-05 - mae: 0.0190 - val_loss: 0.0325 - val_mae: 0.1364\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 0.07477\n",
      "Epoch 115/1000\n",
      "48/48 - 1s - loss: 9.0436e-05 - mae: 0.0204 - val_loss: 0.0315 - val_mae: 0.1341\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 0.07477\n",
      "Epoch 116/1000\n",
      "48/48 - 1s - loss: 7.9544e-05 - mae: 0.0190 - val_loss: 0.0333 - val_mae: 0.1380\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 0.07477\n",
      "Epoch 117/1000\n",
      "48/48 - 1s - loss: 8.2290e-05 - mae: 0.0194 - val_loss: 0.0310 - val_mae: 0.1336\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 0.07477\n",
      "Epoch 118/1000\n",
      "48/48 - 1s - loss: 8.0079e-05 - mae: 0.0191 - val_loss: 0.0322 - val_mae: 0.1358\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 0.07477\n",
      "Epoch 119/1000\n",
      "48/48 - 1s - loss: 7.9222e-05 - mae: 0.0190 - val_loss: 0.0316 - val_mae: 0.1345\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 0.07477\n",
      "Epoch 120/1000\n",
      "48/48 - 1s - loss: 8.6806e-05 - mae: 0.0200 - val_loss: 0.0354 - val_mae: 0.1420\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 0.07477\n",
      "Epoch 121/1000\n",
      "48/48 - 1s - loss: 3.1580e-04 - mae: 0.0376 - val_loss: 0.0252 - val_mae: 0.1203\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 0.07477\n",
      "Epoch 122/1000\n",
      "48/48 - 1s - loss: 1.0747e-04 - mae: 0.0222 - val_loss: 0.0268 - val_mae: 0.1241\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 0.07477\n",
      "Epoch 123/1000\n",
      "48/48 - 1s - loss: 7.5172e-05 - mae: 0.0184 - val_loss: 0.0281 - val_mae: 0.1269\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 0.07477\n",
      "Epoch 124/1000\n",
      "48/48 - 1s - loss: 7.1575e-05 - mae: 0.0179 - val_loss: 0.0299 - val_mae: 0.1308\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 0.07477\n",
      "Epoch 125/1000\n",
      "48/48 - 1s - loss: 6.9191e-05 - mae: 0.0176 - val_loss: 0.0296 - val_mae: 0.1302\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 0.07477\n",
      "Epoch 126/1000\n",
      "48/48 - 1s - loss: 6.9463e-05 - mae: 0.0177 - val_loss: 0.0309 - val_mae: 0.1330\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 0.07477\n",
      "Epoch 127/1000\n",
      "48/48 - 1s - loss: 6.8863e-05 - mae: 0.0176 - val_loss: 0.0300 - val_mae: 0.1310\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 0.07477\n",
      "Epoch 128/1000\n",
      "48/48 - 1s - loss: 6.8077e-05 - mae: 0.0175 - val_loss: 0.0313 - val_mae: 0.1340\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 0.07477\n",
      "Epoch 129/1000\n",
      "48/48 - 1s - loss: 6.9610e-05 - mae: 0.0177 - val_loss: 0.0298 - val_mae: 0.1308\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 0.07477\n",
      "Epoch 130/1000\n",
      "48/48 - 1s - loss: 7.3344e-05 - mae: 0.0182 - val_loss: 0.0307 - val_mae: 0.1327\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 0.07477\n",
      "Epoch 131/1000\n",
      "48/48 - 1s - loss: 6.9529e-05 - mae: 0.0177 - val_loss: 0.0292 - val_mae: 0.1296\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 0.07477\n",
      "Epoch 132/1000\n",
      "48/48 - 1s - loss: 9.3442e-05 - mae: 0.0209 - val_loss: 0.0309 - val_mae: 0.1339\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 0.07477\n",
      "Epoch 133/1000\n",
      "48/48 - 1s - loss: 1.2819e-04 - mae: 0.0245 - val_loss: 0.0289 - val_mae: 0.1289\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 0.07477\n",
      "Epoch 134/1000\n",
      "48/48 - 1s - loss: 7.9275e-05 - mae: 0.0191 - val_loss: 0.0304 - val_mae: 0.1320\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 0.07477\n",
      "Epoch 135/1000\n",
      "48/48 - 1s - loss: 6.5878e-05 - mae: 0.0172 - val_loss: 0.0307 - val_mae: 0.1324\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 0.07477\n",
      "Epoch 136/1000\n",
      "48/48 - 1s - loss: 6.6532e-05 - mae: 0.0173 - val_loss: 0.0312 - val_mae: 0.1338\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 0.07477\n",
      "Epoch 137/1000\n",
      "48/48 - 1s - loss: 6.5503e-05 - mae: 0.0171 - val_loss: 0.0313 - val_mae: 0.1338\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 0.07477\n",
      "Epoch 138/1000\n",
      "48/48 - 1s - loss: 6.2683e-05 - mae: 0.0167 - val_loss: 0.0315 - val_mae: 0.1341\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 0.07477\n",
      "Epoch 139/1000\n",
      "48/48 - 1s - loss: 6.2603e-05 - mae: 0.0167 - val_loss: 0.0308 - val_mae: 0.1328\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 0.07477\n",
      "Epoch 140/1000\n",
      "48/48 - 1s - loss: 6.3006e-05 - mae: 0.0168 - val_loss: 0.0310 - val_mae: 0.1336\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 0.07477\n",
      "Epoch 141/1000\n",
      "48/48 - 1s - loss: 6.3078e-05 - mae: 0.0168 - val_loss: 0.0322 - val_mae: 0.1359\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 0.07477\n",
      "Epoch 142/1000\n",
      "48/48 - 1s - loss: 6.2312e-05 - mae: 0.0167 - val_loss: 0.0313 - val_mae: 0.1339\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 0.07477\n",
      "Epoch 143/1000\n",
      "48/48 - 1s - loss: 6.5254e-05 - mae: 0.0171 - val_loss: 0.0337 - val_mae: 0.1390\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 0.07477\n",
      "Epoch 144/1000\n",
      "48/48 - 1s - loss: 9.9254e-05 - mae: 0.0214 - val_loss: 0.0299 - val_mae: 0.1312\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 0.07477\n",
      "Epoch 145/1000\n",
      "48/48 - 1s - loss: 7.4450e-05 - mae: 0.0184 - val_loss: 0.0324 - val_mae: 0.1360\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 0.07477\n",
      "Epoch 146/1000\n",
      "48/48 - 1s - loss: 6.7421e-05 - mae: 0.0175 - val_loss: 0.0301 - val_mae: 0.1312\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 0.07477\n",
      "Epoch 147/1000\n",
      "48/48 - 1s - loss: 5.9085e-05 - mae: 0.0162 - val_loss: 0.0305 - val_mae: 0.1323\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 0.07477\n",
      "Epoch 148/1000\n",
      "48/48 - 1s - loss: 6.0376e-05 - mae: 0.0164 - val_loss: 0.0319 - val_mae: 0.1352\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 0.07477\n",
      "Epoch 149/1000\n",
      "48/48 - 1s - loss: 5.6835e-05 - mae: 0.0159 - val_loss: 0.0317 - val_mae: 0.1349\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 0.07477\n",
      "Epoch 150/1000\n",
      "48/48 - 1s - loss: 5.8115e-05 - mae: 0.0161 - val_loss: 0.0313 - val_mae: 0.1341\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 0.07477\n",
      "Epoch 151/1000\n",
      "48/48 - 1s - loss: 6.4404e-05 - mae: 0.0171 - val_loss: 0.0302 - val_mae: 0.1320\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 0.07477\n",
      "Epoch 152/1000\n",
      "48/48 - 1s - loss: 7.4566e-05 - mae: 0.0185 - val_loss: 0.0331 - val_mae: 0.1372\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 0.07477\n",
      "Epoch 153/1000\n",
      "48/48 - 1s - loss: 6.1854e-05 - mae: 0.0167 - val_loss: 0.0320 - val_mae: 0.1356\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 0.07477\n",
      "Epoch 154/1000\n",
      "48/48 - 1s - loss: 6.0598e-05 - mae: 0.0165 - val_loss: 0.0325 - val_mae: 0.1366\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 0.07477\n",
      "Epoch 155/1000\n",
      "48/48 - 1s - loss: 7.1742e-05 - mae: 0.0182 - val_loss: 0.0309 - val_mae: 0.1334\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 0.07477\n",
      "Epoch 156/1000\n",
      "48/48 - 1s - loss: 6.2141e-05 - mae: 0.0167 - val_loss: 0.0322 - val_mae: 0.1360\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 0.07477\n",
      "Epoch 157/1000\n",
      "48/48 - 1s - loss: 6.3240e-05 - mae: 0.0169 - val_loss: 0.0319 - val_mae: 0.1351\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 0.07477\n",
      "Epoch 158/1000\n",
      "48/48 - 1s - loss: 5.5293e-05 - mae: 0.0157 - val_loss: 0.0324 - val_mae: 0.1364\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 0.07477\n",
      "Epoch 159/1000\n",
      "48/48 - 1s - loss: 6.2141e-05 - mae: 0.0168 - val_loss: 0.0303 - val_mae: 0.1323\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 0.07477\n",
      "Epoch 160/1000\n",
      "48/48 - 1s - loss: 6.5885e-05 - mae: 0.0173 - val_loss: 0.0325 - val_mae: 0.1364\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 0.07477\n",
      "Epoch 161/1000\n",
      "48/48 - 1s - loss: 5.6093e-05 - mae: 0.0158 - val_loss: 0.0313 - val_mae: 0.1341\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 0.07477\n",
      "Epoch 162/1000\n",
      "48/48 - 1s - loss: 6.0581e-05 - mae: 0.0166 - val_loss: 0.0313 - val_mae: 0.1339\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 0.07477\n",
      "Epoch 163/1000\n",
      "48/48 - 1s - loss: 8.9284e-05 - mae: 0.0202 - val_loss: 0.0329 - val_mae: 0.1371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00163: val_mae did not improve from 0.07477\n",
      "Epoch 164/1000\n",
      "48/48 - 1s - loss: 5.6631e-05 - mae: 0.0159 - val_loss: 0.0315 - val_mae: 0.1341\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 0.07477\n",
      "Epoch 165/1000\n",
      "48/48 - 1s - loss: 5.4731e-05 - mae: 0.0156 - val_loss: 0.0318 - val_mae: 0.1350\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 0.07477\n",
      "Epoch 166/1000\n",
      "48/48 - 1s - loss: 6.7751e-05 - mae: 0.0176 - val_loss: 0.0316 - val_mae: 0.1348\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 0.07477\n",
      "Epoch 167/1000\n",
      "48/48 - 1s - loss: 5.5125e-05 - mae: 0.0157 - val_loss: 0.0317 - val_mae: 0.1350\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 0.07477\n",
      "Epoch 168/1000\n",
      "48/48 - 1s - loss: 6.3685e-05 - mae: 0.0170 - val_loss: 0.0331 - val_mae: 0.1373\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 0.07477\n",
      "Epoch 169/1000\n",
      "48/48 - 1s - loss: 5.5901e-05 - mae: 0.0158 - val_loss: 0.0306 - val_mae: 0.1323\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 0.07477\n",
      "Epoch 170/1000\n",
      "48/48 - 1s - loss: 8.1636e-05 - mae: 0.0195 - val_loss: 0.0316 - val_mae: 0.1348\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 0.07477\n",
      "Epoch 171/1000\n",
      "48/48 - 1s - loss: 6.1978e-05 - mae: 0.0167 - val_loss: 0.0327 - val_mae: 0.1370\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 0.07477\n",
      "Epoch 172/1000\n",
      "48/48 - 1s - loss: 5.6779e-05 - mae: 0.0160 - val_loss: 0.0326 - val_mae: 0.1369\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 0.07477\n",
      "Epoch 173/1000\n",
      "48/48 - 1s - loss: 5.6240e-05 - mae: 0.0159 - val_loss: 0.0321 - val_mae: 0.1358\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 0.07477\n",
      "Epoch 174/1000\n",
      "48/48 - 1s - loss: 5.4014e-05 - mae: 0.0156 - val_loss: 0.0328 - val_mae: 0.1372\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 0.07477\n",
      "Epoch 175/1000\n",
      "48/48 - 1s - loss: 5.3397e-05 - mae: 0.0155 - val_loss: 0.0343 - val_mae: 0.1398\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 0.07477\n",
      "Epoch 176/1000\n",
      "48/48 - 1s - loss: 5.2309e-05 - mae: 0.0153 - val_loss: 0.0314 - val_mae: 0.1344\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 0.07477\n",
      "Epoch 177/1000\n",
      "48/48 - 1s - loss: 5.9844e-05 - mae: 0.0165 - val_loss: 0.0320 - val_mae: 0.1355\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 0.07477\n",
      "Epoch 178/1000\n",
      "48/48 - 1s - loss: 5.2395e-05 - mae: 0.0153 - val_loss: 0.0343 - val_mae: 0.1403\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 0.07477\n",
      "Epoch 179/1000\n",
      "48/48 - 1s - loss: 6.0922e-05 - mae: 0.0167 - val_loss: 0.0319 - val_mae: 0.1352\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 0.07477\n",
      "Epoch 180/1000\n",
      "48/48 - 1s - loss: 6.4035e-05 - mae: 0.0171 - val_loss: 0.0327 - val_mae: 0.1371\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 0.07477\n",
      "Epoch 181/1000\n",
      "48/48 - 1s - loss: 4.7400e-05 - mae: 0.0145 - val_loss: 0.0326 - val_mae: 0.1369\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 0.07477\n",
      "Epoch 182/1000\n",
      "48/48 - 1s - loss: 5.2667e-05 - mae: 0.0154 - val_loss: 0.0321 - val_mae: 0.1359\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 0.07477\n",
      "Epoch 183/1000\n",
      "48/48 - 1s - loss: 5.0912e-05 - mae: 0.0151 - val_loss: 0.0337 - val_mae: 0.1390\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 0.07477\n",
      "Epoch 184/1000\n",
      "48/48 - 1s - loss: 5.6344e-05 - mae: 0.0160 - val_loss: 0.0323 - val_mae: 0.1362\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 0.07477\n",
      "Epoch 185/1000\n",
      "48/48 - 1s - loss: 5.6829e-05 - mae: 0.0161 - val_loss: 0.0327 - val_mae: 0.1370\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 0.07477\n",
      "Epoch 186/1000\n",
      "48/48 - 1s - loss: 5.0056e-05 - mae: 0.0150 - val_loss: 0.0331 - val_mae: 0.1379\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 0.07477\n",
      "Epoch 187/1000\n",
      "48/48 - 1s - loss: 4.8467e-05 - mae: 0.0147 - val_loss: 0.0320 - val_mae: 0.1360\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 0.07477\n",
      "Epoch 188/1000\n",
      "48/48 - 1s - loss: 5.2097e-05 - mae: 0.0153 - val_loss: 0.0324 - val_mae: 0.1368\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 0.07477\n",
      "Epoch 189/1000\n",
      "48/48 - 1s - loss: 5.1749e-05 - mae: 0.0153 - val_loss: 0.0330 - val_mae: 0.1379\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 0.07477\n",
      "Epoch 190/1000\n",
      "48/48 - 1s - loss: 5.2485e-05 - mae: 0.0154 - val_loss: 0.0336 - val_mae: 0.1391\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 0.07477\n",
      "Epoch 191/1000\n",
      "48/48 - 1s - loss: 5.8187e-05 - mae: 0.0163 - val_loss: 0.0349 - val_mae: 0.1416\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 0.07477\n",
      "Epoch 192/1000\n",
      "48/48 - 1s - loss: 5.7911e-05 - mae: 0.0162 - val_loss: 0.0328 - val_mae: 0.1371\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 0.07477\n",
      "Epoch 193/1000\n",
      "48/48 - 1s - loss: 5.4573e-05 - mae: 0.0157 - val_loss: 0.0326 - val_mae: 0.1370\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 0.07477\n",
      "Epoch 194/1000\n",
      "48/48 - 1s - loss: 6.0188e-05 - mae: 0.0166 - val_loss: 0.0306 - val_mae: 0.1328\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 0.07477\n",
      "Epoch 195/1000\n",
      "48/48 - 1s - loss: 6.1092e-05 - mae: 0.0167 - val_loss: 0.0327 - val_mae: 0.1370\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 0.07477\n",
      "Epoch 196/1000\n",
      "48/48 - 1s - loss: 4.6453e-05 - mae: 0.0144 - val_loss: 0.0332 - val_mae: 0.1381\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 0.07477\n",
      "Epoch 197/1000\n",
      "48/48 - 1s - loss: 5.6950e-05 - mae: 0.0161 - val_loss: 0.0322 - val_mae: 0.1363\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 0.07477\n",
      "Epoch 198/1000\n",
      "48/48 - 1s - loss: 5.2757e-05 - mae: 0.0155 - val_loss: 0.0314 - val_mae: 0.1346\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 0.07477\n",
      "Epoch 199/1000\n",
      "48/48 - 1s - loss: 5.1174e-05 - mae: 0.0152 - val_loss: 0.0327 - val_mae: 0.1370\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 0.07477\n",
      "Epoch 200/1000\n",
      "48/48 - 1s - loss: 4.6489e-05 - mae: 0.0144 - val_loss: 0.0333 - val_mae: 0.1380\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 0.07477\n",
      "Epoch 201/1000\n",
      "48/48 - 1s - loss: 4.6812e-05 - mae: 0.0145 - val_loss: 0.0331 - val_mae: 0.1382\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 0.07477\n",
      "Epoch 202/1000\n",
      "48/48 - 1s - loss: 4.5234e-05 - mae: 0.0142 - val_loss: 0.0326 - val_mae: 0.1370\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 0.07477\n",
      "Epoch 203/1000\n",
      "48/48 - 1s - loss: 4.9290e-05 - mae: 0.0149 - val_loss: 0.0328 - val_mae: 0.1376\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 0.07477\n",
      "Epoch 204/1000\n",
      "48/48 - 1s - loss: 5.7855e-05 - mae: 0.0163 - val_loss: 0.0317 - val_mae: 0.1350\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 0.07477\n",
      "Epoch 205/1000\n",
      "48/48 - 1s - loss: 4.9057e-05 - mae: 0.0148 - val_loss: 0.0337 - val_mae: 0.1393\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 0.07477\n",
      "Epoch 206/1000\n",
      "48/48 - 1s - loss: 4.1744e-05 - mae: 0.0136 - val_loss: 0.0330 - val_mae: 0.1378\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 0.07477\n",
      "Epoch 207/1000\n",
      "48/48 - 1s - loss: 4.3604e-05 - mae: 0.0139 - val_loss: 0.0357 - val_mae: 0.1432\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 0.07477\n",
      "Epoch 208/1000\n",
      "48/48 - 1s - loss: 5.7605e-05 - mae: 0.0162 - val_loss: 0.0329 - val_mae: 0.1378\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 0.07477\n",
      "Epoch 209/1000\n",
      "48/48 - 1s - loss: 4.7790e-05 - mae: 0.0147 - val_loss: 0.0331 - val_mae: 0.1383\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 0.07477\n",
      "Epoch 210/1000\n",
      "48/48 - 1s - loss: 4.9649e-05 - mae: 0.0150 - val_loss: 0.0322 - val_mae: 0.1363\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 0.07477\n",
      "Epoch 211/1000\n",
      "48/48 - 1s - loss: 4.5433e-05 - mae: 0.0142 - val_loss: 0.0331 - val_mae: 0.1381\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 0.07477\n",
      "Epoch 212/1000\n",
      "48/48 - 1s - loss: 4.2910e-05 - mae: 0.0138 - val_loss: 0.0330 - val_mae: 0.1380\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 0.07477\n",
      "Epoch 213/1000\n",
      "48/48 - 1s - loss: 4.3227e-05 - mae: 0.0139 - val_loss: 0.0325 - val_mae: 0.1368\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 0.07477\n",
      "Epoch 214/1000\n",
      "48/48 - 1s - loss: 5.3003e-05 - mae: 0.0155 - val_loss: 0.0336 - val_mae: 0.1385\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 0.07477\n",
      "Epoch 215/1000\n",
      "48/48 - 1s - loss: 5.1645e-05 - mae: 0.0153 - val_loss: 0.0314 - val_mae: 0.1347\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 0.07477\n",
      "Epoch 216/1000\n",
      "48/48 - 1s - loss: 4.5640e-05 - mae: 0.0143 - val_loss: 0.0322 - val_mae: 0.1364\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 0.07477\n",
      "Epoch 217/1000\n",
      "48/48 - 1s - loss: 6.0052e-05 - mae: 0.0166 - val_loss: 0.0340 - val_mae: 0.1396\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 0.07477\n",
      "Epoch 218/1000\n",
      "48/48 - 1s - loss: 4.5782e-05 - mae: 0.0144 - val_loss: 0.0326 - val_mae: 0.1367\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 0.07477\n",
      "Epoch 00218: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd2f9ab8400>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "#               loss=custom_weight_loss,\n",
    "              loss='mse',\n",
    "              metrics=['mae']\n",
    "             )\n",
    "\n",
    "es = EarlyStopping(monitor='val_mae', mode='min', verbose=2, patience=PATIENCE)\n",
    "mc = ModelCheckpoint('../../saved_models/load_all_weighted_IPF_95.h5', \n",
    "                     monitor='val_mae', \n",
    "                     mode='min', \n",
    "                     verbose=2, \n",
    "                     save_best_only=True,\n",
    "                    )\n",
    "\n",
    "\n",
    "model.fit(train_X, train_y,\n",
    "          validation_data=(val_X, val_y),\n",
    "          epochs=EPOCHS,\n",
    "          batch_size=BATCH,\n",
    "          verbose=2,\n",
    "          shuffle=True,\n",
    "          callbacks=[es, mc],\n",
    "          sample_weight=sample_weights\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b313809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95854ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6ba252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae65733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
