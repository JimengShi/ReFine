{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4843e104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-10 20:40:16.840388: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from math import sqrt\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "from helper import series_to_supervised\n",
    "from model.mlp import mlp_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "460ee40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c74a745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5714025946899135\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(10)\n",
    "print(random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb30958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p (mbar)</th>\n",
       "      <th>T (degC)</th>\n",
       "      <th>Tpot (K)</th>\n",
       "      <th>Tdew (degC)</th>\n",
       "      <th>rh (%)</th>\n",
       "      <th>VPmax (mbar)</th>\n",
       "      <th>VPact (mbar)</th>\n",
       "      <th>VPdef (mbar)</th>\n",
       "      <th>sh (g/kg)</th>\n",
       "      <th>H2OC (mmol/mol)</th>\n",
       "      <th>rho (g/m**3)</th>\n",
       "      <th>wv (m/s)</th>\n",
       "      <th>max. wv (m/s)</th>\n",
       "      <th>wd (deg)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-01-01 00:00:00</th>\n",
       "      <td>996.528000</td>\n",
       "      <td>-8.304000</td>\n",
       "      <td>265.118000</td>\n",
       "      <td>-9.120000</td>\n",
       "      <td>93.780000</td>\n",
       "      <td>3.260000</td>\n",
       "      <td>3.058000</td>\n",
       "      <td>0.202000</td>\n",
       "      <td>1.910000</td>\n",
       "      <td>3.068000</td>\n",
       "      <td>1309.196000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>1.002000</td>\n",
       "      <td>174.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-01 01:00:00</th>\n",
       "      <td>996.525000</td>\n",
       "      <td>-8.065000</td>\n",
       "      <td>265.361667</td>\n",
       "      <td>-8.861667</td>\n",
       "      <td>93.933333</td>\n",
       "      <td>3.323333</td>\n",
       "      <td>3.121667</td>\n",
       "      <td>0.201667</td>\n",
       "      <td>1.951667</td>\n",
       "      <td>3.133333</td>\n",
       "      <td>1307.981667</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.711667</td>\n",
       "      <td>172.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-01 02:00:00</th>\n",
       "      <td>996.745000</td>\n",
       "      <td>-8.763333</td>\n",
       "      <td>264.645000</td>\n",
       "      <td>-9.610000</td>\n",
       "      <td>93.533333</td>\n",
       "      <td>3.145000</td>\n",
       "      <td>2.940000</td>\n",
       "      <td>0.201667</td>\n",
       "      <td>1.836667</td>\n",
       "      <td>2.950000</td>\n",
       "      <td>1311.816667</td>\n",
       "      <td>0.248333</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>196.816667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-01 03:00:00</th>\n",
       "      <td>996.986667</td>\n",
       "      <td>-8.896667</td>\n",
       "      <td>264.491667</td>\n",
       "      <td>-9.786667</td>\n",
       "      <td>93.200000</td>\n",
       "      <td>3.111667</td>\n",
       "      <td>2.898333</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>1.811667</td>\n",
       "      <td>2.906667</td>\n",
       "      <td>1312.813333</td>\n",
       "      <td>0.176667</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>157.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-01 04:00:00</th>\n",
       "      <td>997.158333</td>\n",
       "      <td>-9.348333</td>\n",
       "      <td>264.026667</td>\n",
       "      <td>-10.345000</td>\n",
       "      <td>92.383333</td>\n",
       "      <td>3.001667</td>\n",
       "      <td>2.775000</td>\n",
       "      <td>0.231667</td>\n",
       "      <td>1.733333</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>1315.355000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>150.093333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       p (mbar)  T (degC)    Tpot (K)  Tdew (degC)     rh (%)  \\\n",
       "Date Time                                                                       \n",
       "2009-01-01 00:00:00  996.528000 -8.304000  265.118000    -9.120000  93.780000   \n",
       "2009-01-01 01:00:00  996.525000 -8.065000  265.361667    -8.861667  93.933333   \n",
       "2009-01-01 02:00:00  996.745000 -8.763333  264.645000    -9.610000  93.533333   \n",
       "2009-01-01 03:00:00  996.986667 -8.896667  264.491667    -9.786667  93.200000   \n",
       "2009-01-01 04:00:00  997.158333 -9.348333  264.026667   -10.345000  92.383333   \n",
       "\n",
       "                     VPmax (mbar)  VPact (mbar)  VPdef (mbar)  sh (g/kg)  \\\n",
       "Date Time                                                                  \n",
       "2009-01-01 00:00:00      3.260000      3.058000      0.202000   1.910000   \n",
       "2009-01-01 01:00:00      3.323333      3.121667      0.201667   1.951667   \n",
       "2009-01-01 02:00:00      3.145000      2.940000      0.201667   1.836667   \n",
       "2009-01-01 03:00:00      3.111667      2.898333      0.210000   1.811667   \n",
       "2009-01-01 04:00:00      3.001667      2.775000      0.231667   1.733333   \n",
       "\n",
       "                     H2OC (mmol/mol)  rho (g/m**3)  wv (m/s)  max. wv (m/s)  \\\n",
       "Date Time                                                                     \n",
       "2009-01-01 00:00:00         3.068000   1309.196000  0.520000       1.002000   \n",
       "2009-01-01 01:00:00         3.133333   1307.981667  0.316667       0.711667   \n",
       "2009-01-01 02:00:00         2.950000   1311.816667  0.248333       0.606667   \n",
       "2009-01-01 03:00:00         2.906667   1312.813333  0.176667       0.606667   \n",
       "2009-01-01 04:00:00         2.780000   1315.355000  0.290000       0.670000   \n",
       "\n",
       "                       wd (deg)  \n",
       "Date Time                        \n",
       "2009-01-01 00:00:00  174.460000  \n",
       "2009-01-01 01:00:00  172.416667  \n",
       "2009-01-01 02:00:00  196.816667  \n",
       "2009-01-01 03:00:00  157.083333  \n",
       "2009-01-01 04:00:00  150.093333  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://keras.io/examples/timeseries/timeseries_weather_forecasting/#climate-data-timeseries\n",
    "data = pd.read_csv(\"../../data/jena_climate_2009_2016_hourly.csv\", index_col=0)\n",
    "data.fillna(0, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3855fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)', 'rh (%)',\n",
       "       'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)',\n",
       "       'H2OC (mmol/mol)', 'rho (g/m**3)', 'wv (m/s)', 'max. wv (m/s)',\n",
       "       'wd (deg)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2191f187",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33b340cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reframed.shape: (70046, 1176)\n"
     ]
    }
   ],
   "source": [
    "values = data.values\n",
    "\n",
    "# specify the number of lag hours\n",
    "n_hours = 24*3\n",
    "n_features = data.shape[-1]\n",
    "k = 12\n",
    "split1 = 0.7\n",
    "split2 = 0.85\n",
    "\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(values, n_hours, k)\n",
    "print(\"reframed.shape:\", reframed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e2084b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape (49032, 1008) (49032, 12) (10507, 1008) (10507, 12) (10507, 1008) (10507, 12)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "reframed_values = reframed.values\n",
    "n_train_hours = int(len(reframed_values)*split1)\n",
    "n_valid_hours = int(len(reframed_values)*split2)\n",
    "\n",
    "train = reframed_values[:n_train_hours, :]\n",
    "val = reframed_values[n_train_hours:n_valid_hours, :]\n",
    "test = reframed_values[n_valid_hours:, :]\n",
    "\n",
    "\n",
    "# split into input and outputs\n",
    "n_obs = n_hours * n_features\n",
    "feature_idx = 5\n",
    "train_X, train_y = train[:, :n_obs], train[:, [n_obs + feature_idx + n_features * i for i in range(k)]]\n",
    "val_X, val_y = val[:, :n_obs], val[:, [n_obs + feature_idx + n_features * i for i in range(k)]]\n",
    "test_X, test_y = test[:, :n_obs], test[:, [n_obs + feature_idx + n_features * i for i in range(k)]]\n",
    "\n",
    "\n",
    "print(\"train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape\", \n",
    "      train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1653a1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape (49032, 72, 14) (49032, 12) (10507, 72, 14) (10507, 12) (10507, 72, 14) (10507, 12)\n"
     ]
    }
   ],
   "source": [
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "train_y = scaler.fit_transform(train_y)\n",
    "\n",
    "val_X = scaler.fit_transform(val_X)\n",
    "val_y = scaler.fit_transform(val_y)\n",
    "\n",
    "test_X = scaler.fit_transform(test_X)\n",
    "test_y = scaler.fit_transform(test_y)\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
    "val_X = val_X.reshape((val_X.shape[0], n_hours, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
    "\n",
    "print(\"train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape\", \n",
    "      train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd896d5",
   "metadata": {},
   "source": [
    "### PM threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8885dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49032,)\n",
      "(10507,)\n",
      "(10507,)\n"
     ]
    }
   ],
   "source": [
    "train_X_pm = train_X[:, 0, feature_idx]\n",
    "print(train_X_pm.shape)\n",
    "\n",
    "val_X_pm = val_X[:, 0, feature_idx]\n",
    "print(val_X_pm.shape)\n",
    "\n",
    "test_X_pm = test_X[:, 0, feature_idx]\n",
    "print(test_X_pm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f154878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_nums = 20\n",
    "\n",
    "# Step 1: Calculate the histogram\n",
    "counts, bin_edges = np.histogram(train_X_pm, bins=bin_nums)\n",
    "\n",
    "# Step 2: Invert counts to assign lower weights to more frequent bins, avoid division by zero by adding a small number (epsilon)\n",
    "epsilon = 1e-8\n",
    "weights = np.sqrt(1.0 / (counts + epsilon))\n",
    "# weights = 1.0 / (counts + epsilon)\n",
    "\n",
    "# Step 3: Normalize the weights (optional)\n",
    "# weights /= np.max(weights)\n",
    "weights /= np.sum(weights) * len(counts)\n",
    "\n",
    "# Step 4: Assign weights to each sample based on the bin it falls into\n",
    "sample_weights = np.zeros(train_X_pm.shape[0])\n",
    "\n",
    "for i, value in enumerate(train_X_pm):\n",
    "    \n",
    "    # Find the index of the bin this sample falls into\n",
    "    bin_index = np.digitize(value, bin_edges) - 1\n",
    "    bin_index = min(bin_index, bin_nums - 1)\n",
    "    \n",
    "    # Assign the corresponding weight\n",
    "    sample_weights[i] = weights[bin_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba3913c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49032,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b70ef2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_weights /= np.sum(sample_weights)\n",
    "# sample_weights /= np.max(sample_weights)\n",
    "# sample_weights = (sample_weights - sample_weights.min()) / (sample_weights.max()-sample_weights.min()) + 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69798e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.5977e+04, 1.8750e+03, 4.5700e+02, 2.8000e+02, 1.6200e+02,\n",
       "        1.2200e+02, 0.0000e+00, 0.0000e+00, 5.7000e+01, 0.0000e+00,\n",
       "        4.0000e+01, 3.4000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 1.6000e+01, 0.0000e+00, 1.2000e+01]),\n",
       " array([0.00035665, 0.00082442, 0.0012922 , 0.00175998, 0.00222775,\n",
       "        0.00269553, 0.00316331, 0.00363109, 0.00409886, 0.00456664,\n",
       "        0.00503442, 0.00550219, 0.00596997, 0.00643775, 0.00690553,\n",
       "        0.0073733 , 0.00784108, 0.00830886, 0.00877663, 0.00924441,\n",
       "        0.00971219]),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sample_weights, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85808ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('sample_weights_pressure_IPF.npy', sample_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa5ce3",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5435f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== model parameters ======\n",
    "mlp_unit1 = 128\n",
    "mlp_unit2 = 128\n",
    "mlp_unit3 = 64\n",
    "mlp_unit4 = 64\n",
    "mlp_unit5 = 32\n",
    "mlp_unit6 = 32\n",
    "mlp_unit7 = 16\n",
    "mlp_unit8 = 16\n",
    "dropout = 0.0\n",
    "kernel_size = 2\n",
    "pool_size = 2\n",
    "learning_rate = 1e-4\n",
    "decay_steps = 10000\n",
    "decay_rate = 0.95\n",
    "PATIENCE = 100\n",
    "EPOCHS = 1000\n",
    "BATCH = 512\n",
    "opt_num = k\n",
    "input_shape = train_X.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0605f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlp_layer(input_shape=input_shape,\n",
    "                   mlp_unit1=mlp_unit1,\n",
    "                   mlp_unit2=mlp_unit2,\n",
    "                   mlp_unit3=mlp_unit3,\n",
    "                   mlp_unit4=mlp_unit4,\n",
    "                   mlp_unit5=mlp_unit5,\n",
    "                   mlp_unit6=mlp_unit6,\n",
    "                   mlp_unit7=mlp_unit7,\n",
    "                   mlp_unit8=mlp_unit8,\n",
    "                   dropout=dropout,\n",
    "                   masked_value=-1,\n",
    "                   opt_num=opt_num\n",
    "                  )\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43622a95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "96/96 - 3s - loss: 9.3015e-06 - mae: 0.0903 - val_loss: 0.0205 - val_mae: 0.1040\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 0.10397, saving model to ../../saved_models/pressure_all_weighted_IPF_95.h5\n",
      "Epoch 2/1000\n",
      "96/96 - 1s - loss: 3.7518e-06 - mae: 0.0571 - val_loss: 0.0089 - val_mae: 0.0611\n",
      "\n",
      "Epoch 00002: val_mae improved from 0.10397 to 0.06111, saving model to ../../saved_models/pressure_all_weighted_IPF_95.h5\n",
      "Epoch 3/1000\n",
      "96/96 - 1s - loss: 2.7536e-06 - mae: 0.0490 - val_loss: 0.0086 - val_mae: 0.0603\n",
      "\n",
      "Epoch 00003: val_mae improved from 0.06111 to 0.06032, saving model to ../../saved_models/pressure_all_weighted_IPF_95.h5\n",
      "Epoch 4/1000\n",
      "96/96 - 1s - loss: 2.5058e-06 - mae: 0.0466 - val_loss: 0.0082 - val_mae: 0.0600\n",
      "\n",
      "Epoch 00004: val_mae improved from 0.06032 to 0.05999, saving model to ../../saved_models/pressure_all_weighted_IPF_95.h5\n",
      "Epoch 5/1000\n",
      "96/96 - 1s - loss: 2.3553e-06 - mae: 0.0450 - val_loss: 0.0070 - val_mae: 0.0540\n",
      "\n",
      "Epoch 00005: val_mae improved from 0.05999 to 0.05398, saving model to ../../saved_models/pressure_all_weighted_IPF_95.h5\n",
      "Epoch 6/1000\n",
      "96/96 - 1s - loss: 2.2276e-06 - mae: 0.0437 - val_loss: 0.0069 - val_mae: 0.0540\n",
      "\n",
      "Epoch 00006: val_mae did not improve from 0.05398\n",
      "Epoch 7/1000\n",
      "96/96 - 1s - loss: 2.1066e-06 - mae: 0.0424 - val_loss: 0.0060 - val_mae: 0.0497\n",
      "\n",
      "Epoch 00007: val_mae improved from 0.05398 to 0.04967, saving model to ../../saved_models/pressure_all_weighted_IPF_95.h5\n",
      "Epoch 8/1000\n",
      "96/96 - 1s - loss: 2.0039e-06 - mae: 0.0413 - val_loss: 0.0055 - val_mae: 0.0481\n",
      "\n",
      "Epoch 00008: val_mae improved from 0.04967 to 0.04806, saving model to ../../saved_models/pressure_all_weighted_IPF_95.h5\n",
      "Epoch 9/1000\n",
      "96/96 - 1s - loss: 1.9164e-06 - mae: 0.0403 - val_loss: 0.0053 - val_mae: 0.0467\n",
      "\n",
      "Epoch 00009: val_mae improved from 0.04806 to 0.04669, saving model to ../../saved_models/pressure_all_weighted_IPF_95.h5\n",
      "Epoch 10/1000\n",
      "96/96 - 1s - loss: 1.8787e-06 - mae: 0.0400 - val_loss: 0.0056 - val_mae: 0.0481\n",
      "\n",
      "Epoch 00010: val_mae did not improve from 0.04669\n",
      "Epoch 11/1000\n",
      "96/96 - 1s - loss: 1.8192e-06 - mae: 0.0393 - val_loss: 0.0059 - val_mae: 0.0495\n",
      "\n",
      "Epoch 00011: val_mae did not improve from 0.04669\n",
      "Epoch 12/1000\n",
      "96/96 - 1s - loss: 1.7512e-06 - mae: 0.0385 - val_loss: 0.0056 - val_mae: 0.0476\n",
      "\n",
      "Epoch 00012: val_mae did not improve from 0.04669\n",
      "Epoch 13/1000\n",
      "96/96 - 1s - loss: 1.7447e-06 - mae: 0.0385 - val_loss: 0.0065 - val_mae: 0.0521\n",
      "\n",
      "Epoch 00013: val_mae did not improve from 0.04669\n",
      "Epoch 14/1000\n",
      "96/96 - 1s - loss: 1.6919e-06 - mae: 0.0378 - val_loss: 0.0060 - val_mae: 0.0497\n",
      "\n",
      "Epoch 00014: val_mae did not improve from 0.04669\n",
      "Epoch 15/1000\n",
      "96/96 - 1s - loss: 1.6666e-06 - mae: 0.0375 - val_loss: 0.0054 - val_mae: 0.0467\n",
      "\n",
      "Epoch 00015: val_mae did not improve from 0.04669\n",
      "Epoch 16/1000\n",
      "96/96 - 1s - loss: 1.6608e-06 - mae: 0.0375 - val_loss: 0.0053 - val_mae: 0.0463\n",
      "\n",
      "Epoch 00016: val_mae improved from 0.04669 to 0.04632, saving model to ../../saved_models/pressure_all_weighted_IPF_95.h5\n",
      "Epoch 17/1000\n",
      "96/96 - 1s - loss: 1.6200e-06 - mae: 0.0370 - val_loss: 0.0055 - val_mae: 0.0468\n",
      "\n",
      "Epoch 00017: val_mae did not improve from 0.04632\n",
      "Epoch 18/1000\n",
      "96/96 - 1s - loss: 1.6370e-06 - mae: 0.0372 - val_loss: 0.0058 - val_mae: 0.0488\n",
      "\n",
      "Epoch 00018: val_mae did not improve from 0.04632\n",
      "Epoch 19/1000\n",
      "96/96 - 1s - loss: 1.5925e-06 - mae: 0.0366 - val_loss: 0.0058 - val_mae: 0.0475\n",
      "\n",
      "Epoch 00019: val_mae did not improve from 0.04632\n",
      "Epoch 20/1000\n",
      "96/96 - 1s - loss: 1.5994e-06 - mae: 0.0367 - val_loss: 0.0055 - val_mae: 0.0459\n",
      "\n",
      "Epoch 00020: val_mae improved from 0.04632 to 0.04594, saving model to ../../saved_models/pressure_all_weighted_IPF_95.h5\n",
      "Epoch 21/1000\n",
      "96/96 - 1s - loss: 1.5720e-06 - mae: 0.0363 - val_loss: 0.0051 - val_mae: 0.0449\n",
      "\n",
      "Epoch 00021: val_mae improved from 0.04594 to 0.04491, saving model to ../../saved_models/pressure_all_weighted_IPF_95.h5\n",
      "Epoch 22/1000\n",
      "96/96 - 1s - loss: 1.5734e-06 - mae: 0.0364 - val_loss: 0.0051 - val_mae: 0.0450\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 0.04491\n",
      "Epoch 23/1000\n",
      "96/96 - 1s - loss: 1.5800e-06 - mae: 0.0365 - val_loss: 0.0049 - val_mae: 0.0432\n",
      "\n",
      "Epoch 00023: val_mae improved from 0.04491 to 0.04317, saving model to ../../saved_models/pressure_all_weighted_IPF_95.h5\n",
      "Epoch 24/1000\n",
      "96/96 - 1s - loss: 1.5640e-06 - mae: 0.0362 - val_loss: 0.0049 - val_mae: 0.0456\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 0.04317\n",
      "Epoch 25/1000\n",
      "96/96 - 1s - loss: 1.5777e-06 - mae: 0.0365 - val_loss: 0.0060 - val_mae: 0.0502\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 0.04317\n",
      "Epoch 26/1000\n",
      "96/96 - 1s - loss: 1.5479e-06 - mae: 0.0361 - val_loss: 0.0052 - val_mae: 0.0455\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 0.04317\n",
      "Epoch 27/1000\n",
      "96/96 - 1s - loss: 1.5436e-06 - mae: 0.0360 - val_loss: 0.0053 - val_mae: 0.0451\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 0.04317\n",
      "Epoch 28/1000\n",
      "96/96 - 1s - loss: 1.5394e-06 - mae: 0.0360 - val_loss: 0.0050 - val_mae: 0.0444\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 0.04317\n",
      "Epoch 29/1000\n",
      "96/96 - 1s - loss: 1.5356e-06 - mae: 0.0359 - val_loss: 0.0059 - val_mae: 0.0489\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 0.04317\n",
      "Epoch 30/1000\n",
      "96/96 - 1s - loss: 1.5207e-06 - mae: 0.0358 - val_loss: 0.0052 - val_mae: 0.0457\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 0.04317\n",
      "Epoch 31/1000\n",
      "96/96 - 1s - loss: 1.5263e-06 - mae: 0.0358 - val_loss: 0.0053 - val_mae: 0.0463\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 0.04317\n",
      "Epoch 32/1000\n",
      "96/96 - 1s - loss: 1.5078e-06 - mae: 0.0355 - val_loss: 0.0049 - val_mae: 0.0439\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 0.04317\n",
      "Epoch 33/1000\n",
      "96/96 - 1s - loss: 1.5155e-06 - mae: 0.0357 - val_loss: 0.0053 - val_mae: 0.0463\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 0.04317\n",
      "Epoch 34/1000\n",
      "96/96 - 1s - loss: 1.4936e-06 - mae: 0.0354 - val_loss: 0.0049 - val_mae: 0.0433\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 0.04317\n",
      "Epoch 35/1000\n",
      "96/96 - 1s - loss: 1.5146e-06 - mae: 0.0357 - val_loss: 0.0052 - val_mae: 0.0453\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 0.04317\n",
      "Epoch 36/1000\n",
      "96/96 - 1s - loss: 1.4760e-06 - mae: 0.0352 - val_loss: 0.0050 - val_mae: 0.0443\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 0.04317\n",
      "Epoch 37/1000\n",
      "96/96 - 1s - loss: 1.4947e-06 - mae: 0.0355 - val_loss: 0.0049 - val_mae: 0.0440\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 0.04317\n",
      "Epoch 38/1000\n",
      "96/96 - 1s - loss: 1.4674e-06 - mae: 0.0351 - val_loss: 0.0058 - val_mae: 0.0501\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 0.04317\n",
      "Epoch 39/1000\n",
      "96/96 - 1s - loss: 1.4702e-06 - mae: 0.0351 - val_loss: 0.0052 - val_mae: 0.0465\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 0.04317\n",
      "Epoch 40/1000\n",
      "96/96 - 1s - loss: 1.4610e-06 - mae: 0.0350 - val_loss: 0.0067 - val_mae: 0.0564\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 0.04317\n",
      "Epoch 41/1000\n",
      "96/96 - 1s - loss: 1.4684e-06 - mae: 0.0352 - val_loss: 0.0052 - val_mae: 0.0461\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 0.04317\n",
      "Epoch 42/1000\n",
      "96/96 - 1s - loss: 1.4490e-06 - mae: 0.0350 - val_loss: 0.0052 - val_mae: 0.0466\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 0.04317\n",
      "Epoch 43/1000\n",
      "96/96 - 1s - loss: 1.4435e-06 - mae: 0.0348 - val_loss: 0.0055 - val_mae: 0.0492\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 0.04317\n",
      "Epoch 44/1000\n",
      "96/96 - 1s - loss: 1.4487e-06 - mae: 0.0350 - val_loss: 0.0057 - val_mae: 0.0494\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 0.04317\n",
      "Epoch 45/1000\n",
      "96/96 - 1s - loss: 1.4618e-06 - mae: 0.0352 - val_loss: 0.0065 - val_mae: 0.0543\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 0.04317\n",
      "Epoch 46/1000\n",
      "96/96 - 1s - loss: 1.4338e-06 - mae: 0.0348 - val_loss: 0.0052 - val_mae: 0.0457\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 0.04317\n",
      "Epoch 47/1000\n",
      "96/96 - 1s - loss: 1.4459e-06 - mae: 0.0351 - val_loss: 0.0065 - val_mae: 0.0540\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 0.04317\n",
      "Epoch 48/1000\n",
      "96/96 - 1s - loss: 1.4304e-06 - mae: 0.0349 - val_loss: 0.0061 - val_mae: 0.0528\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 0.04317\n",
      "Epoch 49/1000\n",
      "96/96 - 1s - loss: 1.4342e-06 - mae: 0.0351 - val_loss: 0.0057 - val_mae: 0.0498\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 0.04317\n",
      "Epoch 50/1000\n",
      "96/96 - 1s - loss: 1.4269e-06 - mae: 0.0348 - val_loss: 0.0058 - val_mae: 0.0503\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 0.04317\n",
      "Epoch 51/1000\n",
      "96/96 - 1s - loss: 1.4395e-06 - mae: 0.0350 - val_loss: 0.0052 - val_mae: 0.0469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00051: val_mae did not improve from 0.04317\n",
      "Epoch 52/1000\n",
      "96/96 - 1s - loss: 1.4300e-06 - mae: 0.0350 - val_loss: 0.0062 - val_mae: 0.0541\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 0.04317\n",
      "Epoch 53/1000\n",
      "96/96 - 1s - loss: 1.4024e-06 - mae: 0.0345 - val_loss: 0.0061 - val_mae: 0.0523\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 0.04317\n",
      "Epoch 54/1000\n",
      "96/96 - 1s - loss: 1.4266e-06 - mae: 0.0349 - val_loss: 0.0054 - val_mae: 0.0479\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 0.04317\n",
      "Epoch 55/1000\n",
      "96/96 - 1s - loss: 1.4078e-06 - mae: 0.0346 - val_loss: 0.0054 - val_mae: 0.0468\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 0.04317\n",
      "Epoch 56/1000\n",
      "96/96 - 1s - loss: 1.4081e-06 - mae: 0.0347 - val_loss: 0.0060 - val_mae: 0.0517\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 0.04317\n",
      "Epoch 57/1000\n",
      "96/96 - 1s - loss: 1.3913e-06 - mae: 0.0345 - val_loss: 0.0065 - val_mae: 0.0540\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 0.04317\n",
      "Epoch 58/1000\n",
      "96/96 - 1s - loss: 1.4014e-06 - mae: 0.0346 - val_loss: 0.0060 - val_mae: 0.0520\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 0.04317\n",
      "Epoch 59/1000\n",
      "96/96 - 1s - loss: 1.3823e-06 - mae: 0.0343 - val_loss: 0.0063 - val_mae: 0.0529\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 0.04317\n",
      "Epoch 60/1000\n",
      "96/96 - 1s - loss: 1.3882e-06 - mae: 0.0344 - val_loss: 0.0065 - val_mae: 0.0558\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 0.04317\n",
      "Epoch 61/1000\n",
      "96/96 - 1s - loss: 1.4040e-06 - mae: 0.0348 - val_loss: 0.0062 - val_mae: 0.0533\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 0.04317\n",
      "Epoch 62/1000\n",
      "96/96 - 1s - loss: 1.4113e-06 - mae: 0.0349 - val_loss: 0.0068 - val_mae: 0.0564\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 0.04317\n",
      "Epoch 63/1000\n",
      "96/96 - 1s - loss: 1.3837e-06 - mae: 0.0345 - val_loss: 0.0059 - val_mae: 0.0494\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 0.04317\n",
      "Epoch 64/1000\n",
      "96/96 - 1s - loss: 1.3692e-06 - mae: 0.0342 - val_loss: 0.0056 - val_mae: 0.0495\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 0.04317\n",
      "Epoch 65/1000\n",
      "96/96 - 1s - loss: 1.4225e-06 - mae: 0.0351 - val_loss: 0.0058 - val_mae: 0.0495\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 0.04317\n",
      "Epoch 66/1000\n",
      "96/96 - 1s - loss: 1.3798e-06 - mae: 0.0344 - val_loss: 0.0067 - val_mae: 0.0567\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 0.04317\n",
      "Epoch 67/1000\n",
      "96/96 - 1s - loss: 1.3677e-06 - mae: 0.0342 - val_loss: 0.0056 - val_mae: 0.0493\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 0.04317\n",
      "Epoch 68/1000\n",
      "96/96 - 1s - loss: 1.3850e-06 - mae: 0.0344 - val_loss: 0.0062 - val_mae: 0.0520\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 0.04317\n",
      "Epoch 69/1000\n",
      "96/96 - 1s - loss: 1.3536e-06 - mae: 0.0340 - val_loss: 0.0062 - val_mae: 0.0519\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 0.04317\n",
      "Epoch 70/1000\n",
      "96/96 - 1s - loss: 1.3712e-06 - mae: 0.0343 - val_loss: 0.0081 - val_mae: 0.0655\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 0.04317\n",
      "Epoch 71/1000\n",
      "96/96 - 1s - loss: 1.3776e-06 - mae: 0.0343 - val_loss: 0.0056 - val_mae: 0.0484\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 0.04317\n",
      "Epoch 72/1000\n",
      "96/96 - 1s - loss: 1.3654e-06 - mae: 0.0343 - val_loss: 0.0060 - val_mae: 0.0515\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 0.04317\n",
      "Epoch 73/1000\n",
      "96/96 - 1s - loss: 1.3625e-06 - mae: 0.0342 - val_loss: 0.0062 - val_mae: 0.0533\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 0.04317\n",
      "Epoch 74/1000\n",
      "96/96 - 1s - loss: 1.3643e-06 - mae: 0.0343 - val_loss: 0.0056 - val_mae: 0.0488\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 0.04317\n",
      "Epoch 75/1000\n",
      "96/96 - 1s - loss: 1.3601e-06 - mae: 0.0343 - val_loss: 0.0061 - val_mae: 0.0519\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 0.04317\n",
      "Epoch 76/1000\n",
      "96/96 - 1s - loss: 1.3546e-06 - mae: 0.0342 - val_loss: 0.0064 - val_mae: 0.0543\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 0.04317\n",
      "Epoch 77/1000\n",
      "96/96 - 1s - loss: 1.3504e-06 - mae: 0.0341 - val_loss: 0.0064 - val_mae: 0.0532\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 0.04317\n",
      "Epoch 78/1000\n",
      "96/96 - 1s - loss: 1.3359e-06 - mae: 0.0339 - val_loss: 0.0064 - val_mae: 0.0531\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 0.04317\n",
      "Epoch 79/1000\n",
      "96/96 - 1s - loss: 1.3383e-06 - mae: 0.0340 - val_loss: 0.0064 - val_mae: 0.0532\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 0.04317\n",
      "Epoch 80/1000\n",
      "96/96 - 1s - loss: 1.3514e-06 - mae: 0.0341 - val_loss: 0.0072 - val_mae: 0.0583\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 0.04317\n",
      "Epoch 81/1000\n",
      "96/96 - 1s - loss: 1.3440e-06 - mae: 0.0341 - val_loss: 0.0071 - val_mae: 0.0578\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 0.04317\n",
      "Epoch 82/1000\n",
      "96/96 - 1s - loss: 1.3306e-06 - mae: 0.0339 - val_loss: 0.0065 - val_mae: 0.0545\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 0.04317\n",
      "Epoch 83/1000\n",
      "96/96 - 1s - loss: 1.3312e-06 - mae: 0.0339 - val_loss: 0.0061 - val_mae: 0.0511\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 0.04317\n",
      "Epoch 84/1000\n",
      "96/96 - 1s - loss: 1.3277e-06 - mae: 0.0338 - val_loss: 0.0063 - val_mae: 0.0522\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 0.04317\n",
      "Epoch 85/1000\n",
      "96/96 - 1s - loss: 1.3260e-06 - mae: 0.0339 - val_loss: 0.0063 - val_mae: 0.0525\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 0.04317\n",
      "Epoch 86/1000\n",
      "96/96 - 1s - loss: 1.3441e-06 - mae: 0.0341 - val_loss: 0.0076 - val_mae: 0.0594\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 0.04317\n",
      "Epoch 87/1000\n",
      "96/96 - 1s - loss: 1.3380e-06 - mae: 0.0341 - val_loss: 0.0071 - val_mae: 0.0574\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 0.04317\n",
      "Epoch 88/1000\n",
      "96/96 - 1s - loss: 1.3193e-06 - mae: 0.0338 - val_loss: 0.0067 - val_mae: 0.0550\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 0.04317\n",
      "Epoch 89/1000\n",
      "96/96 - 1s - loss: 1.3238e-06 - mae: 0.0339 - val_loss: 0.0069 - val_mae: 0.0543\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 0.04317\n",
      "Epoch 90/1000\n",
      "96/96 - 1s - loss: 1.3223e-06 - mae: 0.0338 - val_loss: 0.0061 - val_mae: 0.0507\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 0.04317\n",
      "Epoch 91/1000\n",
      "96/96 - 1s - loss: 1.3280e-06 - mae: 0.0339 - val_loss: 0.0064 - val_mae: 0.0516\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 0.04317\n",
      "Epoch 92/1000\n",
      "96/96 - 1s - loss: 1.3327e-06 - mae: 0.0341 - val_loss: 0.0070 - val_mae: 0.0556\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 0.04317\n",
      "Epoch 93/1000\n",
      "96/96 - 1s - loss: 1.3385e-06 - mae: 0.0340 - val_loss: 0.0070 - val_mae: 0.0565\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 0.04317\n",
      "Epoch 94/1000\n",
      "96/96 - 1s - loss: 1.3414e-06 - mae: 0.0342 - val_loss: 0.0088 - val_mae: 0.0664\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 0.04317\n",
      "Epoch 95/1000\n",
      "96/96 - 1s - loss: 1.3060e-06 - mae: 0.0336 - val_loss: 0.0078 - val_mae: 0.0604\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 0.04317\n",
      "Epoch 96/1000\n",
      "96/96 - 1s - loss: 1.3185e-06 - mae: 0.0338 - val_loss: 0.0067 - val_mae: 0.0530\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 0.04317\n",
      "Epoch 97/1000\n",
      "96/96 - 1s - loss: 1.3282e-06 - mae: 0.0341 - val_loss: 0.0087 - val_mae: 0.0667\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 0.04317\n",
      "Epoch 98/1000\n",
      "96/96 - 1s - loss: 1.3134e-06 - mae: 0.0338 - val_loss: 0.0074 - val_mae: 0.0569\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 0.04317\n",
      "Epoch 99/1000\n",
      "96/96 - 1s - loss: 1.3221e-06 - mae: 0.0340 - val_loss: 0.0080 - val_mae: 0.0602\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 0.04317\n",
      "Epoch 100/1000\n",
      "96/96 - 1s - loss: 1.3107e-06 - mae: 0.0338 - val_loss: 0.0080 - val_mae: 0.0613\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 0.04317\n",
      "Epoch 101/1000\n",
      "96/96 - 1s - loss: 1.2929e-06 - mae: 0.0335 - val_loss: 0.0074 - val_mae: 0.0579\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 0.04317\n",
      "Epoch 102/1000\n",
      "96/96 - 1s - loss: 1.3116e-06 - mae: 0.0338 - val_loss: 0.0076 - val_mae: 0.0587\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 0.04317\n",
      "Epoch 103/1000\n",
      "96/96 - 1s - loss: 1.2936e-06 - mae: 0.0335 - val_loss: 0.0068 - val_mae: 0.0548\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 0.04317\n",
      "Epoch 104/1000\n",
      "96/96 - 1s - loss: 1.3173e-06 - mae: 0.0339 - val_loss: 0.0067 - val_mae: 0.0536\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 0.04317\n",
      "Epoch 105/1000\n",
      "96/96 - 1s - loss: 1.3585e-06 - mae: 0.0345 - val_loss: 0.0072 - val_mae: 0.0563\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 0.04317\n",
      "Epoch 106/1000\n",
      "96/96 - 1s - loss: 1.2838e-06 - mae: 0.0334 - val_loss: 0.0078 - val_mae: 0.0604\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 0.04317\n",
      "Epoch 107/1000\n",
      "96/96 - 1s - loss: 1.3097e-06 - mae: 0.0338 - val_loss: 0.0082 - val_mae: 0.0630\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 0.04317\n",
      "Epoch 108/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 - 1s - loss: 1.2881e-06 - mae: 0.0334 - val_loss: 0.0073 - val_mae: 0.0570\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 0.04317\n",
      "Epoch 109/1000\n",
      "96/96 - 1s - loss: 1.2753e-06 - mae: 0.0333 - val_loss: 0.0074 - val_mae: 0.0578\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 0.04317\n",
      "Epoch 110/1000\n",
      "96/96 - 1s - loss: 1.2819e-06 - mae: 0.0334 - val_loss: 0.0072 - val_mae: 0.0562\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 0.04317\n",
      "Epoch 111/1000\n",
      "96/96 - 1s - loss: 1.2994e-06 - mae: 0.0337 - val_loss: 0.0077 - val_mae: 0.0587\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 0.04317\n",
      "Epoch 112/1000\n",
      "96/96 - 1s - loss: 1.3010e-06 - mae: 0.0338 - val_loss: 0.0074 - val_mae: 0.0576\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 0.04317\n",
      "Epoch 113/1000\n",
      "96/96 - 1s - loss: 1.2963e-06 - mae: 0.0337 - val_loss: 0.0079 - val_mae: 0.0598\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 0.04317\n",
      "Epoch 114/1000\n",
      "96/96 - 1s - loss: 1.2961e-06 - mae: 0.0336 - val_loss: 0.0076 - val_mae: 0.0584\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 0.04317\n",
      "Epoch 115/1000\n",
      "96/96 - 1s - loss: 1.2835e-06 - mae: 0.0334 - val_loss: 0.0082 - val_mae: 0.0617\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 0.04317\n",
      "Epoch 116/1000\n",
      "96/96 - 1s - loss: 1.2821e-06 - mae: 0.0334 - val_loss: 0.0090 - val_mae: 0.0667\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 0.04317\n",
      "Epoch 117/1000\n",
      "96/96 - 1s - loss: 1.2716e-06 - mae: 0.0333 - val_loss: 0.0070 - val_mae: 0.0549\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 0.04317\n",
      "Epoch 118/1000\n",
      "96/96 - 1s - loss: 1.2941e-06 - mae: 0.0336 - val_loss: 0.0071 - val_mae: 0.0546\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 0.04317\n",
      "Epoch 119/1000\n",
      "96/96 - 1s - loss: 1.3215e-06 - mae: 0.0342 - val_loss: 0.0068 - val_mae: 0.0539\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 0.04317\n",
      "Epoch 120/1000\n",
      "96/96 - 1s - loss: 1.2892e-06 - mae: 0.0337 - val_loss: 0.0077 - val_mae: 0.0586\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 0.04317\n",
      "Epoch 121/1000\n",
      "96/96 - 1s - loss: 1.2842e-06 - mae: 0.0335 - val_loss: 0.0081 - val_mae: 0.0605\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 0.04317\n",
      "Epoch 122/1000\n",
      "96/96 - 1s - loss: 1.2857e-06 - mae: 0.0337 - val_loss: 0.0074 - val_mae: 0.0568\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 0.04317\n",
      "Epoch 123/1000\n",
      "96/96 - 1s - loss: 1.2645e-06 - mae: 0.0333 - val_loss: 0.0075 - val_mae: 0.0579\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 0.04317\n",
      "Epoch 124/1000\n",
      "96/96 - 1s - loss: 1.2570e-06 - mae: 0.0332 - val_loss: 0.0076 - val_mae: 0.0581\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 0.04317\n",
      "Epoch 125/1000\n",
      "96/96 - 1s - loss: 1.2912e-06 - mae: 0.0338 - val_loss: 0.0078 - val_mae: 0.0593\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 0.04317\n",
      "Epoch 126/1000\n",
      "96/96 - 1s - loss: 1.2605e-06 - mae: 0.0332 - val_loss: 0.0076 - val_mae: 0.0580\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 0.04317\n",
      "Epoch 127/1000\n",
      "96/96 - 1s - loss: 1.2662e-06 - mae: 0.0333 - val_loss: 0.0077 - val_mae: 0.0584\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 0.04317\n",
      "Epoch 128/1000\n",
      "96/96 - 1s - loss: 1.2546e-06 - mae: 0.0332 - val_loss: 0.0082 - val_mae: 0.0614\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 0.04317\n",
      "Epoch 129/1000\n",
      "96/96 - 1s - loss: 1.2604e-06 - mae: 0.0333 - val_loss: 0.0078 - val_mae: 0.0593\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 0.04317\n",
      "Epoch 130/1000\n",
      "96/96 - 1s - loss: 1.2544e-06 - mae: 0.0332 - val_loss: 0.0073 - val_mae: 0.0568\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 0.04317\n",
      "Epoch 131/1000\n",
      "96/96 - 1s - loss: 1.2506e-06 - mae: 0.0331 - val_loss: 0.0078 - val_mae: 0.0585\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 0.04317\n",
      "Epoch 132/1000\n",
      "96/96 - 1s - loss: 1.2845e-06 - mae: 0.0336 - val_loss: 0.0086 - val_mae: 0.0637\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 0.04317\n",
      "Epoch 133/1000\n",
      "96/96 - 1s - loss: 1.2639e-06 - mae: 0.0334 - val_loss: 0.0079 - val_mae: 0.0588\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 0.04317\n",
      "Epoch 134/1000\n",
      "96/96 - 1s - loss: 1.2507e-06 - mae: 0.0331 - val_loss: 0.0091 - val_mae: 0.0663\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 0.04317\n",
      "Epoch 135/1000\n",
      "96/96 - 1s - loss: 1.2486e-06 - mae: 0.0332 - val_loss: 0.0078 - val_mae: 0.0588\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 0.04317\n",
      "Epoch 136/1000\n",
      "96/96 - 1s - loss: 1.2774e-06 - mae: 0.0336 - val_loss: 0.0084 - val_mae: 0.0616\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 0.04317\n",
      "Epoch 137/1000\n",
      "96/96 - 1s - loss: 1.2484e-06 - mae: 0.0331 - val_loss: 0.0084 - val_mae: 0.0623\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 0.04317\n",
      "Epoch 138/1000\n",
      "96/96 - 1s - loss: 1.2583e-06 - mae: 0.0333 - val_loss: 0.0076 - val_mae: 0.0569\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 0.04317\n",
      "Epoch 139/1000\n",
      "96/96 - 1s - loss: 1.2435e-06 - mae: 0.0330 - val_loss: 0.0086 - val_mae: 0.0633\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 0.04317\n",
      "Epoch 140/1000\n",
      "96/96 - 1s - loss: 1.2392e-06 - mae: 0.0331 - val_loss: 0.0079 - val_mae: 0.0589\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 0.04317\n",
      "Epoch 141/1000\n",
      "96/96 - 1s - loss: 1.2381e-06 - mae: 0.0330 - val_loss: 0.0079 - val_mae: 0.0591\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 0.04317\n",
      "Epoch 142/1000\n",
      "96/96 - 1s - loss: 1.2424e-06 - mae: 0.0331 - val_loss: 0.0081 - val_mae: 0.0599\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 0.04317\n",
      "Epoch 143/1000\n",
      "96/96 - 1s - loss: 1.2487e-06 - mae: 0.0331 - val_loss: 0.0073 - val_mae: 0.0565\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 0.04317\n",
      "Epoch 144/1000\n",
      "96/96 - 1s - loss: 1.2778e-06 - mae: 0.0338 - val_loss: 0.0082 - val_mae: 0.0602\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 0.04317\n",
      "Epoch 145/1000\n",
      "96/96 - 1s - loss: 1.2403e-06 - mae: 0.0331 - val_loss: 0.0084 - val_mae: 0.0608\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 0.04317\n",
      "Epoch 146/1000\n",
      "96/96 - 1s - loss: 1.2794e-06 - mae: 0.0338 - val_loss: 0.0084 - val_mae: 0.0618\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 0.04317\n",
      "Epoch 147/1000\n",
      "96/96 - 1s - loss: 1.2364e-06 - mae: 0.0330 - val_loss: 0.0085 - val_mae: 0.0613\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 0.04317\n",
      "Epoch 148/1000\n",
      "96/96 - 1s - loss: 1.2383e-06 - mae: 0.0330 - val_loss: 0.0082 - val_mae: 0.0604\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 0.04317\n",
      "Epoch 149/1000\n",
      "96/96 - 1s - loss: 1.2183e-06 - mae: 0.0328 - val_loss: 0.0086 - val_mae: 0.0627\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 0.04317\n",
      "Epoch 150/1000\n",
      "96/96 - 1s - loss: 1.2220e-06 - mae: 0.0328 - val_loss: 0.0084 - val_mae: 0.0603\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 0.04317\n",
      "Epoch 151/1000\n",
      "96/96 - 1s - loss: 1.2357e-06 - mae: 0.0330 - val_loss: 0.0078 - val_mae: 0.0586\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 0.04317\n",
      "Epoch 152/1000\n",
      "96/96 - 1s - loss: 1.2400e-06 - mae: 0.0332 - val_loss: 0.0082 - val_mae: 0.0604\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 0.04317\n",
      "Epoch 153/1000\n",
      "96/96 - 1s - loss: 1.2340e-06 - mae: 0.0331 - val_loss: 0.0097 - val_mae: 0.0673\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 0.04317\n",
      "Epoch 154/1000\n",
      "96/96 - 1s - loss: 1.2437e-06 - mae: 0.0332 - val_loss: 0.0083 - val_mae: 0.0596\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 0.04317\n",
      "Epoch 155/1000\n",
      "96/96 - 1s - loss: 1.2245e-06 - mae: 0.0330 - val_loss: 0.0080 - val_mae: 0.0598\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 0.04317\n",
      "Epoch 156/1000\n",
      "96/96 - 1s - loss: 1.2168e-06 - mae: 0.0328 - val_loss: 0.0083 - val_mae: 0.0610\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 0.04317\n",
      "Epoch 157/1000\n",
      "96/96 - 1s - loss: 1.2110e-06 - mae: 0.0327 - val_loss: 0.0086 - val_mae: 0.0623\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 0.04317\n",
      "Epoch 158/1000\n",
      "96/96 - 1s - loss: 1.2098e-06 - mae: 0.0328 - val_loss: 0.0088 - val_mae: 0.0639\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 0.04317\n",
      "Epoch 159/1000\n",
      "96/96 - 1s - loss: 1.2081e-06 - mae: 0.0327 - val_loss: 0.0087 - val_mae: 0.0630\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 0.04317\n",
      "Epoch 160/1000\n",
      "96/96 - 1s - loss: 1.2305e-06 - mae: 0.0332 - val_loss: 0.0083 - val_mae: 0.0605\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 0.04317\n",
      "Epoch 161/1000\n",
      "96/96 - 1s - loss: 1.2045e-06 - mae: 0.0327 - val_loss: 0.0092 - val_mae: 0.0658\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 0.04317\n",
      "Epoch 162/1000\n",
      "96/96 - 1s - loss: 1.2295e-06 - mae: 0.0330 - val_loss: 0.0084 - val_mae: 0.0628\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 0.04317\n",
      "Epoch 163/1000\n",
      "96/96 - 1s - loss: 1.2000e-06 - mae: 0.0326 - val_loss: 0.0087 - val_mae: 0.0633\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 0.04317\n",
      "Epoch 164/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 - 1s - loss: 1.2034e-06 - mae: 0.0327 - val_loss: 0.0085 - val_mae: 0.0618\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 0.04317\n",
      "Epoch 165/1000\n",
      "96/96 - 1s - loss: 1.2022e-06 - mae: 0.0327 - val_loss: 0.0091 - val_mae: 0.0645\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 0.04317\n",
      "Epoch 166/1000\n",
      "96/96 - 1s - loss: 1.2084e-06 - mae: 0.0328 - val_loss: 0.0090 - val_mae: 0.0646\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 0.04317\n",
      "Epoch 167/1000\n",
      "96/96 - 1s - loss: 1.2216e-06 - mae: 0.0330 - val_loss: 0.0082 - val_mae: 0.0609\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 0.04317\n",
      "Epoch 168/1000\n",
      "96/96 - 1s - loss: 1.1967e-06 - mae: 0.0326 - val_loss: 0.0091 - val_mae: 0.0653\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 0.04317\n",
      "Epoch 169/1000\n",
      "96/96 - 1s - loss: 1.1958e-06 - mae: 0.0326 - val_loss: 0.0078 - val_mae: 0.0585\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 0.04317\n",
      "Epoch 170/1000\n",
      "96/96 - 1s - loss: 1.1940e-06 - mae: 0.0326 - val_loss: 0.0087 - val_mae: 0.0623\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 0.04317\n",
      "Epoch 171/1000\n",
      "96/96 - 1s - loss: 1.1940e-06 - mae: 0.0326 - val_loss: 0.0089 - val_mae: 0.0629\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 0.04317\n",
      "Epoch 172/1000\n",
      "96/96 - 1s - loss: 1.1908e-06 - mae: 0.0326 - val_loss: 0.0090 - val_mae: 0.0645\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 0.04317\n",
      "Epoch 173/1000\n",
      "96/96 - 1s - loss: 1.1902e-06 - mae: 0.0326 - val_loss: 0.0091 - val_mae: 0.0636\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 0.04317\n",
      "Epoch 174/1000\n",
      "96/96 - 1s - loss: 1.2066e-06 - mae: 0.0329 - val_loss: 0.0086 - val_mae: 0.0618\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 0.04317\n",
      "Epoch 175/1000\n",
      "96/96 - 1s - loss: 1.1890e-06 - mae: 0.0326 - val_loss: 0.0093 - val_mae: 0.0649\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 0.04317\n",
      "Epoch 176/1000\n",
      "96/96 - 1s - loss: 1.1783e-06 - mae: 0.0325 - val_loss: 0.0078 - val_mae: 0.0581\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 0.04317\n",
      "Epoch 177/1000\n",
      "96/96 - 1s - loss: 1.1859e-06 - mae: 0.0325 - val_loss: 0.0099 - val_mae: 0.0681\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 0.04317\n",
      "Epoch 178/1000\n",
      "96/96 - 1s - loss: 1.1884e-06 - mae: 0.0326 - val_loss: 0.0091 - val_mae: 0.0641\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 0.04317\n",
      "Epoch 179/1000\n",
      "96/96 - 1s - loss: 1.1810e-06 - mae: 0.0324 - val_loss: 0.0094 - val_mae: 0.0654\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 0.04317\n",
      "Epoch 180/1000\n",
      "96/96 - 1s - loss: 1.2078e-06 - mae: 0.0330 - val_loss: 0.0096 - val_mae: 0.0660\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 0.04317\n",
      "Epoch 181/1000\n",
      "96/96 - 1s - loss: 1.1865e-06 - mae: 0.0326 - val_loss: 0.0090 - val_mae: 0.0633\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 0.04317\n",
      "Epoch 182/1000\n",
      "96/96 - 1s - loss: 1.1994e-06 - mae: 0.0329 - val_loss: 0.0095 - val_mae: 0.0661\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 0.04317\n",
      "Epoch 183/1000\n",
      "96/96 - 1s - loss: 1.1741e-06 - mae: 0.0324 - val_loss: 0.0102 - val_mae: 0.0695\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 0.04317\n",
      "Epoch 184/1000\n",
      "96/96 - 1s - loss: 1.1766e-06 - mae: 0.0325 - val_loss: 0.0093 - val_mae: 0.0645\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 0.04317\n",
      "Epoch 185/1000\n",
      "96/96 - 1s - loss: 1.1738e-06 - mae: 0.0325 - val_loss: 0.0098 - val_mae: 0.0687\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 0.04317\n",
      "Epoch 186/1000\n",
      "96/96 - 1s - loss: 1.2025e-06 - mae: 0.0329 - val_loss: 0.0096 - val_mae: 0.0682\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 0.04317\n",
      "Epoch 187/1000\n",
      "96/96 - 1s - loss: 1.1730e-06 - mae: 0.0324 - val_loss: 0.0087 - val_mae: 0.0622\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 0.04317\n",
      "Epoch 188/1000\n",
      "96/96 - 1s - loss: 1.1627e-06 - mae: 0.0323 - val_loss: 0.0101 - val_mae: 0.0697\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 0.04317\n",
      "Epoch 189/1000\n",
      "96/96 - 1s - loss: 1.1695e-06 - mae: 0.0324 - val_loss: 0.0090 - val_mae: 0.0630\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 0.04317\n",
      "Epoch 190/1000\n",
      "96/96 - 1s - loss: 1.1738e-06 - mae: 0.0325 - val_loss: 0.0088 - val_mae: 0.0625\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 0.04317\n",
      "Epoch 191/1000\n",
      "96/96 - 1s - loss: 1.1650e-06 - mae: 0.0324 - val_loss: 0.0085 - val_mae: 0.0616\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 0.04317\n",
      "Epoch 192/1000\n",
      "96/96 - 1s - loss: 1.1759e-06 - mae: 0.0325 - val_loss: 0.0088 - val_mae: 0.0629\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 0.04317\n",
      "Epoch 193/1000\n",
      "96/96 - 1s - loss: 1.1726e-06 - mae: 0.0325 - val_loss: 0.0091 - val_mae: 0.0639\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 0.04317\n",
      "Epoch 194/1000\n",
      "96/96 - 1s - loss: 1.1612e-06 - mae: 0.0324 - val_loss: 0.0085 - val_mae: 0.0612\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 0.04317\n",
      "Epoch 195/1000\n",
      "96/96 - 1s - loss: 1.1534e-06 - mae: 0.0323 - val_loss: 0.0093 - val_mae: 0.0642\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 0.04317\n",
      "Epoch 196/1000\n",
      "96/96 - 1s - loss: 1.1606e-06 - mae: 0.0323 - val_loss: 0.0086 - val_mae: 0.0615\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 0.04317\n",
      "Epoch 197/1000\n",
      "96/96 - 1s - loss: 1.1527e-06 - mae: 0.0323 - val_loss: 0.0091 - val_mae: 0.0635\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 0.04317\n",
      "Epoch 198/1000\n",
      "96/96 - 1s - loss: 1.1556e-06 - mae: 0.0323 - val_loss: 0.0085 - val_mae: 0.0608\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 0.04317\n",
      "Epoch 199/1000\n",
      "96/96 - 1s - loss: 1.1575e-06 - mae: 0.0324 - val_loss: 0.0098 - val_mae: 0.0669\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 0.04317\n",
      "Epoch 200/1000\n",
      "96/96 - 1s - loss: 1.1602e-06 - mae: 0.0324 - val_loss: 0.0095 - val_mae: 0.0656\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 0.04317\n",
      "Epoch 201/1000\n",
      "96/96 - 1s - loss: 1.1446e-06 - mae: 0.0321 - val_loss: 0.0093 - val_mae: 0.0651\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 0.04317\n",
      "Epoch 202/1000\n",
      "96/96 - 1s - loss: 1.1476e-06 - mae: 0.0323 - val_loss: 0.0093 - val_mae: 0.0635\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 0.04317\n",
      "Epoch 203/1000\n",
      "96/96 - 1s - loss: 1.1336e-06 - mae: 0.0320 - val_loss: 0.0100 - val_mae: 0.0680\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 0.04317\n",
      "Epoch 204/1000\n",
      "96/96 - 1s - loss: 1.1626e-06 - mae: 0.0326 - val_loss: 0.0094 - val_mae: 0.0648\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 0.04317\n",
      "Epoch 205/1000\n",
      "96/96 - 1s - loss: 1.1438e-06 - mae: 0.0322 - val_loss: 0.0090 - val_mae: 0.0630\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 0.04317\n",
      "Epoch 206/1000\n",
      "96/96 - 1s - loss: 1.1375e-06 - mae: 0.0321 - val_loss: 0.0091 - val_mae: 0.0634\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 0.04317\n",
      "Epoch 207/1000\n",
      "96/96 - 1s - loss: 1.1320e-06 - mae: 0.0320 - val_loss: 0.0093 - val_mae: 0.0648\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 0.04317\n",
      "Epoch 208/1000\n",
      "96/96 - 1s - loss: 1.1378e-06 - mae: 0.0321 - val_loss: 0.0096 - val_mae: 0.0663\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 0.04317\n",
      "Epoch 209/1000\n",
      "96/96 - 1s - loss: 1.1545e-06 - mae: 0.0324 - val_loss: 0.0091 - val_mae: 0.0630\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 0.04317\n",
      "Epoch 210/1000\n",
      "96/96 - 1s - loss: 1.1404e-06 - mae: 0.0322 - val_loss: 0.0090 - val_mae: 0.0628\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 0.04317\n",
      "Epoch 211/1000\n",
      "96/96 - 1s - loss: 1.1281e-06 - mae: 0.0320 - val_loss: 0.0093 - val_mae: 0.0645\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 0.04317\n",
      "Epoch 212/1000\n",
      "96/96 - 1s - loss: 1.1265e-06 - mae: 0.0320 - val_loss: 0.0101 - val_mae: 0.0692\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 0.04317\n",
      "Epoch 213/1000\n",
      "96/96 - 1s - loss: 1.1327e-06 - mae: 0.0321 - val_loss: 0.0103 - val_mae: 0.0697\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 0.04317\n",
      "Epoch 214/1000\n",
      "96/96 - 1s - loss: 1.1273e-06 - mae: 0.0320 - val_loss: 0.0091 - val_mae: 0.0631\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 0.04317\n",
      "Epoch 215/1000\n",
      "96/96 - 1s - loss: 1.1454e-06 - mae: 0.0324 - val_loss: 0.0098 - val_mae: 0.0676\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 0.04317\n",
      "Epoch 216/1000\n",
      "96/96 - 1s - loss: 1.1190e-06 - mae: 0.0320 - val_loss: 0.0102 - val_mae: 0.0692\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 0.04317\n",
      "Epoch 217/1000\n",
      "96/96 - 1s - loss: 1.1117e-06 - mae: 0.0318 - val_loss: 0.0091 - val_mae: 0.0627\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 0.04317\n",
      "Epoch 218/1000\n",
      "96/96 - 1s - loss: 1.1205e-06 - mae: 0.0320 - val_loss: 0.0096 - val_mae: 0.0659\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 0.04317\n",
      "Epoch 219/1000\n",
      "96/96 - 1s - loss: 1.1260e-06 - mae: 0.0321 - val_loss: 0.0104 - val_mae: 0.0698\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 0.04317\n",
      "Epoch 220/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 - 1s - loss: 1.1307e-06 - mae: 0.0322 - val_loss: 0.0087 - val_mae: 0.0617\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 0.04317\n",
      "Epoch 221/1000\n",
      "96/96 - 1s - loss: 1.1268e-06 - mae: 0.0322 - val_loss: 0.0093 - val_mae: 0.0647\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 0.04317\n",
      "Epoch 222/1000\n",
      "96/96 - 1s - loss: 1.1055e-06 - mae: 0.0318 - val_loss: 0.0096 - val_mae: 0.0659\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 0.04317\n",
      "Epoch 223/1000\n",
      "96/96 - 1s - loss: 1.1145e-06 - mae: 0.0319 - val_loss: 0.0090 - val_mae: 0.0626\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 0.04317\n",
      "Epoch 00223: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9d7aa3a0d0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mae']\n",
    "             )\n",
    "\n",
    "es = EarlyStopping(monitor='val_mae', mode='min', verbose=2, patience=PATIENCE)\n",
    "mc = ModelCheckpoint('../../saved_models/pressure_all_weighted_IPF_95.h5', \n",
    "                     monitor='val_mae', \n",
    "                     mode='min', \n",
    "                     verbose=2, \n",
    "                     save_best_only=True,\n",
    "                    )\n",
    "\n",
    "\n",
    "model.fit(train_X, train_y,\n",
    "          validation_data=(val_X, val_y),\n",
    "          epochs=EPOCHS,\n",
    "          batch_size=BATCH,\n",
    "          verbose=2,\n",
    "          shuffle=True,\n",
    "          callbacks=[es, mc],\n",
    "          sample_weight=sample_weights\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b313809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95854ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6ba252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae65733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
